{"repo":"justadudewhohacks/face-api.js","url":"https://github.com/justadudewhohacks/face-api.js","branch":"master","configs":[{"package":"face-api.js","lang":"ts","dir":"test","framework":"jasmine","pattern":"**/*[._-]{test,spec,unittest,unit}.{ts,js}"}],"tests":[{"name":"returns correct params","suites":["NeuralNetwork","getParamFromPath"],"line":44,"updatePoint":{"line":44,"column":30},"code":"    it('returns correct params', () => tf.tidy(() => {\n      const convFilter = tf.tensor(0)\n      const convBias = tf.tensor(0)\n      const fcWeights = tf.tensor(0)\n      const net = new FakeNeuralNetwork(convFilter, convBias, fcWeights)\n\n      expect(net.getParamFromPath('conv/filter')).toEqual(convFilter)\n      expect(net.getParamFromPath('conv/bias')).toEqual(convBias)\n      expect(net.getParamFromPath('fc')).toEqual(fcWeights)\n    }))","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"throws if param is not a tensor","suites":["NeuralNetwork","getParamFromPath"],"line":55,"updatePoint":{"line":55,"column":39},"code":"    it('throws if param is not a tensor', () => tf.tidy(() => {\n      const net = new FakeNeuralNetwork(null as any)\n      const fakePath = 'conv/filter'\n\n      expect(\n        () => net.getParamFromPath(fakePath)\n      ).toThrowError(`traversePropertyPath - parameter is not a tensor, for path ${fakePath}`)\n    }))","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"throws if key path invalid","suites":["NeuralNetwork","getParamFromPath"],"line":64,"updatePoint":{"line":64,"column":34},"code":"    it('throws if key path invalid', () => tf.tidy(() => {\n      const net = new FakeNeuralNetwork()\n      const fakePath = 'conv2d/foo'\n\n      expect(\n        () => net.getParamFromPath(fakePath)\n      ).toThrowError(`traversePropertyPath - object does not have property conv2d, for path ${fakePath}`)\n    }))","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"sets correct params","suites":["NeuralNetwork","reassignParamFromPath"],"line":77,"updatePoint":{"line":77,"column":27},"code":"    it('sets correct params', () => tf.tidy(() => {\n      const net = new FakeNeuralNetwork()\n\n      const convFilter = tf.tensor(0)\n      const convBias = tf.tensor(0)\n      const fcWeights = tf.tensor(0)\n      net.reassignParamFromPath('conv/filter', convFilter)\n      net.reassignParamFromPath('conv/bias', convBias)\n      net.reassignParamFromPath('fc', fcWeights)\n\n      expect(net.params.conv.filter).toEqual(convFilter)\n      expect(net.params.conv.bias).toEqual(convBias)\n      expect(net.params.fc).toEqual(fcWeights)\n    }))","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"throws if param is not a tensor","suites":["NeuralNetwork","reassignParamFromPath"],"line":92,"updatePoint":{"line":92,"column":39},"code":"    it('throws if param is not a tensor', () => tf.tidy(() => {\n      const net = new FakeNeuralNetwork(null as any)\n      const fakePath = 'conv/filter'\n\n      expect(\n        () => net.reassignParamFromPath(fakePath, tf.tensor(0))\n      ).toThrowError(`traversePropertyPath - parameter is not a tensor, for path ${fakePath}`)\n    }))","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"throws if key path invalid","suites":["NeuralNetwork","reassignParamFromPath"],"line":101,"updatePoint":{"line":101,"column":34},"code":"    it('throws if key path invalid', () => tf.tidy(() => {\n      const net = new FakeNeuralNetwork()\n      const fakePath = 'conv2d/foo'\n\n      expect(\n        () => net.reassignParamFromPath(fakePath, tf.tensor(0))\n      ).toThrowError(`traversePropertyPath - object does not have property conv2d, for path ${fakePath}`)\n    }))","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"returns param tensors with path","suites":["NeuralNetwork","getParamList"],"line":114,"updatePoint":{"line":114,"column":39},"code":"    it('returns param tensors with path', () => tf.tidy(() => {\n      const convFilter = tf.tensor(0)\n      const convBias = tf.tensor(0)\n      const fcWeights = tf.tensor(0)\n      const net = new FakeNeuralNetwork(convFilter, convBias, fcWeights)\n\n      const paramList = net.getParamList()\n\n      expect(paramList.length).toEqual(3)\n      expect(paramList[0].path).toEqual('conv/filter')\n      expect(paramList[1].path).toEqual('conv/bias')\n      expect(paramList[2].path).toEqual('fc')\n      expect(paramList[0].tensor).toEqual(convFilter)\n      expect(paramList[1].tensor).toEqual(convBias)\n      expect(paramList[2].tensor).toEqual(fcWeights)\n    }))","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"returns all frozen params","suites":["NeuralNetwork","getFrozenParams"],"line":135,"updatePoint":{"line":135,"column":33},"code":"    it('returns all frozen params', () => tf.tidy(() => {\n      const convFilter = tf.tensor(0)\n      const convBias = tf.tensor(0)\n      const fcWeights = tf.variable(tf.scalar(0))\n      const net = new FakeNeuralNetwork(convFilter, convBias, fcWeights)\n\n      const frozenParams = net.getFrozenParams()\n\n      expect(frozenParams.length).toEqual(2)\n      expect(frozenParams[0].path).toEqual('conv/filter')\n      expect(frozenParams[1].path).toEqual('conv/bias')\n      expect(frozenParams[0].tensor).toEqual(convFilter)\n      expect(frozenParams[1].tensor).toEqual(convBias)\n    }))","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"returns all trainable params","suites":["NeuralNetwork","getTrainableParams"],"line":154,"updatePoint":{"line":154,"column":36},"code":"    it('returns all trainable params', () => tf.tidy(() => {\n      const convFilter = tf.variable(tf.scalar(0))\n      const convBias = tf.variable(tf.scalar(0))\n      const fcWeights = tf.tensor(0)\n      const net = new FakeNeuralNetwork(convFilter, convBias, fcWeights)\n\n      const trainableParams = net.getTrainableParams()\n\n      expect(trainableParams.length).toEqual(2)\n      expect(trainableParams[0].path).toEqual('conv/filter')\n      expect(trainableParams[1].path).toEqual('conv/bias')\n      expect(trainableParams[0].tensor).toEqual(convFilter)\n      expect(trainableParams[1].tensor).toEqual(convBias)\n    }))","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"disposes all param tensors","suites":["NeuralNetwork","dispose"],"line":173,"updatePoint":{"line":173,"column":34},"code":"    it('disposes all param tensors', () => tf.tidy(() => {\n      const numTensors = tf.memory().numTensors\n      const net = new FakeNeuralNetwork()\n\n      net.dispose()\n\n      expect(net.params).toBe(undefined)\n      expect(tf.memory().numTensors - numTensors).toEqual(0)\n    }))","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"make all param tensors trainable","suites":["NeuralNetwork","variable"],"line":187,"updatePoint":{"line":187,"column":40},"code":"    it('make all param tensors trainable', () => tf.tidy(() => {\n      const net = new FakeNeuralNetwork()\n\n      net.variable()\n\n      expect(net.params.conv.filter instanceof tf.Variable).toBe(true)\n      expect(net.params.conv.bias instanceof tf.Variable).toBe(true)\n      expect(net.params.fc instanceof tf.Variable).toBe(true)\n    }))","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"disposes old tensors","suites":["NeuralNetwork","variable"],"line":197,"updatePoint":{"line":197,"column":28},"code":"    it('disposes old tensors', () => tf.tidy(() => {\n      const net = new FakeNeuralNetwork()\n      const numTensors = tf.memory().numTensors\n\n      net.variable()\n\n      expect(tf.memory().numTensors - numTensors).toEqual(0)\n    }))","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"freezes all param variables","suites":["NeuralNetwork","freeze"],"line":210,"updatePoint":{"line":210,"column":35},"code":"    it('freezes all param variables', () => tf.tidy(() => {\n      const net = new FakeNeuralNetwork(\n        tf.variable(tf.scalar(0)),\n        tf.variable(tf.scalar(0)),\n        tf.variable(tf.scalar(0))\n      )\n\n      net.freeze()\n\n      expect(net.params.conv.filter instanceof tf.Variable).toBe(false)\n      expect(net.params.conv.bias instanceof tf.Variable).toBe(false)\n      expect(net.params.fc instanceof tf.Variable).toBe(false)\n    }))","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"disposes old tensors","suites":["NeuralNetwork","freeze"],"line":224,"updatePoint":{"line":224,"column":28},"code":"    it('disposes old tensors', () => () => {\n      const net = new FakeNeuralNetwork(\n        tf.variable(tf.scalar(0)),\n        tf.variable(tf.scalar(0)),\n        tf.variable(tf.scalar(0))\n      )\n      const numTensors = tf.memory().numTensors\n\n      net.freeze()\n\n      expect(tf.memory().numTensors - numTensors).toEqual(0)\n    })","file":"NeuralNetwork.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for squared input","suites":[],"line":21,"updatePoint":{"line":21,"column":49},"code":"    it('computes face landmarks for squared input', async () => {\n      const { width, height } = imgEl1\n\n      const result = await faceLandmark68Net.detectLandmarks(imgEl1) as FaceLandmarks68\n      expect(result.imageWidth).toEqual(width)\n      expect(result.imageHeight).toEqual(height)\n      expect(result.shift.x).toEqual(0)\n      expect(result.shift.y).toEqual(0)\n      result.positions.forEach((pt, i) => {\n        const { x, y } = faceLandmarkPositions1[i]\n        expectPointClose(pt, { x, y }, 1)\n      })\n    })","file":"tests-legacy/faceLandmark68Net.uncompressed.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for rectangular input","suites":[],"line":35,"updatePoint":{"line":35,"column":53},"code":"    it('computes face landmarks for rectangular input', async () => {\n      const { width, height } = imgElRect\n\n      const result = await faceLandmark68Net.detectLandmarks(imgElRect) as FaceLandmarks68\n      expect(result.imageWidth).toEqual(width)\n      expect(result.imageHeight).toEqual(height)\n      expect(result.shift.x).toEqual(0)\n      expect(result.shift.y).toEqual(0)\n      result.positions.forEach((pt, i) => {\n        const { x, y } = faceLandmarkPositionsRect[i]\n        expectPointClose(pt, { x, y }, 5)\n      })\n    })","file":"tests-legacy/faceLandmark68Net.uncompressed.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for squared input","suites":[],"line":22,"updatePoint":{"line":22,"column":49},"code":"    it('computes face landmarks for squared input', async () => {\n      const { width, height } = imgEl1\n\n      const result = await faceLandmark68TinyNet.detectLandmarks(imgEl1) as FaceLandmarks68\n      expect(result.imageWidth).toEqual(width)\n      expect(result.imageHeight).toEqual(height)\n      expect(result.shift.x).toEqual(0)\n      expect(result.shift.y).toEqual(0)\n      result.positions.forEach((pt, i) => {\n        const { x, y } = faceLandmarkPositions1[i]\n        expectPointClose(pt, { x, y }, 5)\n      })\n    })","file":"tests-legacy/faceLandmark68TinyNet.uncompressed.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for rectangular input","suites":[],"line":36,"updatePoint":{"line":36,"column":53},"code":"    it('computes face landmarks for rectangular input', async () => {\n      const { width, height } = imgElRect\n\n      const result = await faceLandmark68TinyNet.detectLandmarks(imgElRect) as FaceLandmarks68\n      expect(result.imageWidth).toEqual(width)\n      expect(result.imageHeight).toEqual(height)\n      expect(result.shift.x).toEqual(0)\n      expect(result.shift.y).toEqual(0)\n      result.positions.forEach((pt, i) => {\n        const { x, y } = faceLandmarkPositionsRect[i]\n        expectPointClose(pt, { x, y }, 5)\n      })\n    })","file":"tests-legacy/faceLandmark68TinyNet.uncompressed.test.ts","skipped":false,"dir":"test"},{"name":"computes face descriptor for squared input","suites":[],"line":21,"updatePoint":{"line":21,"column":50},"code":"    it('computes face descriptor for squared input', async () => {\n      const result = await faceRecognitionNet.computeFaceDescriptor(imgEl1) as Float32Array\n      expect(result.length).toEqual(128)\n      expect(euclideanDistance(result, faceDescriptor1)).toBeLessThan(0.1)\n    })","file":"tests-legacy/faceRecognitionNet.uncompressed.test.ts","skipped":false,"dir":"test"},{"name":"computes face descriptor for rectangular input","suites":[],"line":27,"updatePoint":{"line":27,"column":54},"code":"    it('computes face descriptor for rectangular input', async () => {\n      const result = await faceRecognitionNet.computeFaceDescriptor(imgElRect) as Float32Array\n      expect(result.length).toEqual(128)\n      expect(euclideanDistance(result, faceDescriptorRect)).toBeLessThan(0.1)\n    })","file":"tests-legacy/faceRecognitionNet.uncompressed.test.ts","skipped":false,"dir":"test"},{"name":"minFaceSize = 20, finds all faces","suites":[],"line":19,"updatePoint":{"line":19,"column":41},"code":"    it('minFaceSize = 20, finds all faces', async () => {\n      const forwardParams = {\n        minFaceSize: 20\n      }\n\n      const results = await mtcnn.forward(imgEl, forwardParams)\n      expect(results.length).toEqual(6)\n\n      const deltas = {\n        maxScoreDelta: 0.01,\n        maxBoxDelta: 10,\n        maxLandmarksDelta: 10\n      }\n      expectMtcnnResults(results, expectedMtcnnLandmarks, [1.0, 1.0, 1.0, 1.0, 0.99, 0.99], deltas)\n    })","file":"tests-legacy/mtcnn/mtcnn.forward.test.ts","skipped":false,"dir":"test"},{"name":"minFaceSize = 80, finds all faces","suites":[],"line":35,"updatePoint":{"line":35,"column":41},"code":"    it('minFaceSize = 80, finds all faces', async () => {\n      const forwardParams = {\n        minFaceSize: 80\n      }\n\n      const results = await mtcnn.forward(imgEl, forwardParams)\n\n      expect(results.length).toEqual(6)\n      const deltas = {\n        maxScoreDelta: 0.01,\n        maxBoxDelta: 15,\n        maxLandmarksDelta: 15\n      }\n      expectMtcnnResults(results, expectedMtcnnLandmarks, [1.0, 1.0, 1.0, 1.0, 1.0, 0.99], deltas)\n    })","file":"tests-legacy/mtcnn/mtcnn.forward.test.ts","skipped":false,"dir":"test"},{"name":"all optional params passed, finds all faces","suites":[],"line":51,"updatePoint":{"line":51,"column":51},"code":"    it('all optional params passed, finds all faces', async () => {\n      const forwardParams = {\n        maxNumScales: 10,\n        scaleFactor: 0.8,\n        scoreThresholds: [0.8, 0.8, 0.9],\n        minFaceSize: 20\n      }\n\n      const results = await mtcnn.forward(imgEl, forwardParams)\n      expect(results.length).toEqual(6)\n\n      const deltas = {\n        maxScoreDelta: 0.01,\n        maxBoxDelta: 15,\n        maxLandmarksDelta: 20\n      }\n      expectMtcnnResults(results, expectedMtcnnLandmarks, [1.0, 1.0, 1.0, 0.99, 1.0, 1.0], deltas)\n    })","file":"tests-legacy/mtcnn/mtcnn.forward.test.ts","skipped":false,"dir":"test"},{"name":"scale steps passed, finds all faces","suites":[],"line":70,"updatePoint":{"line":70,"column":43},"code":"    it('scale steps passed, finds all faces', async () => {\n      const forwardParams = {\n        scaleSteps: [0.6, 0.4, 0.2, 0.15, 0.1, 0.08, 0.02]\n      }\n\n      const results = await mtcnn.forward(imgEl, forwardParams)\n      expect(results.length).toEqual(6)\n\n      const deltas = {\n        maxScoreDelta: 0.01,\n        maxBoxDelta: 15,\n        maxLandmarksDelta: 15\n      }\n      expectMtcnnResults(results, expectedMtcnnLandmarks, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], deltas)\n    })","file":"tests-legacy/mtcnn/mtcnn.forward.test.ts","skipped":false,"dir":"test"},{"name":"minFaceSize = 20, finds all faces","suites":[],"line":18,"updatePoint":{"line":18,"column":41},"code":"    it('minFaceSize = 20, finds all faces', async () => {\n      const forwardParams = {\n        minFaceSize: 20\n      }\n\n      const results = await mtcnn.forward(imgEl, forwardParams)\n      expect(results.length).toEqual(6)\n\n      const deltas = {\n        maxScoreDelta: 0.01,\n        maxBoxDelta: 10,\n        maxLandmarksDelta: 10\n      }\n      expectMtcnnResults(results, expectedMtcnnLandmarks, [1.0, 1.0, 1.0, 1.0, 0.99, 0.99], deltas)\n    })","file":"tests-legacy/mtcnn/mtcnn.forward.uncompressed.test.ts","skipped":false,"dir":"test"},{"name":"minFaceSize = 80, finds all faces","suites":[],"line":34,"updatePoint":{"line":34,"column":41},"code":"    it('minFaceSize = 80, finds all faces', async () => {\n      const forwardParams = {\n        minFaceSize: 80\n      }\n\n      const results = await mtcnn.forward(imgEl, forwardParams)\n\n      expect(results.length).toEqual(6)\n      const deltas = {\n        maxScoreDelta: 0.01,\n        maxBoxDelta: 15,\n        maxLandmarksDelta: 15\n      }\n      expectMtcnnResults(results, expectedMtcnnLandmarks, [1.0, 1.0, 1.0, 1.0, 1.0, 0.99], deltas)\n    })","file":"tests-legacy/mtcnn/mtcnn.forward.uncompressed.test.ts","skipped":false,"dir":"test"},{"name":"all optional params passed, finds all faces","suites":[],"line":50,"updatePoint":{"line":50,"column":51},"code":"    it('all optional params passed, finds all faces', async () => {\n      const forwardParams = {\n        maxNumScales: 10,\n        scaleFactor: 0.8,\n        scoreThresholds: [0.8, 0.8, 0.9],\n        minFaceSize: 20\n      }\n\n      const results = await mtcnn.forward(imgEl, forwardParams)\n      expect(results.length).toEqual(6)\n\n      const deltas = {\n        maxScoreDelta: 0.01,\n        maxBoxDelta: 15,\n        maxLandmarksDelta: 20\n      }\n      expectMtcnnResults(results, expectedMtcnnLandmarks, [1.0, 1.0, 1.0, 0.99, 1.0, 1.0], deltas)\n    })","file":"tests-legacy/mtcnn/mtcnn.forward.uncompressed.test.ts","skipped":false,"dir":"test"},{"name":"scale steps passed, finds all faces","suites":[],"line":69,"updatePoint":{"line":69,"column":43},"code":"    it('scale steps passed, finds all faces', async () => {\n      const forwardParams = {\n        scaleSteps: [0.6, 0.4, 0.2, 0.15, 0.1, 0.08, 0.02]\n      }\n\n      const results = await mtcnn.forward(imgEl, forwardParams)\n      expect(results.length).toEqual(6)\n\n      const deltas = {\n        maxScoreDelta: 0.01,\n        maxBoxDelta: 15,\n        maxLandmarksDelta: 15\n      }\n      expectMtcnnResults(results, expectedMtcnnLandmarks, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], deltas)\n    })","file":"tests-legacy/mtcnn/mtcnn.forward.uncompressed.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces","suites":[],"line":29,"updatePoint":{"line":29,"column":22},"code":"    it('detectAllFaces', async () => {\n      const options = new MtcnnOptions({\n        minFaceSize: 20\n      })\n\n      const results = await faceapi.detectAllFaces(imgEl, options)\n      const maxScoreDelta = 0.01\n      const maxBoxDelta = 10\n      expect(results.length).toEqual(6)\n      expectFaceDetections(results, expectedMtcnnBoxes, expectedScores, maxScoreDelta, maxBoxDelta)\n    })","file":"tests-legacy/mtcnn/mtcnn.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withFaceDescriptors()","suites":[],"line":41,"updatePoint":{"line":41,"column":64},"code":"    it('detectAllFaces.withFaceLandmarks().withFaceDescriptors()', async () => {\n      const options = new MtcnnOptions({\n        minFaceSize: 20\n      })\n\n      const results = await faceapi\n        .detectAllFaces(imgEl, options)\n        .withFaceLandmarks()\n\n      const deltas = {\n        maxScoreDelta: 0.01,\n        maxBoxDelta: 10,\n        maxLandmarksDelta: 6\n      }\n      expect(results.length).toEqual(6)\n      expectFaceDetectionsWithLandmarks(results, expectedFullFaceDescriptions, expectedScores, deltas)\n    })","file":"tests-legacy/mtcnn/mtcnn.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withFaceDescriptors()","suites":[],"line":59,"updatePoint":{"line":59,"column":64},"code":"    it('detectAllFaces.withFaceLandmarks().withFaceDescriptors()', async () => {\n      const options = new MtcnnOptions({\n        minFaceSize: 20\n      })\n\n      const results = await faceapi\n        .detectAllFaces(imgEl, options)\n        .withFaceLandmarks()\n        .withFaceDescriptors()\n\n      const deltas = {\n        maxScoreDelta: 0.01,\n        maxBoxDelta: 10,\n        maxLandmarksDelta: 6,\n        maxDescriptorDelta: 0.2\n      }\n      expect(results.length).toEqual(6)\n      expectFullFaceDescriptions(results, expectedFullFaceDescriptions, expectedScores, deltas)\n    })","file":"tests-legacy/mtcnn/mtcnn.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withFaceDescriptor()","suites":[],"line":79,"updatePoint":{"line":79,"column":65},"code":"    it('detectSingleFace.withFaceLandmarks().withFaceDescriptor()', async () => {\n      const options = new MtcnnOptions({\n        minFaceSize: 20\n      })\n\n      const result = await faceapi\n        .detectSingleFace(imgEl, options)\n        .withFaceLandmarks()\n        .withFaceDescriptor()\n\n      const deltas = {\n        maxScoreDelta: 0.01,\n        maxBoxDelta: 10,\n        maxLandmarksDelta: 6,\n        maxDescriptorDelta: 0.2\n      }\n\n      expect(!!result).toBeTruthy()\n      expectFullFaceDescriptions(\n        result ? [result] : [],\n        [expectedFullFaceDescriptions[0]],\n        [expectedScores[0]],\n        deltas\n      )\n    })","file":"tests-legacy/mtcnn/mtcnn.test.ts","skipped":false,"dir":"test"},{"name":"no memory leaks","suites":[],"line":105,"updatePoint":{"line":105,"column":23},"code":"    it('no memory leaks', async () => {\n      await expectAllTensorsReleased(async () => {\n        await faceapi\n          .detectAllFaces(imgEl, new MtcnnOptions({ minFaceSize: 200 }))\n          .withFaceLandmarks()\n          .withFaceDescriptors()\n      })\n    })","file":"tests-legacy/mtcnn/mtcnn.test.ts","skipped":false,"dir":"test"},{"name":"scores > 0.8","suites":[],"line":17,"updatePoint":{"line":17,"column":20},"code":"    it('scores > 0.8', async () => {\n      const detections = await ssdMobilenetv1.locateFaces(imgEl, { minConfidence: 0.8 }) as faceapi.FaceDetection[]\n\n      expect(detections.length).toEqual(3)\n\n      const expectedScores = [-1, -1, 0.98, 0.88, 0.81, -1]\n      const maxScoreDelta = 0.05\n      const maxBoxDelta = 5\n\n      expectFaceDetections(detections, expectedSsdBoxes, expectedScores, maxScoreDelta, maxBoxDelta)\n    })","file":"tests-legacy/ssdMobilenetv1.locateFaces.uncompressed.test.ts","skipped":false,"dir":"test"},{"name":"scores > 0.5","suites":[],"line":29,"updatePoint":{"line":29,"column":20},"code":"    it('scores > 0.5', async () => {\n      const detections = await ssdMobilenetv1.locateFaces(imgEl, { minConfidence: 0.5 }) as faceapi.FaceDetection[]\n\n      expect(detections.length).toEqual(6)\n\n      const expectedScores = [0.57, 0.76, 0.98, 0.88, 0.81, 0.58]\n      const maxScoreDelta = 0.05\n      const maxBoxDelta = 5\n\n      expectFaceDetections(detections, expectedSsdBoxes, expectedScores, maxScoreDelta, maxBoxDelta)\n    })","file":"tests-legacy/ssdMobilenetv1.locateFaces.uncompressed.test.ts","skipped":false,"dir":"test"},{"name":"recognizes age and gender","suites":[],"line":34,"updatePoint":{"line":34,"column":33},"code":"    it('recognizes age and gender', async () => {\n      const result = await ageGenderNet.predictAgeAndGender(imgElAngry) as AgeAndGenderPrediction\n      expectResultsAngry(result)\n    })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"recognizes age and gender for batch of image elements","suites":[],"line":43,"updatePoint":{"line":43,"column":61},"code":"    it('recognizes age and gender for batch of image elements', async () => {\n      const inputs = [imgElAngry, imgElSurprised]\n\n      const results = await ageGenderNet.predictAgeAndGender(inputs) as AgeAndGenderPrediction[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(2)\n\n      const [resultAngry, resultSurprised] = results\n      expectResultsAngry(resultAngry)\n      expectResultsSurprised(resultSurprised)\n    })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"computes age and gender for batch of tf.Tensor3D","suites":[],"line":55,"updatePoint":{"line":55,"column":56},"code":"    it('computes age and gender for batch of tf.Tensor3D', async () => {\n      const inputs = [imgElAngry, imgElSurprised].map(el => tf.browser.fromPixels(createCanvasFromMedia(el)))\n\n      const results = await ageGenderNet.predictAgeAndGender(inputs) as AgeAndGenderPrediction[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(2)\n\n      const [resultAngry, resultSurprised] = results\n      expectResultsAngry(resultAngry)\n      expectResultsSurprised(resultSurprised)\n    })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"computes age and gender for batch of mixed inputs","suites":[],"line":67,"updatePoint":{"line":67,"column":57},"code":"    it('computes age and gender for batch of mixed inputs', async () => {\n      const inputs = [imgElAngry, tf.browser.fromPixels(createCanvasFromMedia(imgElSurprised))]\n\n      const results = await ageGenderNet.predictAgeAndGender(inputs) as AgeAndGenderPrediction[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(2)\n\n      const [resultAngry, resultSurprised] = results\n      expectResultsAngry(resultAngry)\n      expectResultsSurprised(resultSurprised)\n    })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"single image element","suites":["forwardInput"],"line":85,"updatePoint":{"line":85,"column":30},"code":"      it('single image element', async () => {\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput([imgElAngry])\n          const { age, gender } = await ageGenderNet.forwardInput(netInput)\n          age.dispose()\n          gender.dispose()\n        })\n      })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple image elements","suites":["forwardInput"],"line":94,"updatePoint":{"line":94,"column":33},"code":"      it('multiple image elements', async () => {\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput([imgElAngry, imgElAngry])\n          const { age, gender } = await ageGenderNet.forwardInput(netInput)\n          age.dispose()\n          gender.dispose()\n        })\n      })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"single tf.Tensor3D","suites":["forwardInput"],"line":103,"updatePoint":{"line":103,"column":28},"code":"      it('single tf.Tensor3D', async () => {\n        const tensor = tf.browser.fromPixels(createCanvasFromMedia(imgElAngry))\n\n        await expectAllTensorsReleased(async () => {\n          const { age, gender } = await ageGenderNet.forwardInput(await toNetInput(tensor))\n          age.dispose()\n          gender.dispose()\n        })\n\n        tensor.dispose()\n      })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple tf.Tensor3Ds","suites":["forwardInput"],"line":115,"updatePoint":{"line":115,"column":31},"code":"      it('multiple tf.Tensor3Ds', async () => {\n        const tensors = [imgElAngry, imgElAngry, imgElAngry].map(el => tf.browser.fromPixels(createCanvasFromMedia(el)))\n\n        await expectAllTensorsReleased(async () => {\n          const { age, gender } = await ageGenderNet.forwardInput(await toNetInput(tensors))\n          age.dispose()\n          gender.dispose()\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"single batch size 1 tf.Tensor4Ds","suites":["forwardInput"],"line":127,"updatePoint":{"line":127,"column":42},"code":"      it('single batch size 1 tf.Tensor4Ds', async () => {\n        const tensor = tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(imgElAngry)).expandDims()) as tf.Tensor4D\n\n        await expectAllTensorsReleased(async () => {\n          const { age, gender } = await ageGenderNet.forwardInput(await toNetInput(tensor))\n          age.dispose()\n          gender.dispose()\n        })\n\n        tensor.dispose()\n      })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple batch size 1 tf.Tensor4Ds","suites":["forwardInput"],"line":139,"updatePoint":{"line":139,"column":44},"code":"      it('multiple batch size 1 tf.Tensor4Ds', async () => {\n        const tensors = [imgElAngry, imgElAngry, imgElAngry]\n          .map(el => tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(el)).expandDims())) as tf.Tensor4D[]\n\n        await expectAllTensorsReleased(async () => {\n          const { age, gender } = await ageGenderNet.forwardInput(await toNetInput(tensors))\n          age.dispose()\n          gender.dispose()\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"single image element","suites":["predictExpressions"],"line":156,"updatePoint":{"line":156,"column":30},"code":"      it('single image element', async () => {\n        await expectAllTensorsReleased(async () => {\n          await ageGenderNet.predictAgeAndGender(imgElAngry)\n        })\n      })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple image elements","suites":["predictExpressions"],"line":162,"updatePoint":{"line":162,"column":33},"code":"      it('multiple image elements', async () => {\n        await expectAllTensorsReleased(async () => {\n          await ageGenderNet.predictAgeAndGender([imgElAngry, imgElAngry, imgElAngry])\n        })\n      })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"single tf.Tensor3D","suites":["predictExpressions"],"line":168,"updatePoint":{"line":168,"column":28},"code":"      it('single tf.Tensor3D', async () => {\n        const tensor = tf.browser.fromPixels(createCanvasFromMedia(imgElAngry))\n\n        await expectAllTensorsReleased(async () => {\n          await ageGenderNet.predictAgeAndGender(tensor)\n        })\n\n        tensor.dispose()\n      })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple tf.Tensor3Ds","suites":["predictExpressions"],"line":178,"updatePoint":{"line":178,"column":31},"code":"      it('multiple tf.Tensor3Ds', async () => {\n        const tensors = [imgElAngry, imgElAngry, imgElAngry].map(el => tf.browser.fromPixels(createCanvasFromMedia(el)))\n\n\n        await expectAllTensorsReleased(async () => {\n          await ageGenderNet.predictAgeAndGender(tensors)\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"single batch size 1 tf.Tensor4Ds","suites":["predictExpressions"],"line":189,"updatePoint":{"line":189,"column":42},"code":"      it('single batch size 1 tf.Tensor4Ds', async () => {\n        const tensor = tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(imgElAngry)).expandDims()) as tf.Tensor4D\n\n        await expectAllTensorsReleased(async () => {\n          await ageGenderNet.predictAgeAndGender(tensor)\n        })\n\n        tensor.dispose()\n      })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple batch size 1 tf.Tensor4Ds","suites":["predictExpressions"],"line":199,"updatePoint":{"line":199,"column":44},"code":"      it('multiple batch size 1 tf.Tensor4Ds', async () => {\n        const tensors = [imgElAngry, imgElAngry, imgElAngry]\n          .map(el => tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(el)).expandDims())) as tf.Tensor4D[]\n\n        await expectAllTensorsReleased(async () => {\n          await ageGenderNet.predictAgeAndGender(tensors)\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/ageGenderNet/ageGenderNet.test.ts","skipped":false,"dir":"test"},{"name":"properties","suites":["BoundingBox","constructor"],"line":7,"updatePoint":{"line":7,"column":18},"code":"    it('properties', () => {\n      const box = new BoundingBox(5, 10, 15, 20)\n      expect(box.left).toEqual(5)\n      expect(box.x).toEqual(5)\n      expect(box.top).toEqual(10)\n      expect(box.y).toEqual(10)\n      expect(box.right).toEqual(15)\n      expect(box.bottom).toEqual(20)\n      expect(box.width).toEqual(10)\n      expect(box.height).toEqual(10)\n      expect(box.area).toEqual(100)\n    })","file":"tests/classes/BoundingBox.test.ts","skipped":false,"dir":"test"},{"name":"properties","suites":["BoundingBox","constructor","from IBoundingBox"],"line":9,"updatePoint":{"line":9,"column":20},"code":"      it('properties', () => {\n\n        const box = new Box({ left: 5, top: 10, right: 15, bottom: 20 })\n\n        expect(box.left).toEqual(5)\n        expect(box.x).toEqual(5)\n        expect(box.top).toEqual(10)\n        expect(box.y).toEqual(10)\n        expect(box.right).toEqual(15)\n        expect(box.bottom).toEqual(20)\n        expect(box.width).toEqual(10)\n        expect(box.height).toEqual(10)\n        expect(box.area).toEqual(100)\n      })","file":"tests/classes/Box.test.ts","skipped":false,"dir":"test"},{"name":"properties","suites":["BoundingBox","constructor","from IRect"],"line":28,"updatePoint":{"line":28,"column":20},"code":"      it('properties', () => {\n\n        const box = new Box({ x: 5, y: 10, width: 15, height: 20 })\n\n        expect(box.left).toEqual(5)\n        expect(box.x).toEqual(5)\n        expect(box.top).toEqual(10)\n        expect(box.y).toEqual(10)\n        expect(box.right).toEqual(20)\n        expect(box.bottom).toEqual(30)\n        expect(box.width).toEqual(15)\n        expect(box.height).toEqual(20)\n        expect(box.area).toEqual(300)\n      })","file":"tests/classes/Box.test.ts","skipped":false,"dir":"test"},{"name":"scale down by factor 0.5","suites":["BoundingBox","rescale"],"line":49,"updatePoint":{"line":49,"column":32},"code":"    it('scale down by factor 0.5', () => {\n\n      const box = new Box({ x: 10, y: 20, width: 20, height: 40 })\n\n      const rescaled = box.rescale(0.5)\n\n      expect(rescaled.x).toEqual(5)\n      expect(rescaled.y).toEqual(10)\n      expect(rescaled.width).toEqual(10)\n      expect(rescaled.height).toEqual(20)\n\n    })","file":"tests/classes/Box.test.ts","skipped":false,"dir":"test"},{"name":"scale up by factor 2","suites":["BoundingBox","rescale"],"line":62,"updatePoint":{"line":62,"column":28},"code":"    it('scale up by factor 2', () => {\n\n      const box = new Box({ x: 10, y: 20, width: 20, height: 40 })\n\n      const rescaled = box.rescale(2)\n\n      expect(rescaled.x).toEqual(20)\n      expect(rescaled.y).toEqual(40)\n      expect(rescaled.width).toEqual(40)\n      expect(rescaled.height).toEqual(80)\n\n    })","file":"tests/classes/Box.test.ts","skipped":false,"dir":"test"},{"name":"scale to dimensions ","suites":["BoundingBox","rescale"],"line":75,"updatePoint":{"line":75,"column":28},"code":"    it('scale to dimensions ', () => {\n\n      const box = new Box({ x: 0.1, y: 0.2, width: 0.2, height: 0.4 })\n\n      const rescaled = box.rescale({ width: 100, height: 200 })\n\n      expect(rescaled.x).toEqual(10)\n      expect(rescaled.y).toEqual(40)\n      expect(rescaled.width).toEqual(20)\n      expect(rescaled.height).toEqual(80)\n\n    })","file":"tests/classes/Box.test.ts","skipped":false,"dir":"test"},{"name":"should shift box by x, y","suites":["BoundingBox","shift"],"line":92,"updatePoint":{"line":92,"column":32},"code":"    it('should shift box by x, y', () => {\n\n      const box = new Box({ x: 10, y: 20, width: 20, height: 40 })\n\n      const shifted = box.shift(20, 40)\n\n      expect(shifted.x).toEqual(30)\n      expect(shifted.y).toEqual(60)\n      expect(shifted.width).toEqual(20)\n      expect(shifted.height).toEqual(40)\n\n    })","file":"tests/classes/Box.test.ts","skipped":false,"dir":"test"},{"name":"JSON.stringify()","suites":["globalApi","LabeledFaceDescriptors"],"line":12,"updatePoint":{"line":12,"column":24},"code":"    it('JSON.stringify()', () => {\n      expect(JSON.stringify(new LabeledFaceDescriptors(l1, [f1,f2]))).toBe(json);\n      expect(JSON.stringify({ ld: new LabeledFaceDescriptors(l1, [f1,f2]) })).toBe(`{\"ld\":${json}}`);\n      expect(JSON.stringify([ new LabeledFaceDescriptors(l1, [f1,f2]) ])).toBe(`[${json}]`);\n    });","file":"tests/classes/LabeledFaceDescriptors.test.ts","skipped":false,"dir":"test"},{"name":"fromJSON()","suites":["globalApi","LabeledFaceDescriptors"],"line":18,"updatePoint":{"line":18,"column":18},"code":"    it('fromJSON()', () => {\n      const ld = LabeledFaceDescriptors.fromJSON(JSON.parse(json));\n\n      expect(ld.label).toBe(l1);\n      expect(ld.descriptors.length).toBe(2);\n      expect(ld.descriptors[0]).toEqual(f1);\n      expect(ld.descriptors[1]).toEqual(f2);\n    });","file":"tests/classes/LabeledFaceDescriptors.test.ts","skipped":false,"dir":"test"},{"name":"toJSON() => fromJSON()","suites":["globalApi","LabeledFaceDescriptors"],"line":27,"updatePoint":{"line":27,"column":30},"code":"    it('toJSON() => fromJSON()', () => {\n      const ld = LabeledFaceDescriptors.fromJSON(new LabeledFaceDescriptors(l1, [f1,f2]).toJSON());\n\n      expect(ld.label).toBe(l1);\n      expect(ld.descriptors.length).toBe(2);\n      expect(ld.descriptors[0]).toEqual(f1);\n      expect(ld.descriptors[1]).toEqual(f2);\n    });","file":"tests/classes/LabeledFaceDescriptors.test.ts","skipped":false,"dir":"test"},{"name":"can be created","suites":["Rect","constructor"],"line":8,"updatePoint":{"line":8,"column":22},"code":"    it('can be created', () => {\n      const rect = new Rect(0, 10, 20, 30)\n      expect(rect instanceof Rect).toBe(true)\n      expect(rect instanceof Box).toBe(true)\n      expect(rect.x).toEqual(0)\n      expect(rect.y).toEqual(10)\n      expect(rect.width).toEqual(20)\n      expect(rect.height).toEqual(30)\n    })","file":"tests/classes/Rect.test.ts","skipped":false,"dir":"test"},{"name":"throws if coordinates are invalid","suites":["Rect","constructor"],"line":18,"updatePoint":{"line":18,"column":41},"code":"    it('throws if coordinates are invalid', () => {\n\n      const expectConstructorToThrow = (x: any, y: any, width: any, height: any) => {\n        expect(() => new Rect(x, y, width, height)).toThrowError(`Box.constructor - expected box to be IBoundingBox | IRect, instead have ${JSON.stringify({ x, y, width, height })}`)\n      }\n\n      expectConstructorToThrow(NaN, 10, 20, 30)\n      expectConstructorToThrow(0, Infinity, 20, 30)\n      expectConstructorToThrow(0, 10, -Infinity, 30)\n      expectConstructorToThrow(0, 10, 20, null)\n      expectConstructorToThrow(NaN, Infinity, undefined, null)\n      expectConstructorToThrow(undefined, undefined, undefined, undefined)\n    })","file":"tests/classes/Rect.test.ts","skipped":false,"dir":"test"},{"name":"throws if height or width invalid","suites":["Rect","constructor"],"line":32,"updatePoint":{"line":32,"column":41},"code":"    it('throws if height or width invalid', () => {\n      expect(() => new Rect(0, 10, -20, 30, false)).toThrowError('Box.constructor - width (-20) and height (30) must be positive numbers')\n      expect(() => new Rect(0, 10, 20, -30, false)).toThrowError('Box.constructor - width (20) and height (-30) must be positive numbers')\n    })","file":"tests/classes/Rect.test.ts","skipped":false,"dir":"test"},{"name":"properties","suites":["Rect","constructor"],"line":37,"updatePoint":{"line":37,"column":18},"code":"    it('properties', () => {\n      const rect = new Rect(5, 10, 15, 20)\n      expect(rect.left).toEqual(5)\n      expect(rect.x).toEqual(5)\n      expect(rect.top).toEqual(10)\n      expect(rect.y).toEqual(10)\n      expect(rect.right).toEqual(20)\n      expect(rect.bottom).toEqual(30)\n      expect(rect.width).toEqual(15)\n      expect(rect.height).toEqual(20)\n      expect(rect.area).toEqual(300)\n    })","file":"tests/classes/Rect.test.ts","skipped":false,"dir":"test"},{"name":"returns uris from relative url if no argument passed","suites":["getModelUris"],"line":7,"updatePoint":{"line":7,"column":58},"code":"  it('returns uris from relative url if no argument passed', () => {\n    const result = getModelUris(undefined, FAKE_DEFAULT_MODEL_NAME)\n\n    expect(result.manifestUri).toEqual(`${FAKE_DEFAULT_MODEL_NAME}-weights_manifest.json`)\n    expect(result.modelBaseUri).toEqual('')\n  })","file":"tests/common/getModelUris.test.ts","skipped":false,"dir":"test"},{"name":"returns uris from relative url for empty string","suites":["getModelUris"],"line":14,"updatePoint":{"line":14,"column":53},"code":"  it('returns uris from relative url for empty string', () => {\n    const result = getModelUris('', FAKE_DEFAULT_MODEL_NAME)\n\n    expect(result.manifestUri).toEqual(`${FAKE_DEFAULT_MODEL_NAME}-weights_manifest.json`)\n    expect(result.modelBaseUri).toEqual('')\n  })","file":"tests/common/getModelUris.test.ts","skipped":false,"dir":"test"},{"name":"returns uris for top level url, leading slash preserved","suites":["getModelUris"],"line":21,"updatePoint":{"line":21,"column":61},"code":"  it('returns uris for top level url, leading slash preserved', () => {\n    const result = getModelUris('/', FAKE_DEFAULT_MODEL_NAME)\n\n    expect(result.manifestUri).toEqual(`/${FAKE_DEFAULT_MODEL_NAME}-weights_manifest.json`)\n    expect(result.modelBaseUri).toEqual('/')\n  })","file":"tests/common/getModelUris.test.ts","skipped":false,"dir":"test"},{"name":"returns uris, given url path","suites":["getModelUris"],"line":28,"updatePoint":{"line":28,"column":34},"code":"  it('returns uris, given url path', () => {\n    const uri = 'path/to/modelfiles'\n    const result = getModelUris(uri, FAKE_DEFAULT_MODEL_NAME)\n\n    expect(result.manifestUri).toEqual(`${uri}/${FAKE_DEFAULT_MODEL_NAME}-weights_manifest.json`)\n    expect(result.modelBaseUri).toEqual(uri)\n  })","file":"tests/common/getModelUris.test.ts","skipped":false,"dir":"test"},{"name":"returns uris, given url path, leading slash preserved","suites":["getModelUris"],"line":36,"updatePoint":{"line":36,"column":59},"code":"  it('returns uris, given url path, leading slash preserved', () => {\n    const uri = '/path/to/modelfiles'\n    const result = getModelUris(`/${uri}`, FAKE_DEFAULT_MODEL_NAME)\n\n    expect(result.manifestUri).toEqual(`${uri}/${FAKE_DEFAULT_MODEL_NAME}-weights_manifest.json`)\n    expect(result.modelBaseUri).toEqual(uri)\n  })","file":"tests/common/getModelUris.test.ts","skipped":false,"dir":"test"},{"name":"returns uris, given manifest uri","suites":["getModelUris"],"line":44,"updatePoint":{"line":44,"column":38},"code":"  it('returns uris, given manifest uri', () => {\n    const uri = 'path/to/modelfiles/model-weights_manifest.json'\n    const result = getModelUris(uri, FAKE_DEFAULT_MODEL_NAME)\n\n    expect(result.manifestUri).toEqual(uri)\n    expect(result.modelBaseUri).toEqual('path/to/modelfiles')\n  })","file":"tests/common/getModelUris.test.ts","skipped":false,"dir":"test"},{"name":"returns uris, given manifest uri, leading slash preserved","suites":["getModelUris"],"line":52,"updatePoint":{"line":52,"column":63},"code":"  it('returns uris, given manifest uri, leading slash preserved', () => {\n    const uri = '/path/to/modelfiles/model-weights_manifest.json'\n    const result = getModelUris(uri, FAKE_DEFAULT_MODEL_NAME)\n\n    expect(result.manifestUri).toEqual(uri)\n    expect(result.modelBaseUri).toEqual('/path/to/modelfiles')\n  })","file":"tests/common/getModelUris.test.ts","skipped":false,"dir":"test"},{"name":"returns correct uris, given external path","suites":["getModelUris"],"line":60,"updatePoint":{"line":60,"column":47},"code":"  it('returns correct uris, given external path', () => {\n    const uri = 'https://example.com/path/to/modelfiles';\n    const result = getModelUris(uri, FAKE_DEFAULT_MODEL_NAME)\n\n    expect(result.manifestUri).toEqual(`${uri}/${FAKE_DEFAULT_MODEL_NAME}-weights_manifest.json`)\n    expect(result.modelBaseUri).toEqual(uri)\n  })","file":"tests/common/getModelUris.test.ts","skipped":false,"dir":"test"},{"name":"HTMLImageElement, single box","suites":["extractFaces","extracts canvases"],"line":16,"updatePoint":{"line":16,"column":36},"code":"    it('HTMLImageElement, single box', async () => {\n      const rect = new Rect(0, 0, 50, 60)\n      const canvases = await extractFaces(imgEl, [rect])\n\n      expect(canvases.length).toEqual(1)\n      expect(canvases[0] instanceof Canvas).toBe(true)\n      expect(canvases[0].width).toEqual(50)\n      expect(canvases[0].height).toEqual(60)\n    })","file":"tests/dom/extractFaces.test.ts","skipped":false,"dir":"test"},{"name":"HTMLImageElement, multiple boxes","suites":["extractFaces","extracts canvases"],"line":26,"updatePoint":{"line":26,"column":40},"code":"    it('HTMLImageElement, multiple boxes', async () => {\n      const rects = [\n        new Rect(0, 0, 50, 60),\n        new Rect(50, 50, 70, 80),\n      ]\n      const canvases = await extractFaces(imgEl, rects)\n\n      expect(canvases.length).toEqual(2)\n      expect(canvases[0] instanceof Canvas).toBe(true)\n      expect(canvases[0].width).toEqual(50)\n      expect(canvases[0].height).toEqual(60)\n      expect(canvases[1] instanceof Canvas).toBe(true)\n      expect(canvases[1].width).toEqual(70)\n      expect(canvases[1].height).toEqual(80)\n    })","file":"tests/dom/extractFaces.test.ts","skipped":false,"dir":"test"},{"name":"HTMLCanvasElement, single box","suites":["extractFaces","extracts canvases"],"line":42,"updatePoint":{"line":42,"column":37},"code":"    it('HTMLCanvasElement, single box', async () => {\n      const rect = new Rect(0, 0, 50, 60)\n      const canvases = await extractFaces(canvasEl, [rect])\n\n      expect(canvases.length).toEqual(1)\n      expect(canvases[0] instanceof Canvas).toBe(true)\n      expect(canvases[0].width).toEqual(50)\n      expect(canvases[0].height).toEqual(60)\n    })","file":"tests/dom/extractFaces.test.ts","skipped":false,"dir":"test"},{"name":"HTMLCanvasElement, multiple boxes","suites":["extractFaces","extracts canvases"],"line":52,"updatePoint":{"line":52,"column":41},"code":"    it('HTMLCanvasElement, multiple boxes', async () => {\n      const rects = [\n        new Rect(0, 0, 50, 60),\n        new Rect(50, 50, 70, 80),\n      ]\n      const canvases = await extractFaces(canvasEl, rects)\n\n      expect(canvases.length).toEqual(2)\n      expect(canvases[0] instanceof Canvas).toBe(true)\n      expect(canvases[0].width).toEqual(50)\n      expect(canvases[0].height).toEqual(60)\n      expect(canvases[1] instanceof Canvas).toBe(true)\n      expect(canvases[1].width).toEqual(70)\n      expect(canvases[1].height).toEqual(80)\n    })","file":"tests/dom/extractFaces.test.ts","skipped":false,"dir":"test"},{"name":"clips upper left corner","suites":["extractFaces","box out of image borders"],"line":72,"updatePoint":{"line":72,"column":31},"code":"    it('clips upper left corner', async () => {\n      const rect = new Rect(-10, -10, 110, 110)\n      const canvases = await extractFaces(imgEl, [rect])\n\n      expect(canvases[0].width).toEqual(100)\n      expect(canvases[0].height).toEqual(100)\n    })","file":"tests/dom/extractFaces.test.ts","skipped":false,"dir":"test"},{"name":"clips bottom right corner","suites":["extractFaces","box out of image borders"],"line":80,"updatePoint":{"line":80,"column":33},"code":"    it('clips bottom right corner', async () => {\n      const rect = new Rect(imgEl.width - 100, imgEl.height - 100, 110, 110)\n      const canvases = await extractFaces(imgEl, [rect])\n\n      expect(canvases[0].width).toEqual(100)\n      expect(canvases[0].height).toEqual(100)\n    })","file":"tests/dom/extractFaces.test.ts","skipped":false,"dir":"test"},{"name":"clips upper left and bottom right corners","suites":["extractFaces","box out of image borders"],"line":88,"updatePoint":{"line":88,"column":49},"code":"    it('clips upper left and bottom right corners', async () => {\n      const rect = new Rect(-10, -10, imgEl.width + 20, imgEl.height + 20)\n      const canvases = await extractFaces(imgEl, [rect])\n\n      expect(canvases[0].width).toEqual(imgEl.width)\n      expect(canvases[0].height).toEqual(imgEl.height)\n    })","file":"tests/dom/extractFaces.test.ts","skipped":false,"dir":"test"},{"name":"single box","suites":["extracts tensors"],"line":15,"updatePoint":{"line":15,"column":18},"code":"    it('single box', async () => {\n      const rect = new Rect(0, 0, 50, 60)\n      const tensors = await extractFaceTensors(imgTensor, [rect])\n\n      expect(tensors.length).toEqual(1)\n      expect(tensors[0].shape).toEqual([60, 50, 3])\n      tensors[0].dispose()\n    })","file":"tests/dom/extractFaceTensors.test.ts","skipped":false,"dir":"test"},{"name":"multiple boxes","suites":["extracts tensors"],"line":24,"updatePoint":{"line":24,"column":22},"code":"    it('multiple boxes', async () => {\n      const rects = [\n        new Rect(0, 0, 50, 60),\n        new Rect(50, 50, 70, 80),\n      ]\n      const tensors = await extractFaceTensors(imgTensor, rects)\n\n      expect(tensors.length).toEqual(2)\n      expect(tensors[0].shape).toEqual([60, 50, 3])\n      expect(tensors[1].shape).toEqual([80, 70, 3])\n      tensors[0].dispose()\n      tensors[1].dispose()\n    })","file":"tests/dom/extractFaceTensors.test.ts","skipped":false,"dir":"test"},{"name":"clips upper left corner","suites":["box out of image borders"],"line":42,"updatePoint":{"line":42,"column":31},"code":"    it('clips upper left corner', async () => {\n      const rect = new Rect(-10, -10, 110, 110)\n      const tensors = await extractFaceTensors(imgTensor, [rect])\n\n      expect(tensors[0].shape).toEqual([100, 100, 3])\n      tensors[0].dispose()\n    })","file":"tests/dom/extractFaceTensors.test.ts","skipped":false,"dir":"test"},{"name":"clips bottom right corner","suites":["box out of image borders"],"line":50,"updatePoint":{"line":50,"column":33},"code":"    it('clips bottom right corner', async () => {\n      const rect = new Rect(imgTensor.shape[1] - 100, imgTensor.shape[0] - 100, 110, 110)\n      const tensors = await extractFaceTensors(imgTensor, [rect])\n\n      expect(tensors[0].shape).toEqual([100, 100, 3])\n      tensors[0].dispose()\n    })","file":"tests/dom/extractFaceTensors.test.ts","skipped":false,"dir":"test"},{"name":"clips upper left and bottom right corners","suites":["box out of image borders"],"line":58,"updatePoint":{"line":58,"column":49},"code":"    it('clips upper left and bottom right corners', async () => {\n      const rect = new Rect(-10, -10, imgTensor.shape[1] + 20, imgTensor.shape[0] + 20)\n      const tensors = await extractFaceTensors(imgTensor, [rect])\n\n      expect(tensors[0].shape).toEqual([imgTensor.shape[1], imgTensor.shape[0], 3])\n      tensors[0].dispose()\n    })","file":"tests/dom/extractFaceTensors.test.ts","skipped":false,"dir":"test"},{"name":"invalid mime type","suites":["fetchImage"],"line":5,"updatePoint":{"line":5,"column":23},"code":"  it('invalid mime type', async () => {\n    const url = 'base/test/data/boxes.json'\n\n    let err = ''\n    try {\n      await fetchImage(url)\n    } catch (e) {\n      err = e.toString()\n    }\n\n    expect(err).toContain('fetchImage - expected blob type to be of type image/*, instead have: application/json')\n    expect(err).toContain(url)\n  })","file":"tests/dom/fetchImage.browser.test.ts","skipped":false,"dir":"test"},{"name":"fetches image","suites":["fetchImage"],"line":19,"updatePoint":{"line":19,"column":19},"code":"  it('fetches image', async () => {\n    const url = 'base/test/images/white.png'\n    const img = await fetchImage(url)\n    expect(img instanceof HTMLImageElement).toBe(true)\n  })","file":"tests/dom/fetchImage.browser.test.ts","skipped":false,"dir":"test"},{"name":"fetches json","suites":["fetchJson"],"line":5,"updatePoint":{"line":5,"column":18},"code":"  it('fetches json', async () => {\n    const url = 'test/data/boxes.json'\n    expect(async () => await fetchJson(url)).not.toThrow()\n  })","file":"tests/dom/fetchJson.browser.test.ts","skipped":false,"dir":"test"},{"name":"fetches .weights file","suites":["fetchNetWeights"],"line":5,"updatePoint":{"line":5,"column":27},"code":"  it('fetches .weights file', async () => {\n    const url = 'base/test/data/dummy.weights'\n    const weights = await fetchNetWeights(url)\n    expect(weights instanceof Float32Array).toBe(true)\n  })","file":"tests/dom/fetchNetWeights.browser.test.ts","skipped":false,"dir":"test"},{"name":"404, throws","suites":["fetchOrThrow"],"line":5,"updatePoint":{"line":5,"column":17},"code":"  it('404, throws', async () => {\n    const url = '/does/not/exist'\n\n    let err = ''\n    try {\n      await fetchOrThrow(url)\n    } catch (e) {\n      err = e.toString()\n    }\n\n    expect(err).toContain('failed to fetch: (404)')\n    expect(err).toContain(url)\n  })","file":"tests/dom/fetchOrThrow.browser.test.ts","skipped":false,"dir":"test"},{"name":"HTMLImageElement, batchSize === 1","suites":["NetInput","toBatchTensor"],"line":17,"updatePoint":{"line":17,"column":41},"code":"    it('HTMLImageElement, batchSize === 1', () => tf.tidy(() => {\n      const netInput = new NetInput([imgEl])\n      const batchTensor = netInput.toBatchTensor(100)\n      expect(batchTensor.shape).toEqual([1, 100, 100, 3])\n    }))","file":"tests/dom/NetInput.test.ts","skipped":false,"dir":"test"},{"name":"tf.Tensor3D, batchSize === 1","suites":["NetInput","toBatchTensor"],"line":23,"updatePoint":{"line":23,"column":36},"code":"    it('tf.Tensor3D, batchSize === 1', () => tf.tidy(() => {\n      const tensor = tf.zeros<tf.Rank.R3>([200, 200, 3], 'int32')\n      const netInput = new NetInput([tensor])\n      const batchTensor = netInput.toBatchTensor(100)\n      expect(batchTensor.shape).toEqual([1, 100, 100, 3])\n    }))","file":"tests/dom/NetInput.test.ts","skipped":false,"dir":"test"},{"name":"HTMLImageElements, batchSize === 4","suites":["NetInput","toBatchTensor"],"line":30,"updatePoint":{"line":30,"column":42},"code":"    it('HTMLImageElements, batchSize === 4', () => tf.tidy(() => {\n      const netInput = new NetInput([imgEl, imgEl, imgEl, imgEl])\n      const batchTensor = netInput.toBatchTensor(100)\n      expect(batchTensor.shape).toEqual([4, 100, 100, 3])\n    }))","file":"tests/dom/NetInput.test.ts","skipped":false,"dir":"test"},{"name":"tf.Tensor3Ds, batchSize === 4","suites":["NetInput","toBatchTensor"],"line":36,"updatePoint":{"line":36,"column":37},"code":"    it('tf.Tensor3Ds, batchSize === 4', () => tf.tidy(() => {\n      const tensor = tf.zeros<tf.Rank.R3>([200, 200, 3], 'int32')\n      const netInput = new NetInput([tensor, tensor, tensor, tensor])\n      const batchTensor = netInput.toBatchTensor(100)\n      expect(batchTensor.shape).toEqual([4, 100, 100, 3])\n    }))","file":"tests/dom/NetInput.test.ts","skipped":false,"dir":"test"},{"name":"tf.Tensor3Ds and HTMLImageElements, batchSize === 4","suites":["NetInput","toBatchTensor"],"line":43,"updatePoint":{"line":43,"column":59},"code":"    it('tf.Tensor3Ds and HTMLImageElements, batchSize === 4', () => tf.tidy(() => {\n      const tensor = tf.zeros<tf.Rank.R3>([200, 200, 3], 'int32')\n      const netInput = new NetInput([tensor, tensor, imgEl, imgEl])\n      const batchTensor = netInput.toBatchTensor(100)\n      expect(batchTensor.shape).toEqual([4, 100, 100, 3])\n    }))","file":"tests/dom/NetInput.test.ts","skipped":false,"dir":"test"},{"name":"constructor","suites":["NetInput","no memory leaks"],"line":54,"updatePoint":{"line":54,"column":19},"code":"    it('constructor', async () => {\n      const tensors = [fakeTensor3d(), fakeTensor3d(), fakeTensor3d()]\n\n      await expectAllTensorsReleased(() => {\n        new NetInput([imgEl])\n        new NetInput([imgEl, imgEl, imgEl])\n        new NetInput([tensors[0]])\n        new NetInput(tensors)\n      })\n\n      tensors.forEach(t => t.dispose())\n    })","file":"tests/dom/NetInput.test.ts","skipped":false,"dir":"test"},{"name":"single image element","suites":["NetInput","no memory leaks","toBatchTensor"],"line":69,"updatePoint":{"line":69,"column":30},"code":"      it('single image element', async () => {\n        await expectAllTensorsReleased(() => {\n          const batchTensor = new NetInput([imgEl]).toBatchTensor(100, false)\n          batchTensor.dispose()\n        })\n      })","file":"tests/dom/NetInput.test.ts","skipped":false,"dir":"test"},{"name":"multiple image elements","suites":["NetInput","no memory leaks","toBatchTensor"],"line":76,"updatePoint":{"line":76,"column":33},"code":"      it('multiple image elements', async () => {\n        await expectAllTensorsReleased(() => {\n          const batchTensor = new NetInput([imgEl, imgEl, imgEl]).toBatchTensor(100, false)\n          batchTensor.dispose()\n        })\n      })","file":"tests/dom/NetInput.test.ts","skipped":false,"dir":"test"},{"name":"from HTMLImageElement","suites":["toNetInput","valid args"],"line":19,"updatePoint":{"line":19,"column":29},"code":"    it('from HTMLImageElement', async () => {\n      const netInput = await toNetInput(imgEl)\n      expect(netInput instanceof NetInput).toBe(true)\n      expect(netInput.batchSize).toEqual(1)\n    })","file":"tests/dom/toNetInput.test.ts","skipped":false,"dir":"test"},{"name":"from HTMLCanvasElement","suites":["toNetInput","valid args"],"line":25,"updatePoint":{"line":25,"column":30},"code":"    it('from HTMLCanvasElement', async () => {\n      const netInput = await toNetInput(canvasEl)\n      expect(netInput instanceof NetInput).toBe(true)\n      expect(netInput.batchSize).toEqual(1)\n    })","file":"tests/dom/toNetInput.test.ts","skipped":false,"dir":"test"},{"name":"from HTMLImageElement array","suites":["toNetInput","valid args"],"line":31,"updatePoint":{"line":31,"column":35},"code":"    it('from HTMLImageElement array', async () => {\n      const netInput = await toNetInput([\n        imgEl,\n        imgEl\n      ])\n      expect(netInput instanceof NetInput).toBe(true)\n      expect(netInput.batchSize).toEqual(2)\n    })","file":"tests/dom/toNetInput.test.ts","skipped":false,"dir":"test"},{"name":"from HTMLCanvasElement array","suites":["toNetInput","valid args"],"line":40,"updatePoint":{"line":40,"column":36},"code":"    it('from HTMLCanvasElement array', async () => {\n      const netInput = await toNetInput([\n        canvasEl,\n        canvasEl\n      ])\n      expect(netInput instanceof NetInput).toBe(true)\n      expect(netInput.batchSize).toEqual(2)\n    })","file":"tests/dom/toNetInput.test.ts","skipped":false,"dir":"test"},{"name":"from mixed media array","suites":["toNetInput","valid args"],"line":49,"updatePoint":{"line":49,"column":30},"code":"    it('from mixed media array', async () => {\n      const netInput = await toNetInput([\n        imgEl,\n        canvasEl,\n        canvasEl\n      ])\n      expect(netInput instanceof NetInput).toBe(true)\n      expect(netInput.batchSize).toEqual(3)\n    })","file":"tests/dom/toNetInput.test.ts","skipped":false,"dir":"test"},{"name":"undefined","suites":["toNetInput","invalid args"],"line":62,"updatePoint":{"line":62,"column":17},"code":"    it('undefined', async () => {\n      let errorMessage\n      try {\n        await toNetInput(undefined as any)\n      } catch (error) {\n          errorMessage = error.message;\n      }\n      expect(errorMessage).toBe('toNetInput - expected media to be of type HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | tf.Tensor3D, or to be an element id')\n    })","file":"tests/dom/toNetInput.test.ts","skipped":false,"dir":"test"},{"name":"empty array","suites":["toNetInput","invalid args"],"line":72,"updatePoint":{"line":72,"column":19},"code":"    it('empty array', async () => {\n      let errorMessage\n      try {\n        await toNetInput([])\n      } catch (error) {\n          errorMessage = error.message;\n      }\n      expect(errorMessage).toBe('toNetInput - empty array passed as input')\n    })","file":"tests/dom/toNetInput.test.ts","skipped":false,"dir":"test"},{"name":"undefined at input index 1","suites":["toNetInput","invalid args"],"line":82,"updatePoint":{"line":82,"column":34},"code":"    it('undefined at input index 1', async () => {\n      let errorMessage\n      try {\n        await toNetInput([env.getEnv().createImageElement(), undefined] as any)\n      } catch (error) {\n          errorMessage = error.message;\n      }\n      expect(errorMessage).toBe('toNetInput - at input index 1: expected media to be of type HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | tf.Tensor3D, or to be an element id')\n    })","file":"tests/dom/toNetInput.test.ts","skipped":false,"dir":"test"},{"name":"constructor","suites":["toNetInput","no memory leaks"],"line":96,"updatePoint":{"line":96,"column":19},"code":"    it('constructor', async () => {\n      const tensors = [imgEl, imgEl, imgEl].map(el => tf.browser.fromPixels(createCanvasFromMedia(el)))\n      const tensor4ds = tensors.map(t => t.expandDims<tf.Rank.R4>())\n\n      await expectAllTensorsReleased(async () => {\n        await toNetInput(imgEl)\n        await toNetInput([imgEl, imgEl, imgEl])\n        await toNetInput(tensors[0])\n        await toNetInput(tensors)\n        await toNetInput(tensor4ds[0])\n        await toNetInput(tensor4ds)\n      })\n\n      tensors.forEach(t => t.dispose())\n      tensor4ds.forEach(t => t.dispose())\n    })","file":"tests/dom/toNetInput.test.ts","skipped":false,"dir":"test"},{"name":"recognizes facial expressions","suites":[],"line":20,"updatePoint":{"line":20,"column":37},"code":"    it('recognizes facial expressions', async () => {\n      const result = await faceExpressionNet.predictExpressions(imgElAngry) as FaceExpressions\n      expect(result instanceof FaceExpressions).toBe(true)\n      expect(result.angry).toBeGreaterThan(0.95)\n    })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"recognizes facial expressions for batch of image elements","suites":[],"line":30,"updatePoint":{"line":30,"column":65},"code":"    it('recognizes facial expressions for batch of image elements', async () => {\n      const inputs = [imgElAngry, imgElSurprised]\n\n      const results = await faceExpressionNet.predictExpressions(inputs) as FaceExpressions[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(2)\n\n      const [resultAngry, resultSurprised] = results\n      expect(resultAngry instanceof FaceExpressions).toBe(true)\n      expect(resultSurprised instanceof FaceExpressions).toBe(true)\n      expect(resultAngry.angry).toBeGreaterThan(0.95)\n      expect(resultSurprised.surprised).toBeGreaterThan(0.95)\n    })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"computes face expressions for batch of tf.Tensor3D","suites":[],"line":44,"updatePoint":{"line":44,"column":58},"code":"    it('computes face expressions for batch of tf.Tensor3D', async () => {\n      const inputs = [imgElAngry, imgElSurprised].map(el => tf.browser.fromPixels(createCanvasFromMedia(el)))\n\n      const results = await faceExpressionNet.predictExpressions(inputs) as FaceExpressions[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(2)\n\n      const [resultAngry, resultSurprised] = results\n      expect(resultAngry instanceof FaceExpressions).toBe(true)\n      expect(resultSurprised instanceof FaceExpressions).toBe(true)\n      expect(resultAngry.angry).toBeGreaterThan(0.95)\n      expect(resultSurprised.surprised).toBeGreaterThan(0.95)\n    })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"computes face expressions for batch of mixed inputs","suites":[],"line":58,"updatePoint":{"line":58,"column":59},"code":"    it('computes face expressions for batch of mixed inputs', async () => {\n      const inputs = [imgElAngry, tf.browser.fromPixels(createCanvasFromMedia(imgElSurprised))]\n\n      const results = await faceExpressionNet.predictExpressions(inputs) as FaceExpressions[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(2)\n\n      const [resultAngry, resultSurprised] = results\n      expect(resultAngry instanceof FaceExpressions).toBe(true)\n      expect(resultSurprised instanceof FaceExpressions).toBe(true)\n      expect(resultAngry.angry).toBeGreaterThan(0.95)\n      expect(resultSurprised.surprised).toBeGreaterThan(0.95)\n    })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"single image element","suites":["forwardInput"],"line":78,"updatePoint":{"line":78,"column":30},"code":"      it('single image element', async () => {\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput([imgElAngry])\n          const outTensor = await faceExpressionNet.forwardInput(netInput)\n          outTensor.dispose()\n        })\n      })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple image elements","suites":["forwardInput"],"line":86,"updatePoint":{"line":86,"column":33},"code":"      it('multiple image elements', async () => {\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput([imgElAngry, imgElAngry])\n          const outTensor = await faceExpressionNet.forwardInput(netInput)\n          outTensor.dispose()\n        })\n      })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"single tf.Tensor3D","suites":["forwardInput"],"line":94,"updatePoint":{"line":94,"column":28},"code":"      it('single tf.Tensor3D', async () => {\n        const tensor = tf.browser.fromPixels(createCanvasFromMedia(imgElAngry))\n\n        await expectAllTensorsReleased(async () => {\n          const outTensor = await faceExpressionNet.forwardInput(await toNetInput(tensor))\n          outTensor.dispose()\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple tf.Tensor3Ds","suites":["forwardInput"],"line":105,"updatePoint":{"line":105,"column":31},"code":"      it('multiple tf.Tensor3Ds', async () => {\n        const tensors = [imgElAngry, imgElAngry, imgElAngry].map(el => tf.browser.fromPixels(createCanvasFromMedia(el)))\n\n        await expectAllTensorsReleased(async () => {\n          const outTensor = await faceExpressionNet.forwardInput(await toNetInput(tensors))\n          outTensor.dispose()\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"single batch size 1 tf.Tensor4Ds","suites":["forwardInput"],"line":116,"updatePoint":{"line":116,"column":42},"code":"      it('single batch size 1 tf.Tensor4Ds', async () => {\n        const tensor = tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(imgElAngry)).expandDims()) as tf.Tensor4D\n\n        await expectAllTensorsReleased(async () => {\n          const outTensor = await faceExpressionNet.forwardInput(await toNetInput(tensor))\n          outTensor.dispose()\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple batch size 1 tf.Tensor4Ds","suites":["forwardInput"],"line":127,"updatePoint":{"line":127,"column":44},"code":"      it('multiple batch size 1 tf.Tensor4Ds', async () => {\n        const tensors = [imgElAngry, imgElAngry, imgElAngry]\n          .map(el => tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(el)).expandDims())) as tf.Tensor4D[]\n\n        await expectAllTensorsReleased(async () => {\n          const outTensor = await faceExpressionNet.forwardInput(await toNetInput(tensors))\n          outTensor.dispose()\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"single image element","suites":["predictExpressions"],"line":143,"updatePoint":{"line":143,"column":30},"code":"      it('single image element', async () => {\n        await expectAllTensorsReleased(async () => {\n          await faceExpressionNet.predictExpressions(imgElAngry)\n        })\n      })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple image elements","suites":["predictExpressions"],"line":149,"updatePoint":{"line":149,"column":33},"code":"      it('multiple image elements', async () => {\n        await expectAllTensorsReleased(async () => {\n          await faceExpressionNet.predictExpressions([imgElAngry, imgElAngry, imgElAngry])\n        })\n      })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"single tf.Tensor3D","suites":["predictExpressions"],"line":155,"updatePoint":{"line":155,"column":28},"code":"      it('single tf.Tensor3D', async () => {\n        const tensor = tf.browser.fromPixels(createCanvasFromMedia(imgElAngry))\n\n        await expectAllTensorsReleased(async () => {\n          await faceExpressionNet.predictExpressions(tensor)\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple tf.Tensor3Ds","suites":["predictExpressions"],"line":165,"updatePoint":{"line":165,"column":31},"code":"      it('multiple tf.Tensor3Ds', async () => {\n        const tensors = [imgElAngry, imgElAngry, imgElAngry].map(el => tf.browser.fromPixels(createCanvasFromMedia(el)))\n\n\n        await expectAllTensorsReleased(async () => {\n          await faceExpressionNet.predictExpressions(tensors)\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"single batch size 1 tf.Tensor4Ds","suites":["predictExpressions"],"line":176,"updatePoint":{"line":176,"column":42},"code":"      it('single batch size 1 tf.Tensor4Ds', async () => {\n        const tensor = tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(imgElAngry)).expandDims()) as tf.Tensor4D\n\n        await expectAllTensorsReleased(async () => {\n          await faceExpressionNet.predictExpressions(tensor)\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple batch size 1 tf.Tensor4Ds","suites":["predictExpressions"],"line":186,"updatePoint":{"line":186,"column":44},"code":"      it('multiple batch size 1 tf.Tensor4Ds', async () => {\n        const tensors = [imgElAngry, imgElAngry, imgElAngry]\n          .map(el => tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(el)).expandDims())) as tf.Tensor4D[]\n\n        await expectAllTensorsReleased(async () => {\n          await faceExpressionNet.predictExpressions(tensors)\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceExpressionNet/faceExpressionNet.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for squared input","suites":[],"line":41,"updatePoint":{"line":41,"column":49},"code":"    it('computes face landmarks for squared input', async () => {\n      const { width, height } = imgEl1\n\n      const result = await faceLandmark68Net.detectLandmarks(imgEl1) as FaceLandmarks68\n      expect(result.imageWidth).toEqual(width)\n      expect(result.imageHeight).toEqual(height)\n      expect(result.shift.x).toEqual(0)\n      expect(result.shift.y).toEqual(0)\n      result.positions.forEach((pt, i) => {\n        const { x, y } = faceLandmarkPositions1[i]\n        expectPointClose(pt, { x, y }, 3)\n      })\n    })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for rectangular input","suites":[],"line":55,"updatePoint":{"line":55,"column":53},"code":"    it('computes face landmarks for rectangular input', async () => {\n      const { width, height } = imgElRect\n\n      const result = await faceLandmark68Net.detectLandmarks(imgElRect) as FaceLandmarks68\n      expect(result.imageWidth).toEqual(width)\n      expect(result.imageHeight).toEqual(height)\n      expect(result.shift.x).toEqual(0)\n      expect(result.shift.y).toEqual(0)\n      result.positions.forEach((pt, i) => {\n        const { x, y } = faceLandmarkPositionsRect[i]\n        expectPointClose(pt, { x, y }, 6)\n      })\n    })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for batch of image elements","suites":[],"line":73,"updatePoint":{"line":73,"column":59},"code":"    it('computes face landmarks for batch of image elements', async () => {\n      const inputs = [imgEl1, imgEl2, imgElRect]\n\n      const faceLandmarkPositions = [\n        faceLandmarkPositions1,\n        faceLandmarkPositions2,\n        faceLandmarkPositionsRect\n      ]\n\n      const results = await faceLandmark68Net.detectLandmarks(inputs) as FaceLandmarks68[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(3)\n      results.forEach((result, batchIdx) => {\n        const { width, height } = getInputDims(inputs[batchIdx])\n        expect(result.imageWidth).toEqual(width)\n        expect(result.imageHeight).toEqual(height)\n        expect(result.shift.x).toEqual(0)\n        expect(result.shift.y).toEqual(0)\n        result.positions.forEach(({ x, y }, i) => {\n          expectMaxDelta(x, faceLandmarkPositions[batchIdx][i].x, 5)\n          expectMaxDelta(y, faceLandmarkPositions[batchIdx][i].y, 5)\n        })\n      })\n    })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for batch of tf.Tensor3D","suites":[],"line":98,"updatePoint":{"line":98,"column":56},"code":"    it('computes face landmarks for batch of tf.Tensor3D', async () => {\n      const inputs = [imgEl1, imgEl2, imgElRect].map(el => tf.browser.fromPixels(createCanvasFromMedia(el)))\n\n      const faceLandmarkPositions = [\n        faceLandmarkPositions1,\n        faceLandmarkPositions2,\n        faceLandmarkPositionsRect\n      ]\n\n      const results = await faceLandmark68Net.detectLandmarks(inputs) as FaceLandmarks68[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(3)\n      results.forEach((result, batchIdx) => {\n        const { width, height } = getInputDims(inputs[batchIdx])\n        expect(result.imageWidth).toEqual(width)\n        expect(result.imageHeight).toEqual(height)\n        expect(result.shift.x).toEqual(0)\n        expect(result.shift.y).toEqual(0)\n        result.positions.forEach(({ x, y }, i) => {\n          expectMaxDelta(x, faceLandmarkPositions[batchIdx][i].x, 3)\n          expectMaxDelta(y, faceLandmarkPositions[batchIdx][i].y, 3)\n        })\n      })\n    })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for batch of mixed inputs","suites":[],"line":123,"updatePoint":{"line":123,"column":57},"code":"    it('computes face landmarks for batch of mixed inputs', async () => {\n      const inputs = [imgEl1, tf.browser.fromPixels(createCanvasFromMedia(imgEl2)), tf.browser.fromPixels(createCanvasFromMedia(imgElRect))]\n\n      const faceLandmarkPositions = [\n        faceLandmarkPositions1,\n        faceLandmarkPositions2,\n        faceLandmarkPositionsRect\n      ]\n\n      const results = await faceLandmark68Net.detectLandmarks(inputs) as FaceLandmarks68[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(3)\n      results.forEach((result, batchIdx) => {\n        const { width, height } = getInputDims(inputs[batchIdx])\n        expect(result.imageWidth).toEqual(width)\n        expect(result.imageHeight).toEqual(height)\n        expect(result.shift.x).toEqual(0)\n        expect(result.shift.y).toEqual(0)\n        result.positions.forEach(({ x, y }, i) => {\n          expectMaxDelta(x, faceLandmarkPositions[batchIdx][i].x, 3)\n          expectMaxDelta(y, faceLandmarkPositions[batchIdx][i].y, 3)\n        })\n      })\n    })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"single image element","suites":["forwardInput"],"line":154,"updatePoint":{"line":154,"column":30},"code":"      it('single image element', async () => {\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput([imgEl1])\n          const outTensor = await faceLandmark68Net.forwardInput(netInput)\n          outTensor.dispose()\n        })\n      })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"multiple image elements","suites":["forwardInput"],"line":162,"updatePoint":{"line":162,"column":33},"code":"      it('multiple image elements', async () => {\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput([imgEl1, imgEl1, imgEl1])\n          const outTensor = await faceLandmark68Net.forwardInput(netInput)\n          outTensor.dispose()\n        })\n      })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"single tf.Tensor3D","suites":["forwardInput"],"line":170,"updatePoint":{"line":170,"column":28},"code":"      it('single tf.Tensor3D', async () => {\n        const tensor = tf.browser.fromPixels(createCanvasFromMedia(imgEl1))\n\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput([tensor])\n          const outTensor = await faceLandmark68Net.forwardInput(netInput)\n          outTensor.dispose()\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"multiple tf.Tensor3Ds","suites":["forwardInput"],"line":182,"updatePoint":{"line":182,"column":31},"code":"      it('multiple tf.Tensor3Ds', async () => {\n        const tensors = [imgEl1, imgEl1, imgEl1].map(el => tf.browser.fromPixels(createCanvasFromMedia(el)))\n\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput(tensors)\n          const outTensor = await faceLandmark68Net.forwardInput(netInput)\n          outTensor.dispose()\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"single batch size 1 tf.Tensor4Ds","suites":["forwardInput"],"line":194,"updatePoint":{"line":194,"column":42},"code":"      it('single batch size 1 tf.Tensor4Ds', async () => {\n        const tensor = tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(imgEl1)).expandDims()) as tf.Tensor4D\n\n        await expectAllTensorsReleased(async () => {\n          const outTensor = await faceLandmark68Net.forwardInput(await toNetInput(tensor))\n          outTensor.dispose()\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"multiple batch size 1 tf.Tensor4Ds","suites":["forwardInput"],"line":205,"updatePoint":{"line":205,"column":44},"code":"      it('multiple batch size 1 tf.Tensor4Ds', async () => {\n        const tensors = [imgEl1, imgEl1, imgEl1]\n          .map(el => tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(el)).expandDims())) as tf.Tensor4D[]\n\n        await expectAllTensorsReleased(async () => {\n          const outTensor = await faceLandmark68Net.forwardInput(await toNetInput(tensors))\n          outTensor.dispose()\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"single image element","suites":["detectLandmarks"],"line":221,"updatePoint":{"line":221,"column":30},"code":"      it('single image element', async () => {\n        await expectAllTensorsReleased(async () => {\n          await faceLandmark68Net.detectLandmarks(imgEl1)\n        })\n      })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"multiple image elements","suites":["detectLandmarks"],"line":227,"updatePoint":{"line":227,"column":33},"code":"      it('multiple image elements', async () => {\n        await expectAllTensorsReleased(async () => {\n          await faceLandmark68Net.detectLandmarks([imgEl1, imgEl1, imgEl1])\n        })\n      })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"single tf.Tensor3D","suites":["detectLandmarks"],"line":233,"updatePoint":{"line":233,"column":28},"code":"      it('single tf.Tensor3D', async () => {\n        const tensor = tf.browser.fromPixels(createCanvasFromMedia(imgEl1))\n\n        await expectAllTensorsReleased(async () => {\n          await faceLandmark68Net.detectLandmarks(tensor)\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"multiple tf.Tensor3Ds","suites":["detectLandmarks"],"line":243,"updatePoint":{"line":243,"column":31},"code":"      it('multiple tf.Tensor3Ds', async () => {\n        const tensors = [imgEl1, imgEl1, imgEl1].map(el => tf.browser.fromPixels(createCanvasFromMedia(el)))\n\n\n        await expectAllTensorsReleased(async () => {\n          await faceLandmark68Net.detectLandmarks(tensors)\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"single batch size 1 tf.Tensor4Ds","suites":["detectLandmarks"],"line":254,"updatePoint":{"line":254,"column":42},"code":"      it('single batch size 1 tf.Tensor4Ds', async () => {\n        const tensor = tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(imgEl1)).expandDims()) as tf.Tensor4D\n\n        await expectAllTensorsReleased(async () => {\n          await faceLandmark68Net.detectLandmarks(tensor)\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"multiple batch size 1 tf.Tensor4Ds","suites":["detectLandmarks"],"line":264,"updatePoint":{"line":264,"column":44},"code":"      it('multiple batch size 1 tf.Tensor4Ds', async () => {\n        const tensors = [imgEl1, imgEl1, imgEl1]\n          .map(el => tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(el)).expandDims())) as tf.Tensor4D[]\n\n        await expectAllTensorsReleased(async () => {\n          await faceLandmark68Net.detectLandmarks(tensors)\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceLandmarkNet/faceLandmark68Net.test.ts","skipped":false,"dir":"test"},{"name":"transform x coordinates for width < height","suites":["FaceLandmark68NetBase","postProcess","single batch"],"line":37,"updatePoint":{"line":37,"column":52},"code":"      it('transform x coordinates for width < height', () => {\n\n        const landmarksData = Array(136).fill(0)\n        landmarksData[0] = 0.4\n        landmarksData[1] = 0.55\n        landmarksData[2] = 0.2\n        landmarksData[3] = 0.55\n        landmarksData[4] = 0.1\n        landmarksData[5] = 0.55\n\n        const out = net.postProcess(\n          tf.tensor2d(landmarksData, [1, 136]),\n          128,\n          [{ width: 200, height: 300 }]\n        ).dataSync()\n\n        expect(out[0]).toBeCloseTo(0.35, 2)\n        expect(out[1]).toBeCloseTo(0.55, 2)\n        expect(out[2]).toBeCloseTo(0.05, 2)\n        expect(out[3]).toBeCloseTo(0.55, 2)\n        expect(out[4]).toBeCloseTo(-0.1, 2)\n        expect(out[5]).toBeCloseTo(0.55, 2)\n      })","file":"tests/faceLandmarkNet/FaceLandmark68NetBase.test.ts","skipped":false,"dir":"test"},{"name":"transform y coordinates for height < width","suites":["FaceLandmark68NetBase","postProcess","single batch"],"line":61,"updatePoint":{"line":61,"column":52},"code":"      it('transform y coordinates for height < width', () => {\n\n        const landmarksData = Array(136).fill(0)\n        landmarksData[0] = 0.55\n        landmarksData[1] = 0.4\n        landmarksData[2] = 0.55\n        landmarksData[3] = 0.2\n        landmarksData[4] = 0.55\n        landmarksData[5] = 0.1\n\n        const out = net.postProcess(\n          tf.tensor2d(landmarksData, [1, 136]),\n          128,\n          [{ width: 300, height: 200 }]\n        ).dataSync()\n\n        expect(out[0]).toBeCloseTo(0.55, 2)\n        expect(out[1]).toBeCloseTo(0.35, 2)\n        expect(out[2]).toBeCloseTo(0.55, 2)\n        expect(out[3]).toBeCloseTo(0.05, 2)\n        expect(out[4]).toBeCloseTo(0.55, 2)\n        expect(out[5]).toBeCloseTo(-0.1, 2)\n      })","file":"tests/faceLandmarkNet/FaceLandmark68NetBase.test.ts","skipped":false,"dir":"test"},{"name":"no transformation for height === width","suites":["FaceLandmark68NetBase","postProcess","single batch"],"line":85,"updatePoint":{"line":85,"column":48},"code":"      it('no transformation for height === width', () => {\n\n        const landmarksData = Array(136).fill(0)\n        landmarksData[0] = 0.55\n        landmarksData[1] = 0.4\n        landmarksData[2] = 0.55\n        landmarksData[3] = 0.2\n        landmarksData[4] = 0.55\n        landmarksData[5] = 0.1\n\n        const out = net.postProcess(\n          tf.tensor2d(landmarksData, [1, 136]),\n          128,\n          [{ width: 300, height: 300 }]\n        ).dataSync()\n\n        expect(out[0]).toBeCloseTo(0.55, 2)\n        expect(out[1]).toBeCloseTo(0.4, 2)\n        expect(out[2]).toBeCloseTo(0.55, 2)\n        expect(out[3]).toBeCloseTo(0.2, 2)\n        expect(out[4]).toBeCloseTo(0.55, 2)\n        expect(out[5]).toBeCloseTo(0.1, 2)\n      })","file":"tests/faceLandmarkNet/FaceLandmark68NetBase.test.ts","skipped":false,"dir":"test"},{"name":"transform coordinates correctly for multiple batches","suites":["FaceLandmark68NetBase","postProcess","multiple batches"],"line":113,"updatePoint":{"line":113,"column":62},"code":"      it('transform coordinates correctly for multiple batches', () => {\n\n        const landmarksData1 = Array(136).fill(0)\n        landmarksData1[0] = 0.4\n        landmarksData1[1] = 0.55\n        landmarksData1[2] = 0.2\n        landmarksData1[3] = 0.55\n        landmarksData1[4] = 0.1\n        landmarksData1[5] = 0.55\n        const landmarksData2 = Array(136).fill(0)\n        landmarksData2[0] = 0.55\n        landmarksData2[1] = 0.4\n        landmarksData2[2] = 0.55\n        landmarksData2[3] = 0.2\n        landmarksData2[4] = 0.55\n        landmarksData2[5] = 0.1\n\n        const out = net.postProcess(\n          tf.tensor2d(landmarksData1.concat(landmarksData2).concat(landmarksData1), [3, 136]),\n          128,\n          [{ width: 200, height: 300 }, { width: 300, height: 200 }, { width: 300, height: 300 }]\n        ).dataSync()\n\n        expect(out[0]).toBeCloseTo(0.35, 2)\n        expect(out[1]).toBeCloseTo(0.55, 2)\n        expect(out[2]).toBeCloseTo(0.05, 2)\n        expect(out[3]).toBeCloseTo(0.55, 2)\n        expect(out[4]).toBeCloseTo(-0.1, 2)\n        expect(out[5]).toBeCloseTo(0.55, 2)\n        expect(out[0 + 136]).toBeCloseTo(0.55, 2)\n        expect(out[1 + 136]).toBeCloseTo(0.35, 2)\n        expect(out[2 + 136]).toBeCloseTo(0.55, 2)\n        expect(out[3 + 136]).toBeCloseTo(0.05, 2)\n        expect(out[4 + 136]).toBeCloseTo(0.55, 2)\n        expect(out[5 + 136]).toBeCloseTo(-0.1, 2)\n        expect(out[0 + (136 * 2)]).toBeCloseTo(0.4, 2)\n        expect(out[1 + (136 * 2)]).toBeCloseTo(0.55, 2)\n        expect(out[2 + (136 * 2)]).toBeCloseTo(0.2, 2)\n        expect(out[3 + (136 * 2)]).toBeCloseTo(0.55, 2)\n        expect(out[4 + (136 * 2)]).toBeCloseTo(0.1, 2)\n        expect(out[5 + (136 * 2)]).toBeCloseTo(0.55, 2)\n      })","file":"tests/faceLandmarkNet/FaceLandmark68NetBase.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for squared input","suites":[],"line":35,"updatePoint":{"line":35,"column":49},"code":"    it('computes face landmarks for squared input', async () => {\n      const { width, height } = imgEl1\n\n      const result = await faceLandmark68TinyNet.detectLandmarks(imgEl1) as FaceLandmarks68\n      expect(result.imageWidth).toEqual(width)\n      expect(result.imageHeight).toEqual(height)\n      expect(result.shift.x).toEqual(0)\n      expect(result.shift.y).toEqual(0)\n      result.positions.forEach((pt, i) => {\n        const { x, y } = faceLandmarkPositions1[i]\n        expectPointClose(pt, { x, y }, 5)\n      })\n    })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for rectangular input","suites":[],"line":49,"updatePoint":{"line":49,"column":53},"code":"    it('computes face landmarks for rectangular input', async () => {\n      const { width, height } = imgElRect\n\n      const result = await faceLandmark68TinyNet.detectLandmarks(imgElRect) as FaceLandmarks68\n      expect(result.imageWidth).toEqual(width)\n      expect(result.imageHeight).toEqual(height)\n      expect(result.shift.x).toEqual(0)\n      expect(result.shift.y).toEqual(0)\n      result.positions.forEach((pt, i) => {\n        const { x, y } = faceLandmarkPositionsRect[i]\n        expectPointClose(pt, { x, y }, 5)\n      })\n    })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for batch of image elements","suites":[],"line":67,"updatePoint":{"line":67,"column":59},"code":"    it('computes face landmarks for batch of image elements', async () => {\n      const inputs = [imgEl1, imgEl2, imgElRect]\n\n      const faceLandmarkPositions = [\n        faceLandmarkPositions1,\n        faceLandmarkPositions2,\n        faceLandmarkPositionsRect\n      ]\n\n      const results = await faceLandmark68TinyNet.detectLandmarks(inputs) as FaceLandmarks68[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(3)\n      results.forEach((result, batchIdx) => {\n        const { width, height } = getInputDims(inputs[batchIdx])\n        expect(result.imageWidth).toEqual(width)\n        expect(result.imageHeight).toEqual(height)\n        expect(result.shift.x).toEqual(0)\n        expect(result.shift.y).toEqual(0)\n        result.positions.forEach((pt, i) => {\n          const { x, y } = faceLandmarkPositions[batchIdx][i]\n          expectPointClose(pt, { x, y }, 5)\n        })\n      })\n    })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for batch of tf.Tensor3D","suites":[],"line":92,"updatePoint":{"line":92,"column":56},"code":"    it('computes face landmarks for batch of tf.Tensor3D', async () => {\n      const inputs = [imgEl1, imgEl2, imgElRect].map(el => tf.browser.fromPixels(createCanvasFromMedia(el)))\n\n      const faceLandmarkPositions = [\n        faceLandmarkPositions1,\n        faceLandmarkPositions2,\n        faceLandmarkPositionsRect\n      ]\n\n      const results = await faceLandmark68TinyNet.detectLandmarks(inputs) as FaceLandmarks68[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(3)\n      results.forEach((result, batchIdx) => {\n        const { width, height } = getInputDims(inputs[batchIdx])\n        expect(result.imageWidth).toEqual(width)\n        expect(result.imageHeight).toEqual(height)\n        expect(result.shift.x).toEqual(0)\n        expect(result.shift.y).toEqual(0)\n        result.positions.forEach((pt, i) => {\n          const { x, y } = faceLandmarkPositions[batchIdx][i]\n          expectPointClose(pt, { x, y }, 3)\n        })\n      })\n    })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"computes face landmarks for batch of mixed inputs","suites":[],"line":117,"updatePoint":{"line":117,"column":57},"code":"    it('computes face landmarks for batch of mixed inputs', async () => {\n      const inputs = [imgEl1, tf.browser.fromPixels(createCanvasFromMedia(imgEl2)), tf.browser.fromPixels(createCanvasFromMedia(imgElRect))]\n\n      const faceLandmarkPositions = [\n        faceLandmarkPositions1,\n        faceLandmarkPositions2,\n        faceLandmarkPositionsRect\n      ]\n\n      const results = await faceLandmark68TinyNet.detectLandmarks(inputs) as FaceLandmarks68[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(3)\n      results.forEach((result, batchIdx) => {\n        const { width, height } = getInputDims(inputs[batchIdx])\n        expect(result.imageWidth).toEqual(width)\n        expect(result.imageHeight).toEqual(height)\n        expect(result.shift.x).toEqual(0)\n        expect(result.shift.y).toEqual(0)\n        result.positions.forEach((pt, i) => {\n          const { x, y } = faceLandmarkPositions[batchIdx][i]\n          expectPointClose(pt, { x, y }, 3)\n        })\n      })\n    })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"single image element","suites":["forwardInput"],"line":149,"updatePoint":{"line":149,"column":30},"code":"      it('single image element', async () => {\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput([imgEl1])\n          const outTensor = await faceLandmark68TinyNet.forwardInput(netInput)\n          outTensor.dispose()\n        })\n      })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple image elements","suites":["forwardInput"],"line":157,"updatePoint":{"line":157,"column":33},"code":"      it('multiple image elements', async () => {\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput([imgEl1, imgEl1, imgEl1])\n          const outTensor = await faceLandmark68TinyNet.forwardInput(netInput)\n          outTensor.dispose()\n        })\n      })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"single tf.Tensor3D","suites":["forwardInput"],"line":165,"updatePoint":{"line":165,"column":28},"code":"      it('single tf.Tensor3D', async () => {\n        const tensor = tf.browser.fromPixels(createCanvasFromMedia(imgEl1))\n\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput([tensor])\n          const outTensor = await faceLandmark68TinyNet.forwardInput(netInput)\n          outTensor.dispose()\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple tf.Tensor3Ds","suites":["forwardInput"],"line":177,"updatePoint":{"line":177,"column":31},"code":"      it('multiple tf.Tensor3Ds', async () => {\n        const tensors = [imgEl1, imgEl1, imgEl1].map(el => tf.browser.fromPixels(createCanvasFromMedia(el)))\n\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput(tensors)\n          const outTensor = await faceLandmark68TinyNet.forwardInput(netInput)\n          outTensor.dispose()\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"single batch size 1 tf.Tensor4Ds","suites":["forwardInput"],"line":189,"updatePoint":{"line":189,"column":42},"code":"      it('single batch size 1 tf.Tensor4Ds', async () => {\n        const tensor = tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(imgEl1)).expandDims()) as tf.Tensor4D\n\n        await expectAllTensorsReleased(async () => {\n          const outTensor = await faceLandmark68TinyNet.forwardInput(await toNetInput(tensor))\n          outTensor.dispose()\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple batch size 1 tf.Tensor4Ds","suites":["forwardInput"],"line":200,"updatePoint":{"line":200,"column":44},"code":"      it('multiple batch size 1 tf.Tensor4Ds', async () => {\n        const tensors = [imgEl1, imgEl1, imgEl1]\n          .map(el => tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(el)).expandDims())) as tf.Tensor4D[]\n\n        await expectAllTensorsReleased(async () => {\n          const outTensor = await faceLandmark68TinyNet.forwardInput(await toNetInput(tensors))\n          outTensor.dispose()\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"single image element","suites":["detectLandmarks"],"line":216,"updatePoint":{"line":216,"column":30},"code":"      it('single image element', async () => {\n        await expectAllTensorsReleased(async () => {\n          await faceLandmark68TinyNet.detectLandmarks(imgEl1)\n        })\n      })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple image elements","suites":["detectLandmarks"],"line":222,"updatePoint":{"line":222,"column":33},"code":"      it('multiple image elements', async () => {\n        await expectAllTensorsReleased(async () => {\n          await faceLandmark68TinyNet.detectLandmarks([imgEl1, imgEl1, imgEl1])\n        })\n      })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"single tf.Tensor3D","suites":["detectLandmarks"],"line":228,"updatePoint":{"line":228,"column":28},"code":"      it('single tf.Tensor3D', async () => {\n        const tensor = tf.browser.fromPixels(createCanvasFromMedia(imgEl1))\n\n        await expectAllTensorsReleased(async () => {\n          await faceLandmark68TinyNet.detectLandmarks(tensor)\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple tf.Tensor3Ds","suites":["detectLandmarks"],"line":238,"updatePoint":{"line":238,"column":31},"code":"      it('multiple tf.Tensor3Ds', async () => {\n        const tensors = [imgEl1, imgEl1, imgEl1].map(el => tf.browser.fromPixels(createCanvasFromMedia(el)))\n\n\n        await expectAllTensorsReleased(async () => {\n          await faceLandmark68TinyNet.detectLandmarks(tensors)\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"single batch size 1 tf.Tensor4Ds","suites":["detectLandmarks"],"line":249,"updatePoint":{"line":249,"column":42},"code":"      it('single batch size 1 tf.Tensor4Ds', async () => {\n        const tensor = tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(imgEl1)).expandDims()) as tf.Tensor4D\n\n        await expectAllTensorsReleased(async () => {\n          await faceLandmark68TinyNet.detectLandmarks(tensor)\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple batch size 1 tf.Tensor4Ds","suites":["detectLandmarks"],"line":259,"updatePoint":{"line":259,"column":44},"code":"      it('multiple batch size 1 tf.Tensor4Ds', async () => {\n        const tensors = [imgEl1, imgEl1, imgEl1]\n          .map(el => tf.tidy(() => tf.browser.fromPixels(createCanvasFromMedia(el)).expandDims())) as tf.Tensor4D[]\n\n        await expectAllTensorsReleased(async () => {\n          await faceLandmark68TinyNet.detectLandmarks(tensors)\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceLandmarkNet/faceLandmark68TinyNet.test.ts","skipped":false,"dir":"test"},{"name":"computes face descriptor for squared input","suites":[],"line":27,"updatePoint":{"line":27,"column":50},"code":"    it('computes face descriptor for squared input', async () => {\n      const result = await faceRecognitionNet.computeFaceDescriptor(imgEl1) as Float32Array\n      expect(result.length).toEqual(128)\n      expect(euclideanDistance(result, faceDescriptor1)).toBeLessThan(0.1)\n    })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"computes face descriptor for rectangular input","suites":[],"line":33,"updatePoint":{"line":33,"column":54},"code":"    it('computes face descriptor for rectangular input', async () => {\n      const result = await faceRecognitionNet.computeFaceDescriptor(imgElRect) as Float32Array\n      expect(result.length).toEqual(128)\n      expect(euclideanDistance(result, faceDescriptorRect)).toBeLessThan(0.1)\n    })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"computes face descriptors for batch of image elements","suites":[],"line":44,"updatePoint":{"line":44,"column":61},"code":"    it('computes face descriptors for batch of image elements', async () => {\n      const inputs = [imgEl1, imgEl2, imgElRect]\n\n      const faceDescriptors = [\n        faceDescriptor1,\n        faceDescriptor2,\n        faceDescriptorRect\n      ]\n\n      const results = await faceRecognitionNet.computeFaceDescriptor(inputs) as Float32Array[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(3)\n      results.forEach((result, batchIdx) => {\n        expect(euclideanDistance(result, faceDescriptors[batchIdx])).toBeLessThan(0.1)\n      })\n    })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"computes face descriptors for batch of tf.Tensor3D","suites":[],"line":61,"updatePoint":{"line":61,"column":58},"code":"    it('computes face descriptors for batch of tf.Tensor3D', async () => {\n      const inputs = [imgEl1, imgEl2, imgElRect].map(el => tf.browser.fromPixels(el))\n\n      const faceDescriptors = [\n        faceDescriptor1,\n        faceDescriptor2,\n        faceDescriptorRect\n      ]\n\n      const results = await faceRecognitionNet.computeFaceDescriptor(inputs) as Float32Array[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(3)\n      results.forEach((result, batchIdx) => {\n        expect(euclideanDistance(result, faceDescriptors[batchIdx])).toBeLessThan(0.1)\n      })\n    })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"computes face descriptors for batch of mixed inputs","suites":[],"line":78,"updatePoint":{"line":78,"column":59},"code":"    it('computes face descriptors for batch of mixed inputs', async () => {\n      const inputs = [imgEl1, tf.browser.fromPixels(imgEl2), tf.browser.fromPixels(imgElRect)]\n\n      const faceDescriptors = [\n        faceDescriptor1,\n        faceDescriptor2,\n        faceDescriptorRect\n      ]\n\n      const results = await faceRecognitionNet.computeFaceDescriptor(inputs) as Float32Array[]\n      expect(Array.isArray(results)).toBe(true)\n      expect(results.length).toEqual(3)\n      results.forEach((result, batchIdx) => {\n        expect(euclideanDistance(result, faceDescriptors[batchIdx])).toBeLessThan(0.1)\n      })\n    })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"single image element","suites":["forwardInput"],"line":101,"updatePoint":{"line":101,"column":30},"code":"      it('single image element', async () => {\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput([imgEl1])\n          const outTensor = await faceRecognitionNet.forwardInput(netInput)\n          outTensor.dispose()\n        })\n      })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple image elements","suites":["forwardInput"],"line":109,"updatePoint":{"line":109,"column":33},"code":"      it('multiple image elements', async () => {\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput([imgEl1, imgEl1, imgEl1])\n          const outTensor = await faceRecognitionNet.forwardInput(netInput)\n          outTensor.dispose()\n        })\n      })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"single tf.Tensor3D","suites":["forwardInput"],"line":117,"updatePoint":{"line":117,"column":28},"code":"      it('single tf.Tensor3D', async () => {\n        const tensor = tf.browser.fromPixels(imgEl1)\n\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput([tensor])\n          const outTensor = await faceRecognitionNet.forwardInput(netInput)\n          outTensor.dispose()\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple tf.Tensor3Ds","suites":["forwardInput"],"line":129,"updatePoint":{"line":129,"column":31},"code":"      it('multiple tf.Tensor3Ds', async () => {\n        const tensors = [imgEl1, imgEl1, imgEl1].map(el => tf.browser.fromPixels(el))\n\n        await expectAllTensorsReleased(async () => {\n          const netInput = new NetInput(tensors)\n          const outTensor = await faceRecognitionNet.forwardInput(netInput)\n          outTensor.dispose()\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"single batch size 1 tf.Tensor4Ds","suites":["forwardInput"],"line":141,"updatePoint":{"line":141,"column":42},"code":"      it('single batch size 1 tf.Tensor4Ds', async () => {\n        const tensor = tf.tidy(() => tf.browser.fromPixels(imgEl1).expandDims()) as tf.Tensor4D\n\n        await expectAllTensorsReleased(async () => {\n          const outTensor = await faceRecognitionNet.forwardInput(await toNetInput(tensor))\n          outTensor.dispose()\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple batch size 1 tf.Tensor4Ds","suites":["forwardInput"],"line":152,"updatePoint":{"line":152,"column":44},"code":"      it('multiple batch size 1 tf.Tensor4Ds', async () => {\n        const tensors = [imgEl1, imgEl1, imgEl1]\n          .map(el => tf.tidy(() => tf.browser.fromPixels(el).expandDims())) as tf.Tensor4D[]\n\n        await expectAllTensorsReleased(async () => {\n          const outTensor = await faceRecognitionNet.forwardInput(await toNetInput(tensors))\n          outTensor.dispose()\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"single image element","suites":["computeFaceDescriptor"],"line":168,"updatePoint":{"line":168,"column":30},"code":"      it('single image element', async () => {\n        await expectAllTensorsReleased(async () => {\n          await faceRecognitionNet.computeFaceDescriptor(imgEl1)\n        })\n      })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple image elements","suites":["computeFaceDescriptor"],"line":174,"updatePoint":{"line":174,"column":33},"code":"      it('multiple image elements', async () => {\n        await expectAllTensorsReleased(async () => {\n          await faceRecognitionNet.computeFaceDescriptor([imgEl1, imgEl1, imgEl1])\n        })\n      })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"single tf.Tensor3D","suites":["computeFaceDescriptor"],"line":180,"updatePoint":{"line":180,"column":28},"code":"      it('single tf.Tensor3D', async () => {\n        const tensor = tf.browser.fromPixels(imgEl1)\n\n        await expectAllTensorsReleased(async () => {\n          await faceRecognitionNet.computeFaceDescriptor(tensor)\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple tf.Tensor3Ds","suites":["computeFaceDescriptor"],"line":190,"updatePoint":{"line":190,"column":31},"code":"      it('multiple tf.Tensor3Ds', async () => {\n        const tensors = [imgEl1, imgEl1, imgEl1].map(el => tf.browser.fromPixels(el))\n\n\n        await expectAllTensorsReleased(async () => {\n          await faceRecognitionNet.computeFaceDescriptor(tensors)\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"single batch size 1 tf.Tensor4Ds","suites":["computeFaceDescriptor"],"line":201,"updatePoint":{"line":201,"column":42},"code":"      it('single batch size 1 tf.Tensor4Ds', async () => {\n        const tensor = tf.tidy(() => tf.browser.fromPixels(imgEl1).expandDims()) as tf.Tensor4D\n\n        await expectAllTensorsReleased(async () => {\n          await faceRecognitionNet.computeFaceDescriptor(tensor)\n        })\n\n        tensor.dispose()\n      })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"multiple batch size 1 tf.Tensor4Ds","suites":["computeFaceDescriptor"],"line":211,"updatePoint":{"line":211,"column":44},"code":"      it('multiple batch size 1 tf.Tensor4Ds', async () => {\n        const tensors = [imgEl1, imgEl1, imgEl1]\n          .map(el => tf.tidy(() => tf.browser.fromPixels(el).expandDims())) as tf.Tensor4D[]\n\n        await expectAllTensorsReleased(async () => {\n          await faceRecognitionNet.computeFaceDescriptor(tensors)\n        })\n\n        tensors.forEach(t => t.dispose())\n      })","file":"tests/faceRecognitionNet/faceRecognitionNet.test.ts","skipped":false,"dir":"test"},{"name":"returns WithFaceDetection","suites":["extendWithFaceDetection"],"line":7,"updatePoint":{"line":7,"column":31},"code":"  it('returns WithFaceDetection', () => {\n\n    const withFaceDetection = extendWithFaceDetection({}, detection)\n    expect(withFaceDetection.detection).toEqual(detection)\n\n  })","file":"tests/factories/WithFaceDetection.test.ts","skipped":false,"dir":"test"},{"name":"extends source object","suites":["extendWithFaceDetection"],"line":14,"updatePoint":{"line":14,"column":27},"code":"  it('extends source object', () => {\n\n    const srcProp = { foo: true }\n\n    const withFaceDetection = extendWithFaceDetection({ srcProp }, detection)\n    expect(withFaceDetection.detection).toEqual(detection)\n    expect(withFaceDetection.srcProp).toEqual(srcProp)\n\n  })","file":"tests/factories/WithFaceDetection.test.ts","skipped":false,"dir":"test"},{"name":"returns WithFaceLandmarks","suites":["extendWithFaceDetection"],"line":10,"updatePoint":{"line":10,"column":31},"code":"  it('returns WithFaceLandmarks', () => {\n\n    const srcObj = {}\n    const srcObjWithFaceDetection = makeSrcObjectWithFaceDetection(srcObj)\n    const withFaceLandmarks = extendWithFaceLandmarks(srcObjWithFaceDetection, unshiftedLandmarks)\n\n    expect(withFaceLandmarks.detection).toEqual(detection)\n    expect(withFaceLandmarks.unshiftedLandmarks).toEqual(unshiftedLandmarks)\n    expect(withFaceLandmarks.alignedRect instanceof FaceDetection).toBe(true)\n    expect(withFaceLandmarks.landmarks instanceof FaceLandmarks68).toBe(true)\n\n  })","file":"tests/factories/WithFaceLandmarks.test.ts","skipped":false,"dir":"test"},{"name":"extends source object","suites":["extendWithFaceDetection"],"line":23,"updatePoint":{"line":23,"column":27},"code":"  it('extends source object', () => {\n\n    const srcObj = { srcProp: { foo: true } }\n    const srcObjWithFaceDetection = makeSrcObjectWithFaceDetection(srcObj)\n    const withFaceLandmarks = extendWithFaceLandmarks(srcObjWithFaceDetection, unshiftedLandmarks)\n\n    expect(withFaceLandmarks.srcProp).toEqual(srcObj.srcProp)\n\n    expect(withFaceLandmarks.detection).toEqual(detection)\n    expect(withFaceLandmarks.unshiftedLandmarks).toEqual(unshiftedLandmarks)\n    expect(withFaceLandmarks.alignedRect instanceof FaceDetection).toBe(true)\n    expect(withFaceLandmarks.landmarks instanceof FaceLandmarks68).toBe(true)\n\n  })","file":"tests/factories/WithFaceLandmarks.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceExpressions()","suites":["without face alignment"],"line":56,"updatePoint":{"line":56,"column":46},"code":"      it('detectAllFaces.withFaceExpressions()', async () => {\n        const results = await detectAllFaces(imgEl, faceDetectorOptions)\n          .withFaceExpressions()\n\n        expect(results.length).toEqual(6)\n        expectFaceExpressions(results)\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withAgeAndGender()","suites":["without face alignment"],"line":64,"updatePoint":{"line":64,"column":43},"code":"      it('detectAllFaces.withAgeAndGender()', async () => {\n        const results = await detectAllFaces(imgEl, faceDetectorOptions)\n          .withAgeAndGender()\n\n        expect(results.length).toEqual(6)\n        expectAgesAndGender(results, false)\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceExpressions().withAgeAndGender()","suites":["without face alignment"],"line":72,"updatePoint":{"line":72,"column":65},"code":"      it('detectAllFaces.withFaceExpressions().withAgeAndGender()', async () => {\n        const results = await detectAllFaces(imgEl, faceDetectorOptions)\n          .withFaceExpressions()\n          .withAgeAndGender()\n\n        expect(results.length).toEqual(6)\n        expectFaceExpressions(results)\n        expectAgesAndGender(results, false)\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withAgeAndGender().withFaceExpressions()","suites":["without face alignment"],"line":82,"updatePoint":{"line":82,"column":65},"code":"      it('detectAllFaces.withAgeAndGender().withFaceExpressions()', async () => {\n        const results = await detectAllFaces(imgEl, faceDetectorOptions)\n          .withAgeAndGender()\n          .withFaceExpressions()\n\n        expect(results.length).toEqual(6)\n        expectFaceExpressions(results)\n        expectAgesAndGender(results, false)\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withFaceExpressions()","suites":["with face alignment"],"line":96,"updatePoint":{"line":96,"column":66},"code":"      it('detectAllFaces.withFaceLandmarks().withFaceExpressions()', async () => {\n        const results = await detectAllFaces(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withFaceExpressions()\n\n        expect(results.length).toEqual(6)\n        expectFaceExpressions(results)\n        expectFaceDetectionsWithLandmarks(results, expectedFullFaceDescriptions, expectedScores, deltas)\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withAgeAndGender()","suites":["with face alignment"],"line":106,"updatePoint":{"line":106,"column":63},"code":"      it('detectAllFaces.withFaceLandmarks().withAgeAndGender()', async () => {\n        const results = await detectAllFaces(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withAgeAndGender()\n\n        expect(results.length).toEqual(6)\n        expectAgesAndGender(results)\n        expectFaceDetectionsWithLandmarks(results, expectedFullFaceDescriptions, expectedScores, deltas)\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withFaceDescriptors()","suites":["with face alignment"],"line":116,"updatePoint":{"line":116,"column":66},"code":"      it('detectAllFaces.withFaceLandmarks().withFaceDescriptors()', async () => {\n        const results = await detectAllFaces(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withFaceDescriptors()\n\n        expect(results.length).toEqual(6)\n        expectFullFaceDescriptions(results, expectedFullFaceDescriptions, expectedScores, deltas)\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withFaceExpressions().withAgeAndGender()","suites":["with face alignment"],"line":125,"updatePoint":{"line":125,"column":85},"code":"      it('detectAllFaces.withFaceLandmarks().withFaceExpressions().withAgeAndGender()', async () => {\n        const results = await detectAllFaces(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withFaceExpressions()\n          .withAgeAndGender()\n\n        expect(results.length).toEqual(6)\n        expectFaceExpressions(results)\n        expectAgesAndGender(results)\n        expectFaceDetectionsWithLandmarks(results, expectedFullFaceDescriptions, expectedScores, deltas)\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withAgeAndGender().withFaceExpressions()","suites":["with face alignment"],"line":137,"updatePoint":{"line":137,"column":85},"code":"      it('detectAllFaces.withFaceLandmarks().withAgeAndGender().withFaceExpressions()', async () => {\n        const results = await detectAllFaces(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withAgeAndGender()\n          .withFaceExpressions()\n\n        expect(results.length).toEqual(6)\n        expectFaceExpressions(results)\n        expectAgesAndGender(results)\n        expectFaceDetectionsWithLandmarks(results, expectedFullFaceDescriptions, expectedScores, deltas)\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withFaceExpressions().withFaceDescriptors()","suites":["with face alignment"],"line":149,"updatePoint":{"line":149,"column":88},"code":"      it('detectAllFaces.withFaceLandmarks().withFaceExpressions().withFaceDescriptors()', async () => {\n        const results = await detectAllFaces(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withFaceExpressions()\n          .withFaceDescriptors()\n\n        expect(results.length).toEqual(6)\n        expectFaceExpressions(results)\n        expectFullFaceDescriptions(results, expectedFullFaceDescriptions, expectedScores, deltas)\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withAgeAndGender().withFaceDescriptors()","suites":["with face alignment"],"line":160,"updatePoint":{"line":160,"column":85},"code":"      it('detectAllFaces.withFaceLandmarks().withAgeAndGender().withFaceDescriptors()', async () => {\n        const results = await detectAllFaces(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withAgeAndGender()\n          .withFaceDescriptors()\n\n        expect(results.length).toEqual(6)\n        expectAgesAndGender(results)\n        expectFullFaceDescriptions(results, expectedFullFaceDescriptions, expectedScores, deltas)\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withFaceExpressions().withAgeAndGender().withFaceDescriptors()","suites":["with face alignment"],"line":171,"updatePoint":{"line":171,"column":107},"code":"      it('detectAllFaces.withFaceLandmarks().withFaceExpressions().withAgeAndGender().withFaceDescriptors()', async () => {\n        const results = await detectAllFaces(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withFaceExpressions()\n          .withAgeAndGender()\n          .withFaceDescriptors()\n\n        expect(results.length).toEqual(6)\n        expectFaceExpressions(results)\n        expectAgesAndGender(results)\n        expectFullFaceDescriptions(results, expectedFullFaceDescriptions, expectedScores, deltas)\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withFaceDescriptors()","suites":["no memory leaks"],"line":188,"updatePoint":{"line":188,"column":66},"code":"      it('detectAllFaces.withFaceLandmarks().withFaceDescriptors()', async () => {\n        await expectAllTensorsReleased(async () => {\n          await detectAllFaces(imgEl, faceDetectorOptions)\n            .withFaceLandmarks()\n            .withFaceDescriptors()\n        })\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withFaceExpressions().withAgeAndGender().withFaceDescriptors()","suites":["no memory leaks"],"line":196,"updatePoint":{"line":196,"column":107},"code":"      it('detectAllFaces.withFaceLandmarks().withFaceExpressions().withAgeAndGender().withFaceDescriptors()', async () => {\n        await expectAllTensorsReleased(async () => {\n          await detectAllFaces(imgEl, faceDetectorOptions)\n            .withFaceLandmarks()\n            .withFaceExpressions()\n            .withAgeAndGender()\n            .withFaceDescriptors()\n        })\n      })","file":"tests/globalApi/detectAllFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceExpressions()","suites":["without face alignment"],"line":80,"updatePoint":{"line":80,"column":48},"code":"      it('detectSingleFace.withFaceExpressions()', async () => {\n        const result = await detectSingleFace(imgEl, faceDetectorOptions)\n          .withFaceExpressions()\n\n        expectFaceExpressions(result)\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withAgeAndGender()","suites":["without face alignment"],"line":87,"updatePoint":{"line":87,"column":45},"code":"      it('detectSingleFace.withAgeAndGender()', async () => {\n        const result = await detectSingleFace(imgEl, faceDetectorOptions)\n          .withAgeAndGender()\n\n        expectAgeAndGender(result, false)\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceExpressions().withAgeAndGender()","suites":["without face alignment"],"line":94,"updatePoint":{"line":94,"column":67},"code":"      it('detectSingleFace.withFaceExpressions().withAgeAndGender()', async () => {\n        const result = await detectSingleFace(imgEl, faceDetectorOptions)\n          .withFaceExpressions()\n          .withAgeAndGender()\n\n        expectFaceExpressions(result)\n        expectAgeAndGender(result, false)\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withAgeAndGender().withFaceExpressions()","suites":["without face alignment"],"line":103,"updatePoint":{"line":103,"column":67},"code":"      it('detectSingleFace.withAgeAndGender().withFaceExpressions()', async () => {\n        const result = await detectSingleFace(imgEl, faceDetectorOptions)\n          .withAgeAndGender()\n          .withFaceExpressions()\n\n        expectFaceExpressions(result)\n        expectAgeAndGender(result, false)\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withFaceExpressions()","suites":["with face alignment"],"line":116,"updatePoint":{"line":116,"column":68},"code":"      it('detectSingleFace.withFaceLandmarks().withFaceExpressions()', async () => {\n        const result = await detectSingleFace(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withFaceExpressions()\n\n        expectFaceExpressions(result)\n        expectFaceDetectionWithLandmarks(result)\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withAgeAndGender()","suites":["with face alignment"],"line":125,"updatePoint":{"line":125,"column":65},"code":"      it('detectSingleFace.withFaceLandmarks().withAgeAndGender()', async () => {\n        const result = await detectSingleFace(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withAgeAndGender()\n\n        expectAgeAndGender(result)\n        expectFaceDetectionWithLandmarks(result)\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withFaceDescriptor()","suites":["with face alignment"],"line":134,"updatePoint":{"line":134,"column":67},"code":"      it('detectSingleFace.withFaceLandmarks().withFaceDescriptor()', async () => {\n        const result = await detectSingleFace(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withFaceDescriptor()\n\n        expectFullFaceDescription(result)\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withFaceExpressions().withAgeAndGender()","suites":["with face alignment"],"line":142,"updatePoint":{"line":142,"column":87},"code":"      it('detectSingleFace.withFaceLandmarks().withFaceExpressions().withAgeAndGender()', async () => {\n        const result = await detectSingleFace(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withFaceExpressions()\n          .withAgeAndGender()\n\n        expectFaceExpressions(result)\n        expectAgeAndGender(result)\n        expectFaceDetectionWithLandmarks(result)\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withAgeAndGender().withFaceExpressions()","suites":["with face alignment"],"line":153,"updatePoint":{"line":153,"column":87},"code":"      it('detectSingleFace.withFaceLandmarks().withAgeAndGender().withFaceExpressions()', async () => {\n        const result = await detectSingleFace(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withAgeAndGender()\n          .withFaceExpressions()\n\n        expectFaceExpressions(result)\n        expectAgeAndGender(result)\n        expectFaceDetectionWithLandmarks(result)\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withFaceExpressions().withFaceDescriptor()","suites":["with face alignment"],"line":164,"updatePoint":{"line":164,"column":89},"code":"      it('detectSingleFace.withFaceLandmarks().withFaceExpressions().withFaceDescriptor()', async () => {\n        const result = await detectSingleFace(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withFaceExpressions()\n          .withFaceDescriptor()\n\n        expectFaceExpressions(result)\n        expectFullFaceDescription(result)\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withAgeAndGender().withFaceDescriptor()","suites":["with face alignment"],"line":174,"updatePoint":{"line":174,"column":86},"code":"      it('detectSingleFace.withFaceLandmarks().withAgeAndGender().withFaceDescriptor()', async () => {\n        const result = await detectSingleFace(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withAgeAndGender()\n          .withFaceDescriptor()\n\n        expectAgeAndGender(result)\n        expectFullFaceDescription(result)\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withFaceExpressions().withAgeAndGender().withFaceDescriptor()","suites":["with face alignment"],"line":184,"updatePoint":{"line":184,"column":108},"code":"      it('detectSingleFace.withFaceLandmarks().withFaceExpressions().withAgeAndGender().withFaceDescriptor()', async () => {\n        const result = await detectSingleFace(imgEl, faceDetectorOptions)\n          .withFaceLandmarks()\n          .withFaceExpressions()\n          .withAgeAndGender()\n          .withFaceDescriptor()\n\n        expectFaceExpressions(result)\n        expectAgeAndGender(result)\n        expectFullFaceDescription(result)\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withFaceDescriptor()","suites":["no memory leaks"],"line":200,"updatePoint":{"line":200,"column":67},"code":"      it('detectSingleFace.withFaceLandmarks().withFaceDescriptor()', async () => {\n        await expectAllTensorsReleased(async () => {\n          await detectSingleFace(imgEl, faceDetectorOptions)\n            .withFaceLandmarks()\n            .withFaceDescriptor()\n        })\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withFaceExpressions().withAgeAndGender().withFaceDescriptor()","suites":["no memory leaks"],"line":208,"updatePoint":{"line":208,"column":108},"code":"      it('detectSingleFace.withFaceLandmarks().withFaceExpressions().withAgeAndGender().withFaceDescriptor()', async () => {\n        await expectAllTensorsReleased(async () => {\n          await detectSingleFace(imgEl, faceDetectorOptions)\n            .withFaceLandmarks()\n            .withFaceExpressions()\n            .withAgeAndGender()\n            .withFaceDescriptor()\n        })\n      })","file":"tests/globalApi/detectSingleFace.test.ts","skipped":false,"dir":"test"},{"name":"JSON.stringify()","suites":["globalApi","FaceMatcher"],"line":21,"updatePoint":{"line":21,"column":24},"code":"    it('JSON.stringify()', () => {\n      expect(JSON.stringify(new FaceMatcher(lds, dt))).toBe(json);\n      expect(JSON.stringify({ m: new FaceMatcher(lds, dt) })).toBe(`{\"m\":${json}}`);\n      expect(JSON.stringify([ new FaceMatcher(lds, dt) ])).toBe(`[${json}]`);\n    });","file":"tests/globalApi/FaceMatcher.test.ts","skipped":false,"dir":"test"},{"name":"fromJSON()","suites":["globalApi","FaceMatcher"],"line":27,"updatePoint":{"line":27,"column":18},"code":"    it('fromJSON()', () => {\n      const fm = FaceMatcher.fromJSON(JSON.parse(json));\n\n      expect(fm.distanceThreshold).toBe(dt);\n      expect(fm.labeledDescriptors.length).toBe(2);\n      expect(fm.labeledDescriptors[0].label).toBe(l1);\n      expect(fm.labeledDescriptors[0].descriptors.length).toBe(2);\n      expect(fm.labeledDescriptors[0].descriptors[0]).toEqual(f1);\n      expect(fm.labeledDescriptors[0].descriptors[1]).toEqual(f2);\n      expect(fm.labeledDescriptors[1].label).toBe(l2);\n      expect(fm.labeledDescriptors[1].descriptors.length).toBe(2);\n      expect(fm.labeledDescriptors[1].descriptors[0]).toEqual(f3);\n      expect(fm.labeledDescriptors[1].descriptors[1]).toEqual(f4);\n    });","file":"tests/globalApi/FaceMatcher.test.ts","skipped":false,"dir":"test"},{"name":"toJSON() => fromJSON()","suites":["globalApi","FaceMatcher"],"line":42,"updatePoint":{"line":42,"column":30},"code":"    it('toJSON() => fromJSON()', () => {\n      const fm = FaceMatcher.fromJSON(new FaceMatcher(lds, dt).toJSON());\n\n      expect(fm.distanceThreshold).toBe(dt);\n      expect(fm.labeledDescriptors.length).toBe(2);\n      expect(fm.labeledDescriptors[0].label).toBe(l1);\n      expect(fm.labeledDescriptors[0].descriptors.length).toBe(2);\n      expect(fm.labeledDescriptors[0].descriptors[0]).toEqual(f1);\n      expect(fm.labeledDescriptors[0].descriptors[1]).toEqual(f2);\n      expect(fm.labeledDescriptors[1].label).toBe(l2);\n      expect(fm.labeledDescriptors[1].descriptors.length).toBe(2);\n      expect(fm.labeledDescriptors[1].descriptors[0]).toEqual(f3);\n      expect(fm.labeledDescriptors[1].descriptors[1]).toEqual(f4);\n    });","file":"tests/globalApi/FaceMatcher.test.ts","skipped":false,"dir":"test"},{"name":"should be 1.0","suites":["iou"],"line":7,"updatePoint":{"line":7,"column":19},"code":"  it('should be 1.0', () => tf.tidy(() => {\n\n    const box = new Rect(0, 0, 20, 20)\n\n    expect(iou(box, box)).toEqual(1)\n\n  }))","file":"tests/ops/iou.test.ts","skipped":false,"dir":"test"},{"name":"should be 0","suites":["iou"],"line":15,"updatePoint":{"line":15,"column":17},"code":"  it('should be 0', () => tf.tidy(() => {\n\n    const box1 = new Rect(0, 0, 20, 20)\n    const box2 = new Rect(20, 20, 20, 20)\n\n    expect(iou(box1, box2)).toEqual(0)\n\n  }))","file":"tests/ops/iou.test.ts","skipped":false,"dir":"test"},{"name":"should be 0.5","suites":["iou"],"line":24,"updatePoint":{"line":24,"column":19},"code":"  it('should be 0.5', () => tf.tidy(() => {\n\n    const box1 = new Rect(0, 0, 20, 20)\n    const box2 = new Rect(0, 0, 10, 20)\n\n    expect(iou(box1, box2)).toEqual(0.5)\n\n  }))","file":"tests/ops/iou.test.ts","skipped":false,"dir":"test"},{"name":"should be 0.5","suites":["iou"],"line":33,"updatePoint":{"line":33,"column":19},"code":"  it('should be 0.5', () => tf.tidy(() => {\n\n    const box1 = new Rect(0, 0, 20, 20)\n    const box2 = new Rect(0, 10, 20, 10)\n\n    expect(iou(box1, box2)).toEqual(0.5)\n\n  }))","file":"tests/ops/iou.test.ts","skipped":false,"dir":"test"},{"name":"is padded to square by 2 columns","suites":["padToSquare","even size"],"line":10,"updatePoint":{"line":10,"column":40},"code":"    it('is padded to square by 2 columns', () => tf.tidy(() => {\n      const imgTensor = tf.tensor4d(Array(24).fill(1), [1, 4, 2, 3])\n      const result = padToSquare(imgTensor)\n\n      expect(result.shape).toEqual([1, 4, 4, 3])\n\n      const paddedCols = tf.unstack(result, 2)\n      expect(paddedCols.length).toEqual(4)\n      expect(paddedCols[0].dataSync()).toEqual(ones(12))\n      expect(paddedCols[1].dataSync()).toEqual(ones(12))\n      expect(paddedCols[2].dataSync()).toEqual(zeros(12))\n      expect(paddedCols[3].dataSync()).toEqual(zeros(12))\n    }))","file":"tests/ops/padToSquare.test.ts","skipped":false,"dir":"test"},{"name":"is padded to square by 2 columns and centered","suites":["padToSquare","even size"],"line":24,"updatePoint":{"line":24,"column":53},"code":"    it('is padded to square by 2 columns and centered', () => tf.tidy(() => {\n      const imgTensor = tf.tensor4d(Array(24).fill(1), [1, 4, 2, 3])\n      const result = padToSquare(imgTensor, true)\n\n      expect(result.shape).toEqual([1, 4, 4, 3])\n\n      const paddedCols = tf.unstack(result, 2)\n      expect(paddedCols.length).toEqual(4)\n      expect(paddedCols[0].dataSync()).toEqual(zeros(12))\n      expect(paddedCols[1].dataSync()).toEqual(ones(12))\n      expect(paddedCols[2].dataSync()).toEqual(ones(12))\n      expect(paddedCols[3].dataSync()).toEqual(zeros(12))\n    }))","file":"tests/ops/padToSquare.test.ts","skipped":false,"dir":"test"},{"name":"is padded to square by 1 column","suites":["padToSquare","even size"],"line":38,"updatePoint":{"line":38,"column":39},"code":"    it('is padded to square by 1 column', () => tf.tidy(() => {\n      const imgTensor = tf.tensor4d(Array(36).fill(1), [1, 4, 3, 3])\n      const result = padToSquare(imgTensor)\n\n      expect(result.shape).toEqual([1, 4, 4, 3])\n\n      const paddedCols = tf.unstack(result, 2)\n      expect(paddedCols.length).toEqual(4)\n      expect(paddedCols[0].dataSync()).toEqual(ones(12))\n      expect(paddedCols[1].dataSync()).toEqual(ones(12))\n      expect(paddedCols[2].dataSync()).toEqual(ones(12))\n      expect(paddedCols[3].dataSync()).toEqual(zeros(12))\n    }))","file":"tests/ops/padToSquare.test.ts","skipped":false,"dir":"test"},{"name":"is padded to square by 1 column and centered","suites":["padToSquare","even size"],"line":52,"updatePoint":{"line":52,"column":52},"code":"    it('is padded to square by 1 column and centered', () => tf.tidy(() => {\n      const imgTensor = tf.tensor4d(Array(36).fill(1), [1, 4, 3, 3])\n      const result = padToSquare(imgTensor, true)\n\n      expect(result.shape).toEqual([1, 4, 4, 3])\n\n      const paddedCols = tf.unstack(result, 2)\n      expect(paddedCols.length).toEqual(4)\n      expect(paddedCols[0].dataSync()).toEqual(ones(12))\n      expect(paddedCols[1].dataSync()).toEqual(ones(12))\n      expect(paddedCols[2].dataSync()).toEqual(ones(12))\n      expect(paddedCols[3].dataSync()).toEqual(zeros(12))\n    }))","file":"tests/ops/padToSquare.test.ts","skipped":false,"dir":"test"},{"name":"is padded to square by 3 columns","suites":["padToSquare","uneven size"],"line":70,"updatePoint":{"line":70,"column":40},"code":"    it('is padded to square by 3 columns', () => tf.tidy(() => {\n      const imgTensor = tf.tensor4d(Array(30).fill(1), [1, 5, 2, 3])\n      const result = padToSquare(imgTensor)\n\n      expect(result.shape).toEqual([1, 5, 5, 3])\n\n      const paddedCols = tf.unstack(result, 2)\n      expect(paddedCols.length).toEqual(5)\n      expect(paddedCols[0].dataSync()).toEqual(ones(15))\n      expect(paddedCols[1].dataSync()).toEqual(ones(15))\n      expect(paddedCols[2].dataSync()).toEqual(zeros(15))\n      expect(paddedCols[3].dataSync()).toEqual(zeros(15))\n      expect(paddedCols[4].dataSync()).toEqual(zeros(15))\n    }))","file":"tests/ops/padToSquare.test.ts","skipped":false,"dir":"test"},{"name":"is padded to square by 3 columns and centered","suites":["padToSquare","uneven size"],"line":85,"updatePoint":{"line":85,"column":53},"code":"    it('is padded to square by 3 columns and centered', () => tf.tidy(() => {\n      const imgTensor = tf.tensor4d(Array(30).fill(1), [1, 5, 2, 3])\n      const result = padToSquare(imgTensor, true)\n\n      expect(result.shape).toEqual([1, 5, 5, 3])\n\n      const paddedCols = tf.unstack(result, 2)\n      expect(paddedCols.length).toEqual(5)\n      expect(paddedCols[0].dataSync()).toEqual(zeros(15))\n      expect(paddedCols[1].dataSync()).toEqual(ones(15))\n      expect(paddedCols[2].dataSync()).toEqual(ones(15))\n      expect(paddedCols[3].dataSync()).toEqual(zeros(15))\n      expect(paddedCols[4].dataSync()).toEqual(zeros(15))\n    }))","file":"tests/ops/padToSquare.test.ts","skipped":false,"dir":"test"},{"name":"is padded to square by 1 column","suites":["padToSquare","uneven size"],"line":100,"updatePoint":{"line":100,"column":39},"code":"    it('is padded to square by 1 column', () => tf.tidy(() => {\n      const imgTensor = tf.tensor4d(Array(60).fill(1), [1, 5, 4, 3])\n      const result = padToSquare(imgTensor)\n\n      expect(result.shape).toEqual([1, 5, 5, 3])\n\n      const paddedCols = tf.unstack(result, 2)\n      expect(paddedCols.length).toEqual(5)\n      expect(paddedCols[0].dataSync()).toEqual(ones(15))\n      expect(paddedCols[1].dataSync()).toEqual(ones(15))\n      expect(paddedCols[2].dataSync()).toEqual(ones(15))\n      expect(paddedCols[3].dataSync()).toEqual(ones(15))\n      expect(paddedCols[4].dataSync()).toEqual(zeros(15))\n    }))","file":"tests/ops/padToSquare.test.ts","skipped":false,"dir":"test"},{"name":"is padded to square by 1 column and centered","suites":["padToSquare","uneven size"],"line":115,"updatePoint":{"line":115,"column":52},"code":"    it('is padded to square by 1 column and centered', () => tf.tidy(() => {\n      const imgTensor = tf.tensor4d(Array(60).fill(1), [1, 5, 4, 3])\n      const result = padToSquare(imgTensor, true)\n\n      expect(result.shape).toEqual([1, 5, 5, 3])\n\n      const paddedCols = tf.unstack(result, 2)\n      expect(paddedCols.length).toEqual(5)\n      expect(paddedCols[0].dataSync()).toEqual(ones(15))\n      expect(paddedCols[1].dataSync()).toEqual(ones(15))\n      expect(paddedCols[2].dataSync()).toEqual(ones(15))\n      expect(paddedCols[3].dataSync()).toEqual(ones(15))\n      expect(paddedCols[4].dataSync()).toEqual(zeros(15))\n    }))","file":"tests/ops/padToSquare.test.ts","skipped":false,"dir":"test"},{"name":"resizes FaceDetection","suites":["resizeResults"],"line":17,"updatePoint":{"line":17,"column":27},"code":"  it('resizes FaceDetection', () => {\n\n    const width = 200\n    const height = 400\n\n    const expected = detection.forSize(width, height)\n    const resized = resizeResults(detection, { width, height })\n\n    expect(resized.imageWidth).toEqual(width)\n    expect(resized.imageHeight).toEqual(height)\n    expectRectClose(resized.box, expected.box, 0)\n\n  })","file":"tests/resizeResults.test.ts","skipped":false,"dir":"test"},{"name":"resizes FaceLandmarks","suites":["resizeResults"],"line":31,"updatePoint":{"line":31,"column":27},"code":"  it('resizes FaceLandmarks', () => {\n\n    const width = 200\n    const height = 400\n\n    const expected = unshiftedLandmarks.forSize(width, height)\n    const resized = resizeResults(unshiftedLandmarks, { width, height })\n\n    expect(resized.imageWidth).toEqual(width)\n    expect(resized.imageHeight).toEqual(height)\n    expectPointsClose(resized.positions, expected.positions, 0)\n\n  })","file":"tests/resizeResults.test.ts","skipped":false,"dir":"test"},{"name":"resizes WithFaceDetection","suites":["resizeResults"],"line":45,"updatePoint":{"line":45,"column":31},"code":"  it('resizes WithFaceDetection', () => {\n\n    const width = 200\n    const height = 400\n\n    const expected = detection.forSize(width, height)\n    const resized = resizeResults(extendWithFaceDetection({}, detection), { width, height })\n\n    expect(resized.detection.imageWidth).toEqual(width)\n    expect(resized.detection.imageHeight).toEqual(height)\n    expectRectClose(resized.detection.box, expected.box, 0)\n\n  })","file":"tests/resizeResults.test.ts","skipped":false,"dir":"test"},{"name":"resizes WithFaceLandmarks","suites":["resizeResults"],"line":59,"updatePoint":{"line":59,"column":31},"code":"  it('resizes WithFaceLandmarks', () => {\n\n    const width = 200\n    const height = 400\n\n    const expectedRect = detection.forSize(width, height)\n    const expectedLandmarks = unshiftedLandmarks.forSize(expectedRect.box.width, expectedRect.box.height)\n    const resized = resizeResults(\n      extendWithFaceLandmarks(\n        extendWithFaceDetection({}, detection),\n        unshiftedLandmarks\n      ),\n      { width, height }\n    )\n\n    expect(resized.detection.imageWidth).toEqual(width)\n    expect(resized.detection.imageHeight).toEqual(height)\n    expectRectClose(resized.detection.box, expectedRect.box, 0)\n\n    expect(resized.unshiftedLandmarks.imageWidth).toEqual(expectedRect.box.width)\n    expect(resized.unshiftedLandmarks.imageHeight).toEqual(expectedRect.box.height)\n    expectPointsClose(resized.unshiftedLandmarks.positions, expectedLandmarks.positions, 0)\n\n  })","file":"tests/resizeResults.test.ts","skipped":false,"dir":"test"},{"name":"scores > 0.7","suites":[],"line":17,"updatePoint":{"line":17,"column":20},"code":"    it('scores > 0.7', async () => {\n      const detections = await ssdMobilenetv1.locateFaces(imgEl, { minConfidence: 0.7 }) as faceapi.FaceDetection[]\n\n      expect(detections.length).toEqual(4)\n\n      const expectedScores = [-1, 0.81, 0.97, 0.88, 0.84, -1]\n      const maxScoreDelta = 0.05\n      const maxBoxDelta = 4\n\n      expectFaceDetections(detections, expectedSsdBoxes, expectedScores, maxScoreDelta, maxBoxDelta)\n    })","file":"tests/ssdMobilenetv1/ssdMobilenetv1.locateFaces.test.ts","skipped":false,"dir":"test"},{"name":"scores > 0.5","suites":[],"line":29,"updatePoint":{"line":29,"column":20},"code":"    it('scores > 0.5', async () => {\n      const detections = await ssdMobilenetv1.locateFaces(imgEl, { minConfidence: 0.5 }) as faceapi.FaceDetection[]\n\n      expect(detections.length).toEqual(6)\n\n      const expectedScores = [0.54, 0.81, 0.97, 0.88, 0.84, 0.61]\n      const maxScoreDelta = 0.05\n      const maxBoxDelta = 5\n\n      expectFaceDetections(detections, expectedSsdBoxes, expectedScores, maxScoreDelta, maxBoxDelta)\n    })","file":"tests/ssdMobilenetv1/ssdMobilenetv1.locateFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces","suites":["ssdMobilenetv1 - node"],"line":24,"updatePoint":{"line":24,"column":22},"code":"    it('detectAllFaces', async () => {\n      const options = new SsdMobilenetv1Options({\n        minConfidence: 0.5\n      })\n\n      const results = await faceapi.detectAllFaces(imgTensor, options)\n\n      const maxScoreDelta = 0.05\n      const maxBoxDelta = 5\n      expect(results.length).toEqual(6)\n      expectFaceDetections(results, expectedSsdBoxes, expectedScores, maxScoreDelta, maxBoxDelta)\n    })","file":"tests/ssdMobilenetv1/ssdMobilenetv1.node.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks()","suites":["ssdMobilenetv1 - node"],"line":37,"updatePoint":{"line":37,"column":42},"code":"    it('detectAllFaces.withFaceLandmarks()', async () => {\n      const options = new SsdMobilenetv1Options({\n        minConfidence: 0.5\n      })\n\n      const results = await faceapi\n        .detectAllFaces(imgTensor, options)\n        .withFaceLandmarks()\n\n      const deltas = {\n        maxScoreDelta: 0.05,\n        maxBoxDelta: 5,\n        maxLandmarksDelta: 4\n      }\n      expect(results.length).toEqual(6)\n      expectFaceDetectionsWithLandmarks(results, expectedFullFaceDescriptions, expectedScores, deltas)\n    })","file":"tests/ssdMobilenetv1/ssdMobilenetv1.node.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withFaceDescriptors()","suites":["ssdMobilenetv1 - node"],"line":55,"updatePoint":{"line":55,"column":64},"code":"    it('detectAllFaces.withFaceLandmarks().withFaceDescriptors()', async () => {\n      const options = new SsdMobilenetv1Options({\n        minConfidence: 0.5\n      })\n\n      const results = await faceapi\n        .detectAllFaces(imgTensor, options)\n        .withFaceLandmarks()\n        .withFaceDescriptors()\n\n      const deltas = {\n        maxScoreDelta: 0.05,\n        maxBoxDelta: 5,\n        maxLandmarksDelta: 4,\n        maxDescriptorDelta: 0.2\n      }\n      expect(results.length).toEqual(6)\n      expectFullFaceDescriptions(results, expectedFullFaceDescriptions, expectedScores, deltas)\n    })","file":"tests/ssdMobilenetv1/ssdMobilenetv1.node.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withFaceDescriptor()","suites":["ssdMobilenetv1 - node"],"line":75,"updatePoint":{"line":75,"column":65},"code":"    it('detectSingleFace.withFaceLandmarks().withFaceDescriptor()', async () => {\n      const options = new SsdMobilenetv1Options({\n        minConfidence: 0.5\n      })\n\n      const result = await faceapi\n        .detectSingleFace(imgTensor, options)\n        .withFaceLandmarks()\n        .withFaceDescriptor()\n\n      const deltas = {\n        maxScoreDelta: 0.05,\n        maxBoxDelta: 5,\n        maxLandmarksDelta: 4,\n        maxDescriptorDelta: 0.2\n      }\n\n      expect(!!result).toBeTruthy()\n      expectFullFaceDescriptions(\n        result ? [result] : [],\n        [expectedFullFaceDescriptions[2]],\n        [expectedScores[2]],\n        deltas\n      )\n    })","file":"tests/ssdMobilenetv1/ssdMobilenetv1.node.test.ts","skipped":false,"dir":"test"},{"name":"no memory leaks","suites":["ssdMobilenetv1 - node"],"line":101,"updatePoint":{"line":101,"column":23},"code":"    it('no memory leaks', async () => {\n      await expectAllTensorsReleased(async () => {\n        await faceapi\n          .detectAllFaces(imgTensor, new SsdMobilenetv1Options())\n          .withFaceLandmarks()\n          .withFaceDescriptors()\n      })\n    })","file":"tests/ssdMobilenetv1/ssdMobilenetv1.node.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces","suites":[],"line":23,"updatePoint":{"line":23,"column":22},"code":"    it('detectAllFaces', async () => {\n      const options = new SsdMobilenetv1Options({\n        minConfidence: 0.5\n      })\n\n      const results = await faceapi.detectAllFaces(imgEl, options)\n\n      const maxScoreDelta = 0.05\n      const maxBoxDelta = 5\n      expect(results.length).toEqual(6)\n      expectFaceDetections(results, expectedSsdBoxes, expectedScores, maxScoreDelta, maxBoxDelta)\n    })","file":"tests/ssdMobilenetv1/ssdMobilenetv1.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks()","suites":[],"line":36,"updatePoint":{"line":36,"column":42},"code":"    it('detectAllFaces.withFaceLandmarks()', async () => {\n      const options = new SsdMobilenetv1Options({\n        minConfidence: 0.5\n      })\n\n      const results = await faceapi\n        .detectAllFaces(imgEl, options)\n        .withFaceLandmarks()\n\n      const deltas = {\n        maxScoreDelta: 0.05,\n        maxBoxDelta: 5,\n        maxLandmarksDelta: 3\n      }\n      expect(results.length).toEqual(6)\n      expectFaceDetectionsWithLandmarks(results, expectedFullFaceDescriptions, expectedScores, deltas)\n    })","file":"tests/ssdMobilenetv1/ssdMobilenetv1.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withFaceDescriptors()","suites":[],"line":54,"updatePoint":{"line":54,"column":64},"code":"    it('detectAllFaces.withFaceLandmarks().withFaceDescriptors()', async () => {\n      const options = new SsdMobilenetv1Options({\n        minConfidence: 0.5\n      })\n\n      const results = await faceapi\n        .detectAllFaces(imgEl, options)\n        .withFaceLandmarks()\n        .withFaceDescriptors()\n\n      const deltas = {\n        maxScoreDelta: 0.05,\n        maxBoxDelta: 5,\n        maxLandmarksDelta: 3,\n        maxDescriptorDelta: 0.2\n      }\n      expect(results.length).toEqual(6)\n      expectFullFaceDescriptions(results, expectedFullFaceDescriptions, expectedScores, deltas)\n    })","file":"tests/ssdMobilenetv1/ssdMobilenetv1.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withFaceDescriptor()","suites":[],"line":74,"updatePoint":{"line":74,"column":65},"code":"    it('detectSingleFace.withFaceLandmarks().withFaceDescriptor()', async () => {\n      const options = new SsdMobilenetv1Options({\n        minConfidence: 0.5\n      })\n\n      const result = await faceapi\n        .detectSingleFace(imgEl, options)\n        .withFaceLandmarks()\n        .withFaceDescriptor()\n\n      const deltas = {\n        maxScoreDelta: 0.05,\n        maxBoxDelta: 5,\n        maxLandmarksDelta: 3,\n        maxDescriptorDelta: 0.2\n      }\n\n      expect(!!result).toBeTruthy()\n      expectFullFaceDescriptions(\n        result ? [result] : [],\n        [expectedFullFaceDescriptions[2]],\n        [expectedScores[2]],\n        deltas\n      )\n    })","file":"tests/ssdMobilenetv1/ssdMobilenetv1.test.ts","skipped":false,"dir":"test"},{"name":"no memory leaks","suites":[],"line":100,"updatePoint":{"line":100,"column":23},"code":"    it('no memory leaks', async () => {\n      await expectAllTensorsReleased(async () => {\n        await faceapi\n          .detectAllFaces(imgEl, new SsdMobilenetv1Options())\n          .withFaceLandmarks()\n          .withFaceDescriptors()\n      })\n    })","file":"tests/ssdMobilenetv1/ssdMobilenetv1.test.ts","skipped":false,"dir":"test"},{"name":"inputSize 320, finds all faces","suites":[],"line":17,"updatePoint":{"line":17,"column":38},"code":"    it('inputSize 320, finds all faces', async () => {\n      const detections = await tinyFaceDetector.locateFaces(imgEl, { inputSize: 320 }) as faceapi.FaceDetection[]\n\n      expect(detections.length).toEqual(6)\n\n      const expectedScores = [0.77, 0.75, 0.88, 0.77, 0.83, 0.85]\n      const maxScoreDelta = 0.05\n      const maxBoxDelta = 40\n\n      expectFaceDetections(detections, expectedTinyFaceDetectorBoxes, expectedScores, maxScoreDelta, maxBoxDelta)\n    })","file":"tests/tinyFaceDetector/tinyFaceDetector.locateFaces.test.ts","skipped":false,"dir":"test"},{"name":"inputSize 416, finds all faces","suites":[],"line":29,"updatePoint":{"line":29,"column":38},"code":"    it('inputSize 416, finds all faces', async () => {\n      const detections = await tinyFaceDetector.locateFaces(imgEl, { inputSize: 416 }) as faceapi.FaceDetection[]\n\n      expect(detections.length).toEqual(6)\n\n      const expectedScores = [0.7, 0.82, 0.93, 0.86, 0.79, 0.84]\n      const maxScoreDelta = 0.05\n      const maxBoxDelta = 5\n\n      expectFaceDetections(detections, expectedTinyFaceDetectorBoxes, expectedScores, maxScoreDelta, maxBoxDelta)\n    })","file":"tests/tinyFaceDetector/tinyFaceDetector.locateFaces.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces","suites":["tinyFaceDetector - node"],"line":24,"updatePoint":{"line":24,"column":22},"code":"    it('detectAllFaces', async () => {\n      const options = new TinyFaceDetectorOptions({\n        inputSize: 416\n      })\n\n      const results = await faceapi.detectAllFaces(imgTensor, options)\n\n      const maxScoreDelta = 0.05\n      const maxBoxDelta = 5\n      expect(results.length).toEqual(6)\n      expectFaceDetections(results, expectedTinyFaceDetectorBoxes, expectedScores, maxScoreDelta, maxBoxDelta)\n    })","file":"tests/tinyFaceDetector/tinyFaceDetector.node.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks()","suites":["tinyFaceDetector - node"],"line":37,"updatePoint":{"line":37,"column":42},"code":"    it('detectAllFaces.withFaceLandmarks()', async () => {\n      const options = new TinyFaceDetectorOptions({\n        inputSize: 416\n      })\n\n      const results = await faceapi\n        .detectAllFaces(imgTensor, options)\n        .withFaceLandmarks()\n\n      const deltas = {\n        maxScoreDelta: 0.05,\n        maxBoxDelta: 5,\n        maxLandmarksDelta: 10\n      }\n      expect(results.length).toEqual(6)\n      expectFaceDetectionsWithLandmarks(results, expectedFullFaceDescriptions, expectedScores, deltas)\n    })","file":"tests/tinyFaceDetector/tinyFaceDetector.node.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withFaceDescriptors()","suites":["tinyFaceDetector - node"],"line":55,"updatePoint":{"line":55,"column":64},"code":"    it('detectAllFaces.withFaceLandmarks().withFaceDescriptors()', async () => {\n      const options = new TinyFaceDetectorOptions({\n        inputSize: 416\n      })\n\n      const results = await faceapi\n        .detectAllFaces(imgTensor, options)\n        .withFaceLandmarks()\n        .withFaceDescriptors()\n\n      const deltas = {\n        maxScoreDelta: 0.05,\n        maxBoxDelta: 5,\n        maxLandmarksDelta: 10,\n        maxDescriptorDelta: 0.2\n      }\n      expect(results.length).toEqual(6)\n      expectFullFaceDescriptions(results, expectedFullFaceDescriptions, expectedScores, deltas)\n    })","file":"tests/tinyFaceDetector/tinyFaceDetector.node.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withFaceDescriptor()","suites":["tinyFaceDetector - node"],"line":75,"updatePoint":{"line":75,"column":65},"code":"    it('detectSingleFace.withFaceLandmarks().withFaceDescriptor()', async () => {\n      const options = new TinyFaceDetectorOptions({\n        inputSize: 416\n      })\n\n      const result = await faceapi\n        .detectSingleFace(imgTensor, options)\n        .withFaceLandmarks()\n        .withFaceDescriptor()\n\n      const deltas = {\n        maxScoreDelta: 0.05,\n        maxBoxDelta: 5,\n        maxLandmarksDelta: 10,\n        maxDescriptorDelta: 0.2\n      }\n\n      expect(!!result).toBeTruthy()\n      expectFullFaceDescriptions(\n        result ? [result] : [],\n        [expectedFullFaceDescriptions[2]],\n        [expectedScores[2]],\n        deltas\n      )\n    })","file":"tests/tinyFaceDetector/tinyFaceDetector.node.test.ts","skipped":false,"dir":"test"},{"name":"no memory leaks","suites":["tinyFaceDetector - node"],"line":101,"updatePoint":{"line":101,"column":23},"code":"    it('no memory leaks', async () => {\n      await expectAllTensorsReleased(async () => {\n        await faceapi\n          .detectAllFaces(imgTensor, new TinyFaceDetectorOptions())\n          .withFaceLandmarks()\n          .withFaceDescriptors()\n      })\n    })","file":"tests/tinyFaceDetector/tinyFaceDetector.node.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces","suites":["detectAllFaces"],"line":31,"updatePoint":{"line":31,"column":24},"code":"      it('detectAllFaces', async () => {\n        const options = new TinyFaceDetectorOptions({\n          inputSize: 416\n        })\n\n        const results = await faceapi.detectAllFaces(imgEl, options)\n\n        expect(results.length).toEqual(6)\n        expectFaceDetections(results, expectedTinyFaceDetectorBoxes, expectedScores, deltas.maxScoreDelta, deltas.maxBoxDelta)\n      })","file":"tests/tinyFaceDetector/tinyFaceDetector.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks()","suites":["detectAllFaces"],"line":42,"updatePoint":{"line":42,"column":44},"code":"      it('detectAllFaces.withFaceLandmarks()', async () => {\n        const options = new TinyFaceDetectorOptions({\n          inputSize: 416\n        })\n\n        const results = await faceapi\n          .detectAllFaces(imgEl, options)\n          .withFaceLandmarks()\n\n        expect(results.length).toEqual(6)\n        expectFaceDetectionsWithLandmarks(results, expectedFullFaceDescriptions, expectedScores, deltas)\n      })","file":"tests/tinyFaceDetector/tinyFaceDetector.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces.withFaceLandmarks().withFaceDescriptors()","suites":["detectAllFaces"],"line":55,"updatePoint":{"line":55,"column":66},"code":"      it('detectAllFaces.withFaceLandmarks().withFaceDescriptors()', async () => {\n        const options = new TinyFaceDetectorOptions({\n          inputSize: 416\n        })\n\n        const results = await faceapi\n          .detectAllFaces(imgEl, options)\n          .withFaceLandmarks()\n          .withFaceDescriptors()\n\n        expect(results.length).toEqual(6)\n        expectFullFaceDescriptions(results, expectedFullFaceDescriptions, expectedScores, deltas)\n      })","file":"tests/tinyFaceDetector/tinyFaceDetector.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace","suites":["detectSingleFace"],"line":73,"updatePoint":{"line":73,"column":26},"code":"      it('detectSingleFace', async () => {\n        const options = new TinyFaceDetectorOptions({\n          inputSize: 416\n        })\n\n        const result = await faceapi\n          .detectSingleFace(imgEl, options)\n\n        expect(!!result).toBeTruthy()\n        expectFaceDetections(\n          result ? [result] : [],\n          [expectedTinyFaceDetectorBoxes[2]],\n          [expectedScores[2]],\n          deltas.maxScoreDelta,\n          deltas.maxBoxDelta\n        )\n      })","file":"tests/tinyFaceDetector/tinyFaceDetector.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks()","suites":["detectSingleFace"],"line":91,"updatePoint":{"line":91,"column":46},"code":"      it('detectSingleFace.withFaceLandmarks()', async () => {\n        const options = new TinyFaceDetectorOptions({\n          inputSize: 416\n        })\n\n        const result = await faceapi\n          .detectSingleFace(imgEl, options)\n          .withFaceLandmarks()\n\n        expect(!!result).toBeTruthy()\n        expectFaceDetectionsWithLandmarks(\n          result ? [result] : [],\n          [expectedFullFaceDescriptions[2]],\n          [expectedScores[2]],\n          deltas\n        )\n      })","file":"tests/tinyFaceDetector/tinyFaceDetector.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace.withFaceLandmarks().withFaceDescriptor()","suites":["detectSingleFace"],"line":109,"updatePoint":{"line":109,"column":67},"code":"      it('detectSingleFace.withFaceLandmarks().withFaceDescriptor()', async () => {\n        const options = new TinyFaceDetectorOptions({\n          inputSize: 416\n        })\n\n        const result = await faceapi\n          .detectSingleFace(imgEl, options)\n          .withFaceLandmarks()\n          .withFaceDescriptor()\n\n        expect(!!result).toBeTruthy()\n        expectFullFaceDescriptions(\n          result ? [result] : [],\n          [expectedFullFaceDescriptions[2]],\n          [expectedScores[2]],\n          deltas\n        )\n      })","file":"tests/tinyFaceDetector/tinyFaceDetector.test.ts","skipped":false,"dir":"test"},{"name":"detectAllFaces","suites":["no memory leaks"],"line":132,"updatePoint":{"line":132,"column":24},"code":"      it('detectAllFaces', async () => {\n        await expectAllTensorsReleased(async () => {\n          await faceapi\n            .detectAllFaces(imgEl, new TinyFaceDetectorOptions())\n            .withFaceLandmarks()\n            .withFaceDescriptors()\n        })\n      })","file":"tests/tinyFaceDetector/tinyFaceDetector.test.ts","skipped":false,"dir":"test"},{"name":"detectSingleFace","suites":["no memory leaks"],"line":141,"updatePoint":{"line":141,"column":26},"code":"      it('detectSingleFace', async () => {\n        await expectAllTensorsReleased(async () => {\n          await faceapi\n            .detectSingleFace(imgEl, new TinyFaceDetectorOptions())\n            .withFaceLandmarks()\n            .withFaceDescriptor()\n        })\n      })","file":"tests/tinyFaceDetector/tinyFaceDetector.test.ts","skipped":false,"dir":"test"},{"name":"0 is valid","suites":["utils","isValidNumber"],"line":7,"updatePoint":{"line":7,"column":18},"code":"    it('0 is valid', () => {\n      expect(utils.isValidNumber(0)).toBe(true)\n    })","file":"utils/index.test.ts","skipped":false,"dir":"test"},{"name":"1 is valid","suites":["utils","isValidNumber"],"line":11,"updatePoint":{"line":11,"column":18},"code":"    it('1 is valid', () => {\n      expect(utils.isValidNumber(1)).toBe(true)\n    })","file":"utils/index.test.ts","skipped":false,"dir":"test"},{"name":"-1 is valid","suites":["utils","isValidNumber"],"line":15,"updatePoint":{"line":15,"column":19},"code":"    it('-1 is valid', () => {\n      expect(utils.isValidNumber(-1)).toBe(true)\n    })","file":"utils/index.test.ts","skipped":false,"dir":"test"},{"name":"NaN is invalid","suites":["utils","isValidNumber"],"line":19,"updatePoint":{"line":19,"column":22},"code":"    it('NaN is invalid', () => {\n      expect(utils.isValidNumber(NaN)).toBe(false)\n    })","file":"utils/index.test.ts","skipped":false,"dir":"test"},{"name":"Infinity is invalid","suites":["utils","isValidNumber"],"line":23,"updatePoint":{"line":23,"column":27},"code":"    it('Infinity is invalid', () => {\n      expect(utils.isValidNumber(Infinity)).toBe(false)\n    })","file":"utils/index.test.ts","skipped":false,"dir":"test"},{"name":"-Infinity is invalid","suites":["utils","isValidNumber"],"line":27,"updatePoint":{"line":27,"column":28},"code":"    it('-Infinity is invalid', () => {\n      expect(utils.isValidNumber(-Infinity)).toBe(false)\n    })","file":"utils/index.test.ts","skipped":false,"dir":"test"},{"name":"null is invalid","suites":["utils","isValidNumber"],"line":31,"updatePoint":{"line":31,"column":23},"code":"    it('null is invalid', () => {\n      expect(utils.isValidNumber(null)).toBe(false)\n    })","file":"utils/index.test.ts","skipped":false,"dir":"test"},{"name":"undefined is invalid","suites":["utils","isValidNumber"],"line":35,"updatePoint":{"line":35,"column":28},"code":"    it('undefined is invalid', () => {\n      expect(utils.isValidNumber(undefined)).toBe(false)\n    })","file":"utils/index.test.ts","skipped":false,"dir":"test"}]}