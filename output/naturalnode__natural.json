{"repo":"NaturalNode/natural","url":"https://github.com/NaturalNode/natural","branch":"master","configs":[{"package":"natural","lang":"js","dir":"spec","framework":"jasmine","pattern":"**/*[._-]{test,spec,unittest,unit}.{ts,js}"}],"tests":[{"name":"should tokenize strings","suites":["aggressive_tokenizer_es"],"line":28,"updatePoint":{"line":28,"column":29,"index":1255},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('hola yo me llamo eduardo y esudié ingeniería')).toEqual(['hola', 'yo', 'me', 'llamo', 'eduardo', 'y', 'esudié', 'ingeniería']);\n  });","file":"aggressive_tokenizer_es_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["aggressive_tokenizer_fr"],"line":30,"updatePoint":{"line":30,"column":29,"index":2187},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize(text)).toEqual(tokenized);\n  });","file":"aggressive_tokenizer_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should handle hyphens in words correctly","suites":["aggressive_tokenizer_fr"],"line":33,"updatePoint":{"line":33,"column":46,"index":2313},"code":"  it('should handle hyphens in words correctly', function () {\n    const sentence = 'Des sous-pages dans le sous-bois de la ville de Paris';\n    const res = tokenizer.tokenize(sentence);\n    const expectedRes = ['Des', 'sous-pages', 'dans', 'le', 'sous-bois', 'de', 'la', 'ville', 'de', 'Paris'];\n    expect(res).toEqual(expectedRes);\n  });","file":"aggressive_tokenizer_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["aggressive_tokenizer_nl"],"line":28,"updatePoint":{"line":28,"column":29,"index":1272},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('\\'s Morgens is het nog erg koud, vertelde de weerman over een van de radio\\'s')).toEqual(['\\'s', 'Morgens', 'is', 'het', 'nog', 'erg', 'koud', 'vertelde', 'de', 'weerman', 'over', 'een', 'van', 'de', 'radio\\'s']);\n  });","file":"aggressive_tokenizer_nl_spec.js","skipped":false,"dir":"spec"},{"name":"should handle hyphens in words correctly","suites":["aggressive_tokenizer_nl"],"line":31,"updatePoint":{"line":31,"column":46,"index":1586},"code":"  it('should handle hyphens in words correctly', function () {\n    const sentence = 'clearing-systeem front-office-automatisering christelijk-historisch mond-op-mond, kant-en-klaar, kruidje-roer-me-niet, doe-het-zelver';\n    const res = tokenizer.tokenize(sentence);\n    const expectedRes = ['clearing-systeem', 'front-office-automatisering', 'christelijk-historisch', 'mond-op-mond', 'kant-en-klaar', 'kruidje-roer-me-niet', 'doe-het-zelver'];\n    expect(res).toEqual(expectedRes);\n  });","file":"aggressive_tokenizer_nl_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["aggressive_tokenizer_pt"],"line":28,"updatePoint":{"line":28,"column":29,"index":1255},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('isso é coração')).toEqual(['isso', 'é', 'coração']);\n  });","file":"aggressive_tokenizer_pt_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow punctuation","suites":["aggressive_tokenizer_pt"],"line":43,"updatePoint":{"line":43,"column":32,"index":1761},"code":"  it('should swallow punctuation', function () {\n    expect(tokenizer.tokenize('isso é coração, no')).toEqual(['isso', 'é', 'coração', 'no']);\n  });","file":"aggressive_tokenizer_pt_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow final punctuation","suites":["aggressive_tokenizer_pt"],"line":46,"updatePoint":{"line":46,"column":38,"index":1916},"code":"  it('should swallow final punctuation', function () {\n    expect(tokenizer.tokenize('isso é coração, no?')).toEqual(['isso', 'é', 'coração', 'no']);\n  });","file":"aggressive_tokenizer_pt_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow initial punctuation","suites":["aggressive_tokenizer_pt"],"line":49,"updatePoint":{"line":49,"column":40,"index":2074},"code":"  it('should swallow initial punctuation', function () {\n    expect(tokenizer.tokenize('.isso é coração, no')).toEqual(['isso', 'é', 'coração', 'no']);\n  });","file":"aggressive_tokenizer_pt_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow duplicate punctuation","suites":["aggressive_tokenizer_pt"],"line":52,"updatePoint":{"line":52,"column":42,"index":2234},"code":"  it('should swallow duplicate punctuation', function () {\n    expect(tokenizer.tokenize('eu vou... pause')).toEqual(['eu', 'vou', 'pause']);\n  });","file":"aggressive_tokenizer_pt_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["aggressive_tokenizer"],"line":28,"updatePoint":{"line":28,"column":29,"index":1267},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('these are things')).toEqual(['these', 'are', 'things']);\n  });","file":"aggressive_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow punctuation","suites":["aggressive_tokenizer"],"line":43,"updatePoint":{"line":43,"column":32,"index":1785},"code":"  it('should swallow punctuation', function () {\n    expect(tokenizer.tokenize('these are things, no')).toEqual(['these', 'are', 'things', 'no']);\n  });","file":"aggressive_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow final punctuation","suites":["aggressive_tokenizer"],"line":46,"updatePoint":{"line":46,"column":38,"index":1944},"code":"  it('should swallow final punctuation', function () {\n    expect(tokenizer.tokenize('these are things, no?')).toEqual(['these', 'are', 'things', 'no']);\n  });","file":"aggressive_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow initial punctuation","suites":["aggressive_tokenizer"],"line":49,"updatePoint":{"line":49,"column":40,"index":2106},"code":"  it('should swallow initial punctuation', function () {\n    expect(tokenizer.tokenize('.these are things, no')).toEqual(['these', 'are', 'things', 'no']);\n  });","file":"aggressive_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow duplicate punctuation","suites":["aggressive_tokenizer"],"line":52,"updatePoint":{"line":52,"column":42,"index":2270},"code":"  it('should swallow duplicate punctuation', function () {\n    expect(tokenizer.tokenize('i shal... pause')).toEqual(['i', 'shal', 'pause']);\n  });","file":"aggressive_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should remove underscores","suites":["aggressive_tokenizer"],"line":55,"updatePoint":{"line":55,"column":31,"index":2407},"code":"  it('should remove underscores', function () {\n    expect(tokenizer.tokenize('_ hi_this_is_a_test_case_ for__removing___underscores_')).toEqual(['hi', 'this', 'is', 'a', 'test', 'case', 'for', 'removing', 'underscores']);\n  });","file":"aggressive_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["aggressive_tokenizer_sv"],"line":28,"updatePoint":{"line":28,"column":29,"index":1273},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('Ett tu tre')).toEqual(['Ett', 'tu', 'tre']);\n  });","file":"aggressive_tokenizer_sv_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow punctuation","suites":["aggressive_tokenizer_sv"],"line":39,"updatePoint":{"line":39,"column":32,"index":1585},"code":"  it('should swallow punctuation', function () {\n    expect(tokenizer.tokenize('Ett, tu, tre')).toEqual(['Ett', 'tu', 'tre']);\n  });","file":"aggressive_tokenizer_sv_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow final punctuation","suites":["aggressive_tokenizer_sv"],"line":42,"updatePoint":{"line":42,"column":38,"index":1724},"code":"  it('should swallow final punctuation', function () {\n    expect(tokenizer.tokenize('Ett, tu, tre?')).toEqual(['Ett', 'tu', 'tre']);\n  });","file":"aggressive_tokenizer_sv_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow initial punctuation","suites":["aggressive_tokenizer_sv"],"line":45,"updatePoint":{"line":45,"column":40,"index":1866},"code":"  it('should swallow initial punctuation', function () {\n    expect(tokenizer.tokenize('.Ett, tu, tre')).toEqual(['Ett', 'tu', 'tre']);\n  });","file":"aggressive_tokenizer_sv_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow duplicate punctuation","suites":["aggressive_tokenizer_sv"],"line":48,"updatePoint":{"line":48,"column":42,"index":2010},"code":"  it('should swallow duplicate punctuation', function () {\n    expect(tokenizer.tokenize('Ett, tu... tre')).toEqual(['Ett', 'tu', 'tre']);\n  });","file":"aggressive_tokenizer_sv_spec.js","skipped":false,"dir":"spec"},{"name":"should not split on hyphen or Swedish letters","suites":["aggressive_tokenizer_sv"],"line":51,"updatePoint":{"line":51,"column":51,"index":2164},"code":"  it('should not split on hyphen or Swedish letters', function () {\n    expect(tokenizer.tokenize('It-bolaget ägs av en 30-åring')).toEqual(['It-bolaget', 'ägs', 'av', 'en', '30-åring']);\n  });","file":"aggressive_tokenizer_sv_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["aggressive_tokenizer_vi"],"line":28,"updatePoint":{"line":28,"column":29,"index":1265},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('Tôi bị lạc')).toEqual(['Tôi', 'bị', 'lạc']);\n  });","file":"aggressive_tokenizer_vi_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow final punctuation","suites":["aggressive_tokenizer_vi"],"line":43,"updatePoint":{"line":43,"column":38,"index":1830},"code":"  it('should swallow final punctuation', function () {\n    expect(tokenizer.tokenize('Làm ơn đợi một lát!')).toEqual(['Làm', 'ơn', 'đợi', 'một', 'lát']);\n  });","file":"aggressive_tokenizer_vi_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow final punctuation","suites":["aggressive_tokenizer_vi"],"line":46,"updatePoint":{"line":46,"column":38,"index":1990},"code":"  it('should swallow final punctuation', function () {\n    expect(tokenizer.tokenize('Tôi đang tìm John.')).toEqual(['Tôi', 'đang', 'tìm', 'John']);\n  });","file":"aggressive_tokenizer_vi_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow initial punctuation","suites":["aggressive_tokenizer_vi"],"line":49,"updatePoint":{"line":49,"column":40,"index":2147},"code":"  it('should swallow initial punctuation', function () {\n    expect(tokenizer.tokenize('.Đi thẳng, sau đó rẽ trái / phải')).toEqual(['Đi', 'thẳng', 'sau', 'đó', 'rẽ', 'trái', 'phải']);\n  });","file":"aggressive_tokenizer_vi_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow duplicate punctuation","suites":["aggressive_tokenizer_vi"],"line":52,"updatePoint":{"line":52,"column":42,"index":2340},"code":"  it('should swallow duplicate punctuation', function () {\n    expect(tokenizer.tokenize('.Xin giữ máy!')).toEqual(['Xin', 'giữ', 'máy']);\n  });","file":"aggressive_tokenizer_vi_spec.js","skipped":false,"dir":"spec"},{"name":"should classify with arrays","suites":["bayes classifier","classifier"],"line":41,"updatePoint":{"line":41,"column":35,"index":1802},"code":"    it('should classify with arrays', function () {\n      const classifier = setupClassifier();\n      classifier.train();\n      expect(classifier.classify(['bug', 'code'])).toBe('computing');\n      expect(classifier.classify(['read', 'thing'])).toBe('literature');\n    });","file":"bayes_classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should classify with parallel training","suites":["bayes classifier","classifier"],"line":47,"updatePoint":{"line":47,"column":46,"index":2086},"code":"    it('should classify with parallel training', function () {\n      const classifier = setupClassifier();\n      // Check for parallel method\n      if (classifier.trainParallel) {\n        classifier.trainParallel(2, function (err) {\n          if (err) {\n            console.log(err);\n            return;\n          }\n          expect(classifier.classify(['bug', 'code'])).toBe('computing');\n          expect(classifier.classify(['read', 'thing'])).toBe('literature');\n          // asyncSpecDone();\n        });\n      }\n    });","file":"bayes_classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should classify with parallel batched training","suites":["bayes classifier","classifier"],"line":63,"updatePoint":{"line":63,"column":54,"index":2620},"code":"    it('should classify with parallel batched training', function () {\n      const classifier = setupClassifier();\n      if (classifier.trainParallelBatches) {\n        // Check for parallel method\n        classifier.events.on('doneTraining', function () {\n          expect(classifier.classify(['bug', 'code'])).toBe('computing');\n          expect(classifier.classify(['read', 'thing'])).toBe('literature');\n          // asyncSpecDone();\n        });\n\n        classifier.trainParallelBatches({\n          numThreads: 2,\n          batchSize: 2\n        });\n      }\n    });","file":"bayes_classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should provide all classification scores","suites":["bayes classifier","classifier"],"line":79,"updatePoint":{"line":79,"column":48,"index":3182},"code":"    it('should provide all classification scores', function () {\n      const classifier = setupClassifier();\n      classifier.train();\n      expect(classifier.getClassifications('i write code')[0].label).toBe('computing');\n      expect(classifier.getClassifications('i write code')[1].label).toBe('literature');\n    });","file":"bayes_classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should classify with strings","suites":["bayes classifier","classifier"],"line":95,"updatePoint":{"line":95,"column":36,"index":3990},"code":"    it('should classify with strings', function () {\n      const classifier = setupClassifierWithSentences();\n      classifier.train();\n      expect(classifier.classify('a bug in the code')).toBe('computing');\n      expect(classifier.classify('read all the books')).toBe('literature');\n    });","file":"bayes_classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should classify and re-classify after document-removal","suites":["bayes classifier","classifier"],"line":101,"updatePoint":{"line":101,"column":62,"index":4310},"code":"    it('should classify and re-classify after document-removal', function () {\n      const classifier = new natural.BayesClassifier();\n      let item;\n      const classifications = {};\n\n      // Add some good/bad docs and train\n      classifier.addDocument('foo bar baz', 'good');\n      classifier.addDocument('qux zooby', 'bad');\n      classifier.addDocument('asdf qwer', 'bad');\n      classifier.train();\n      expect(classifier.classify('foo')).toBe('good');\n      expect(classifier.classify('qux')).toBe('bad');\n\n      // Remove one of the bad docs, retrain\n      classifier.removeDocument('qux zooby', 'bad');\n      classifier.retrain();\n\n      // Simple `classify` will still return a single result, even if\n      // ratio for each side is equal -- have to compare actual values in\n      // the classifications, should be equal since qux is unclassified\n      const arr = classifier.getClassifications('qux');\n      for (let i = 0, ii = arr.length; i < ii; i++) {\n        item = arr[i];\n        classifications[item.label] = item.value;\n      }\n      expect(classifications.good).toEqual(classifications.bad);\n\n      // Re-classify as good, retrain\n      classifier.addDocument('qux zooby', 'good');\n      classifier.retrain();\n\n      // Should now be good, original docs should be unaffected\n      expect(classifier.classify('foo')).toBe('good');\n      expect(classifier.classify('qux')).toBe('good');\n    });","file":"bayes_classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should serialize and deserialize a working classifier","suites":["bayes classifier","classifier"],"line":136,"updatePoint":{"line":136,"column":61,"index":5726},"code":"    it('should serialize and deserialize a working classifier', function () {\n      const classifier = setupClassifierWithSentences();\n      const obj = JSON.stringify(classifier);\n      const newClassifier = natural.BayesClassifier.restore(JSON.parse(obj));\n      newClassifier.addDocument('kick a ball', 'sports');\n      newClassifier.addDocument('hit some balls', 'sports');\n      newClassifier.addDocument('kick and punch', 'sports');\n      newClassifier.train();\n      expect(newClassifier.classify('a bug in the code')).toBe('computing');\n      expect(newClassifier.classify('read all the books')).toBe('literature');\n      expect(newClassifier.classify('kick butt')).toBe('sports');\n    });","file":"bayes_classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should accept an optional smoothing parameter for the Bayesian estimates","suites":["bayes classifier","classifier"],"line":184,"updatePoint":{"line":184,"column":80,"index":8143},"code":"    it('should accept an optional smoothing parameter for the Bayesian estimates', function () {\n      const defaultClassifier = new natural.BayesClassifier();\n      const PorterStemmer = require('../lib/natural/stemmers/porter_stemmer');\n      const newClassifier1 = new natural.BayesClassifier(PorterStemmer);\n      const newClassifier2 = new natural.BayesClassifier(PorterStemmer, 0.1);\n      expect(defaultClassifier.classifier.smoothing).toBe(1.0);\n      expect(newClassifier1.classifier.smoothing).toBe(1.0);\n      expect(newClassifier2.classifier.smoothing).toBe(0.1);\n    });","file":"bayes_classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should process an English newspaper article just like the dariusk/pos-js module","suites":["Brill's POS Tagger"],"line":43,"updatePoint":{"line":43,"column":85,"index":2071},"code":"  it('should process an English newspaper article just like the dariusk/pos-js module', function () {\n    lexicon = new natural.Lexicon('EN', 'NN');\n    expect(lexicon.nrEntries()).toBeGreaterThan(0);\n    ruleSet = new natural.RuleSet('EN');\n    expect(ruleSet.nrRules()).toBeGreaterThan(0);\n    brillPOSTagger = new natural.BrillPOSTagger(lexicon, ruleSet);\n    englishSentences.forEach(function (sentence, index) {\n      const tokenizedSentence = tokenizer.tokenize(sentence);\n      const taggedSentence = brillPOSTagger.tag(tokenizedSentence);\n      expect(compareTaggedSentences(englishTagResults[index], taggedSentence)).toBe(true);\n    });\n  });","file":"brill_pos_tagger_spec.js","skipped":false,"dir":"spec"},{"name":"should tag a Dutch news paper article","suites":["Brill's POS Tagger"],"line":55,"updatePoint":{"line":55,"column":43,"index":2681},"code":"  it('should tag a Dutch news paper article', function () {\n    lexicon = new natural.Lexicon('DU', 'N');\n    expect(lexicon.nrEntries()).toBeGreaterThan(0);\n    const ruleSet = new natural.RuleSet('DU');\n    expect(ruleSet.nrRules()).toBeGreaterThan(0);\n    brillPOSTagger = new natural.BrillPOSTagger(lexicon, ruleSet);\n    dutchSentences.forEach(function (sentence, index) {\n      const tokenizedSentence = tokenizer.tokenize(sentence);\n      const taggedSentence = brillPOSTagger.tag(tokenizedSentence);\n      expect(tokenizedSentence.length).toEqual(taggedSentence.taggedWords.length);\n    });\n  });","file":"brill_pos_tagger_spec.js","skipped":false,"dir":"spec"},{"name":"should split the corpus in a training and testing corpus","suites":["Brill's POS Trainer"],"line":47,"updatePoint":{"line":47,"column":62,"index":1768},"code":"  it('should split the corpus in a training and testing corpus', function () {\n    corpus = new Corpus(brownCorpus, JSON_FLAG, SentenceClass);\n    DEBUG && console.log('Corpus: ' + JSON.stringify(corpus, null, 2));\n    corpora = corpus.splitInTrainAndTest(percentageTrain);\n    expect(corpora[0].nrSentences() + corpora[1].nrSentences()).toEqual(corpus.nrSentences());\n  });","file":"brill_pos_trainer_spec.js","skipped":false,"dir":"spec"},{"name":"should build a lexicon from the training corpus","suites":["Brill's POS Trainer"],"line":53,"updatePoint":{"line":53,"column":53,"index":2134},"code":"  it('should build a lexicon from the training corpus', function () {\n    trainLexicon = corpora[0].buildLexicon();\n    // Set default category to noun (NN)\n    // and default category for capitalised words to proper noun (NP)\n    trainLexicon.setDefaultCategories('NN', 'NP');\n    expect(trainLexicon.nrEntries()).not.toEqual(0);\n  });","file":"brill_pos_trainer_spec.js","skipped":false,"dir":"spec"},{"name":"should set up the rule templates","suites":["Brill's POS Trainer"],"line":60,"updatePoint":{"line":60,"column":38,"index":2456},"code":"  it('should set up the rule templates', function () {\n    templates = selectRuleTemplates(templateNames);\n    expect(templates.length).toEqual(templateNames.length);\n  });","file":"brill_pos_trainer_spec.js","skipped":false,"dir":"spec"},{"name":"should train on the training corpus to derive transformation rules","suites":["Brill's POS Trainer"],"line":64,"updatePoint":{"line":64,"column":72,"index":2663},"code":"  it('should train on the training corpus to derive transformation rules', function () {\n    trainer = new natural.BrillPOSTrainer(1);\n    ruleSet = trainer.train(corpora[0], templates, trainLexicon);\n    expect(ruleSet.nrRules()).toBeGreaterThan(0);\n    expect(trainer.printRulesWithScores()).not.toEqual('');\n  });","file":"brill_pos_trainer_spec.js","skipped":false,"dir":"spec"},{"name":"should test the derived transformation rules on the test corpus","suites":["Brill's POS Trainer"],"line":70,"updatePoint":{"line":70,"column":69,"index":2977},"code":"  it('should test the derived transformation rules on the test corpus', function () {\n    const tagger = new natural.BrillPOSTagger(trainLexicon, ruleSet);\n    const tester = new natural.BrillPOSTester();\n    const scores = tester.test(corpora[1], tagger);\n    expect(scores[1]).toBeGreaterThan(0);\n  });","file":"brill_pos_trainer_spec.js","skipped":false,"dir":"spec"},{"name":"should ignore an empty document","suites":["classifier","addDocument"],"line":28,"updatePoint":{"line":28,"column":39,"index":1238},"code":"    it('should ignore an empty document', function () {\n      const classifier = new natural.BayesClassifier();\n      classifier.addDocument('', 'philosophy');\n      expect(classifier.docs.length).toBe(0);\n    });","file":"classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should increment features","suites":["classifier","addDocument"],"line":33,"updatePoint":{"line":33,"column":33,"index":1446},"code":"    it('should increment features', function () {\n      const classifier = new natural.BayesClassifier();\n      classifier.addDocument('foo', '');\n      classifier.addDocument('foo', '');\n      classifier.addDocument('bar', '');\n      classifier.addDocument('bar', '');\n      classifier.addDocument('bar', '');\n      classifier.addDocument('baz', '');\n      expect(classifier.docs.length).toBe(6);\n      expect(classifier.features.foo).toBe(2);\n      expect(classifier.features.bar).toBe(3);\n      expect(classifier.features.baz).toBe(1);\n    });","file":"classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should be emitted when a document is classified","suites":["classifier","events emitters"],"line":48,"updatePoint":{"line":48,"column":55,"index":2065},"code":"    it('should be emitted when a document is classified', function () {\n      const classifier = new natural.BayesClassifier();\n      classifier.addDocument('i fixed the box', 'computing');\n      classifier.addDocument('i write code', 'computing');\n      classifier.addDocument('nasty script code', 'computing');\n      classifier.addDocument('write a book', 'literature');\n      classifier.addDocument('read a book', 'literature');\n      classifier.addDocument('study the books', 'literature');\n      classifier.train();\n      const pushedEvents = [];\n      function eventRegister(obj) {\n        pushedEvents.push(obj);\n      }\n      ;\n      function assertEventResults() {\n        teardown();\n        expect(pushedEvents[0].index).toBe(0);\n        expect(pushedEvents[0].total).toBe(6);\n        expect(pushedEvents.length).toBe(6);\n      }\n      function teardown() {\n        classifier.events.removeListener('trainedWithDocument', eventRegister);\n        classifier.events.removeListener('doneTraining', assertEventResults);\n      }\n      classifier.events.on('trainedWithDocument', eventRegister);\n      classifier.events.on('doneTraining', assertEventResults);\n    });","file":"classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should emit events only on an instance of Classifier","suites":["classifier","events emitters"],"line":75,"updatePoint":{"line":75,"column":60,"index":3243},"code":"    it('should emit events only on an instance of Classifier', function () {\n      const classifier = new natural.BayesClassifier();\n      classifier.addDocument('i fixed the box', 'computing');\n      classifier.addDocument('i write code', 'computing');\n      classifier.addDocument('write a book', 'literature');\n      classifier.addDocument('study the books', 'literature');\n      const pushedEvents = [];\n      function eventRegister(obj) {\n        pushedEvents.push(obj);\n      }\n      ;\n      function assertEventResults() {\n        teardown();\n        expect(pushedEvents.length).toBe(0);\n      }\n      function teardown() {\n        classifier.events.removeListener('trainedWithDocument', eventRegister);\n        classifier.events.removeListener('doneTraining', assertEventResults);\n      }\n      const classifier2 = new natural.BayesClassifier();\n      classifier2.events.on('trainedWithDocument', eventRegister);\n      classifier.events.on('doneTraining', assertEventResults);\n      classifier.train();\n    });","file":"classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should do nothing if text is not a string","suites":["classifier","removeDocument"],"line":109,"updatePoint":{"line":109,"column":49,"index":4651},"code":"    it('should do nothing if text is not a string', function () {\n      expect(classifier.docs.length).toBe(4);\n      classifier.removeDocument(['write a book'], 'literature');\n      classifier.removeDocument(['study the books'], 'literature');\n      expect(classifier.docs.length).toBe(4);\n    });","file":"classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should do nothing if text is not a match","suites":["classifier","removeDocument"],"line":115,"updatePoint":{"line":115,"column":48,"index":4949},"code":"    it('should do nothing if text is not a match', function () {\n      expect(classifier.docs.length).toBe(4);\n      classifier.removeDocument('something else', 'literature');\n      classifier.removeDocument('another thing', 'literature');\n      expect(classifier.docs.length).toBe(4);\n    });","file":"classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should handle 1er cases","suites":["count_inflector"],"line":27,"updatePoint":{"line":27,"column":29,"index":1234},"code":"  it('should handle 1er cases', function () {\n    expect(CountInflector.nth(1)).toBe('1er');\n  });","file":"count_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should handle the 2e cases","suites":["count_inflector"],"line":30,"updatePoint":{"line":30,"column":32,"index":1336},"code":"  it('should handle the 2e cases', function () {\n    expect(CountInflector.nth(0)).toBe('0e');\n    expect(CountInflector.nth(2)).toBe('2e');\n    expect(CountInflector.nth(3)).toBe('3e');\n    expect(CountInflector.nth(5)).toBe('5e');\n    expect(CountInflector.nth(11)).toBe('11e');\n    expect(CountInflector.nth(100)).toBe('100e');\n    expect(CountInflector.nth(999)).toBe('999e');\n  });","file":"count_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should handle roman numerals","suites":["count_inflector"],"line":39,"updatePoint":{"line":39,"column":34,"index":1725},"code":"  it('should handle roman numerals', function () {\n    expect(CountInflector.nth('I')).toBe('Ier');\n    expect(CountInflector.nth('XX')).toBe('XXe');\n  });","file":"count_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should handle 1st cases","suites":["count_inflector"],"line":27,"updatePoint":{"line":27,"column":29,"index":1227},"code":"  it('should handle 1st cases', function () {\n    expect(CountInflector.nth(1)).toBe('1st');\n    expect(CountInflector.nth(101)).toBe('101st');\n    expect(CountInflector.nth(11)).not.toBe('11st');\n    expect(CountInflector.nth(111)).not.toBe('111st');\n  });","file":"count_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle the 12th cases","suites":["count_inflector"],"line":33,"updatePoint":{"line":33,"column":34,"index":1490},"code":"  it('should handle the 12th cases', function () {\n    expect(CountInflector.nth(12)).toBe('12th');\n    expect(CountInflector.nth(112)).toBe('112th');\n    expect(CountInflector.nth(1112)).toBe('1112th');\n  });","file":"count_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle the 11th cases","suites":["count_inflector"],"line":38,"updatePoint":{"line":38,"column":34,"index":1700},"code":"  it('should handle the 11th cases', function () {\n    expect(CountInflector.nth(11)).toBe('11th');\n    expect(CountInflector.nth(111)).toBe('111th');\n    expect(CountInflector.nth(1111)).toBe('1111th');\n  });","file":"count_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle the 13th cases","suites":["count_inflector"],"line":43,"updatePoint":{"line":43,"column":34,"index":1910},"code":"  it('should handle the 13th cases', function () {\n    expect(CountInflector.nth(13)).toBe('13th');\n    expect(CountInflector.nth(113)).toBe('113th');\n    expect(CountInflector.nth(1113)).toBe('1113th');\n  });","file":"count_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle the th cases","suites":["count_inflector"],"line":48,"updatePoint":{"line":48,"column":32,"index":2118},"code":"  it('should handle the th cases', function () {\n    expect(CountInflector.nth(10)).toBe('10th');\n    expect(CountInflector.nth(4)).toBe('4th');\n    expect(CountInflector.nth(400)).toBe('400th');\n    expect(CountInflector.nth(404)).toBe('404th');\n    expect(CountInflector.nth(5)).toBe('5th');\n    expect(CountInflector.nth(5000)).toBe('5000th');\n    expect(CountInflector.nth(5005)).toBe('5005th');\n    expect(CountInflector.nth(9)).toBe('9th');\n    expect(CountInflector.nth(90009)).toBe('90009th');\n    expect(CountInflector.nth(90000)).toBe('90000th');\n  });","file":"count_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle 2nd cases","suites":["count_inflector"],"line":60,"updatePoint":{"line":60,"column":29,"index":2678},"code":"  it('should handle 2nd cases', function () {\n    expect(CountInflector.nth(2)).toBe('2nd');\n    expect(CountInflector.nth(12)).not.toBe('12nd');\n  });","file":"count_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle 3rd cases","suites":["count_inflector"],"line":64,"updatePoint":{"line":64,"column":29,"index":2830},"code":"  it('should handle 3rd cases', function () {\n    expect(CountInflector.nth(3)).toBe('3rd');\n    expect(CountInflector.nth(13)).not.toBe('13rd');\n  });","file":"count_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should be 0 when given equal strings","suites":["DamerauLevenshtein","default"],"line":7,"updatePoint":{"line":7,"column":44,"index":375},"code":"    it('should be 0 when given equal strings', function () {\n      expect(damerauLevenshtein('test', 'test')).toBe(0);\n    });","file":"damerau_levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should calculate 1 for adjacent transposition","suites":["DamerauLevenshtein","default"],"line":10,"updatePoint":{"line":10,"column":53,"index":511},"code":"    it('should calculate 1 for adjacent transposition', function () {\n      expect(damerauLevenshtein('za', 'az')).toBe(1);\n      expect(damerauLevenshtein('Tomato', 'oTmato')).toBe(1);\n    });","file":"damerau_levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should handle custom transposition_cost","suites":["DamerauLevenshtein","default"],"line":14,"updatePoint":{"line":14,"column":47,"index":699},"code":"    it('should handle custom transposition_cost', function () {\n      expect(damerauLevenshtein('za', 'az', {\n        transposition_cost: 0\n      })).toBe(0);\n    });","file":"damerau_levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should calculate 2 when there are 2 transpositions","suites":["DamerauLevenshtein","default"],"line":19,"updatePoint":{"line":19,"column":58,"index":877},"code":"    it('should calculate 2 when there are 2 transpositions', function () {\n      expect(damerauLevenshtein('tomato', 'otmaot')).toBe(2);\n    });","file":"damerau_levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should calculate 2 for 1 transposition and 1 insertion","suites":["DamerauLevenshtein","default"],"line":22,"updatePoint":{"line":22,"column":62,"index":1026},"code":"    it('should calculate 2 for 1 transposition and 1 insertion', function () {\n      expect(damerauLevenshtein('CA', 'ABC')).toBe(2);\n      expect(damerauLevenshtein('a cat', 'a abct')).toBe(2);\n    });","file":"damerau_levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should calculate 0 for equal strings","suites":["DamerauLevenshtein","options.restricted = true"],"line":31,"updatePoint":{"line":31,"column":44,"index":1326},"code":"    it('should calculate 0 for equal strings', function () {\n      expect(damerauLevenshtein('identity', 'identity', restricted)).toBe(0);\n    });","file":"damerau_levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should calculate 1 for an adjacent transposition","suites":["DamerauLevenshtein","options.restricted = true"],"line":34,"updatePoint":{"line":34,"column":56,"index":1485},"code":"    it('should calculate 1 for an adjacent transposition', function () {\n      expect(damerauLevenshtein('za', 'az', restricted)).toBe(1);\n    });","file":"damerau_levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should not count transposition more than 1 char away","suites":["DamerauLevenshtein","options.restricted = true"],"line":37,"updatePoint":{"line":37,"column":60,"index":1636},"code":"    it('should not count transposition more than 1 char away', function () {\n      expect(damerauLevenshtein('CA', 'ABC', restricted)).toBe(3);\n    });","file":"damerau_levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should combine search with Damerau","suites":["DamerauLevenshtein","options.restricted = true"],"line":41,"updatePoint":{"line":41,"column":40,"index":1774},"code":"  it('should combine search with Damerau', function () {\n    const source = 'The RainCoat BookStore';\n    const target = 'All the best books are here at the Rain Coats Book Store';\n    const result = damerauLevenshteinSearch(source, target);\n    expect(result).toEqual({\n      substring: 'the Rain Coats Book Store',\n      distance: 4,\n      offset: 31\n    });\n  });","file":"damerau_levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should handle exact matches","suites":["dice"],"line":5,"updatePoint":{"line":5,"column":33,"index":146},"code":"  it('should handle exact matches', function () {\n    expect(dice('john', 'john')).toBe(1);\n  });","file":"dice_coefficient_spec.js","skipped":false,"dir":"spec"},{"name":"should match single character words","suites":["dice"],"line":8,"updatePoint":{"line":8,"column":41,"index":252},"code":"  it('should match single character words', function () {\n    expect(dice('a', 'a')).toBe(1);\n    expect(dice('a', 'b')).toBe(0);\n  });","file":"dice_coefficient_spec.js","skipped":false,"dir":"spec"},{"name":"should handle total mis-matches","suites":["dice"],"line":12,"updatePoint":{"line":12,"column":37,"index":384},"code":"  it('should handle total mis-matches', function () {\n    expect(dice('john', 'matt')).toBe(0);\n  });","file":"dice_coefficient_spec.js","skipped":false,"dir":"spec"},{"name":"should handle a typical case","suites":["dice"],"line":17,"updatePoint":{"line":17,"column":34,"index":550},"code":"  it('should handle a typical case', function () {\n    expect(dice('night', 'nacht')).toBe(0.25);\n  });","file":"dice_coefficient_spec.js","skipped":false,"dir":"spec"},{"name":"should sanitize case","suites":["dice"],"line":20,"updatePoint":{"line":20,"column":26,"index":646},"code":"  it('should sanitize case', function () {\n    expect(dice('night', 'NIGHT')).toBe(1);\n  });","file":"dice_coefficient_spec.js","skipped":false,"dir":"spec"},{"name":"should sanitize spacing","suites":["dice"],"line":23,"updatePoint":{"line":23,"column":29,"index":742},"code":"  it('should sanitize spacing', function () {\n    expect(dice('the   space', 'the space')).toBe(1);\n  });","file":"dice_coefficient_spec.js","skipped":false,"dir":"spec"},{"name":"should compare complete texts","suites":["dice"],"line":26,"updatePoint":{"line":26,"column":35,"index":854},"code":"  it('should compare complete texts', function () {\n    const text1 = require('./test_data/Wikipedia_EN_FrenchRevolution.json').text;\n    const text2 = require('./test_data/Wikipedia_EN_InfluenceOfTheFrenchRevolution.json').text;\n    expect(dice(text1, text2)).toBe(0.7939374395356337);\n  });","file":"dice_coefficient_spec.js","skipped":false,"dir":"spec"},{"name":"should drop initial silent consonants","suites":["double metaphone"],"line":27,"updatePoint":{"line":27,"column":43,"index":1243},"code":"  it('should drop initial silent consonants', function () {\n    doubleMetaphone.process('gnat', function (spy) {\n      expect(spy.initialSilentConsonantSkipped).toBeTruthy();\n    });\n  });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should consider initial vowels to be A","suites":["double metaphone","vowels"],"line":33,"updatePoint":{"line":33,"column":46,"index":1470},"code":"    it('should consider initial vowels to be A', function () {\n      let encodings = doubleMetaphone.process('England');\n      expect(encodings[0]).toMatch(/^A/);\n      expect(encodings[1]).toMatch(/^A/);\n      encodings = doubleMetaphone.process('astromech');\n      expect(encodings[0]).toMatch(/^A/);\n      expect(encodings[1]).toMatch(/^A/);\n      encodings = doubleMetaphone.process('être');\n      expect(encodings[0]).toMatch(/^A/);\n      expect(encodings[1]).toMatch(/^A/);\n      encodings = doubleMetaphone.process('éte');\n      expect(encodings[0]).toMatch(/^A/);\n      expect(encodings[1]).toMatch(/^A/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode B to P","suites":["double metaphone","B"],"line":49,"updatePoint":{"line":49,"column":28,"index":2110},"code":"    it('should encode B to P', function () {\n      const encodings = doubleMetaphone.process('berry');\n      expect(encodings[0]).toMatch(/^P/);\n      expect(encodings[1]).toMatch(/^P/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode BB to P","suites":["double metaphone","B"],"line":54,"updatePoint":{"line":54,"column":29,"index":2306},"code":"    it('should encode BB to P', function () {\n      const encodings = doubleMetaphone.process('tabby');\n      expect(encodings[0]).toContain('P');\n      expect(encodings[0]).not.toContain('PP');\n      expect(encodings[0]).not.toContain('PB');\n      expect(encodings[1]).toContain('P');\n      expect(encodings[1]).not.toContain('PP');\n      expect(encodings[1]).not.toContain('PB');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode MACHER's C to K","suites":["double metaphone","C"],"line":65,"updatePoint":{"line":65,"column":37,"index":2740},"code":"    it(\"should encode MACHER's C to K\", function () {\n      const encodings = doubleMetaphone.process('stomacher');\n      expect(encodings[0]).toContain('K');\n      expect(encodings[1]).toContain('K');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode CEASAR's C to S","suites":["double metaphone","C"],"line":70,"updatePoint":{"line":70,"column":37,"index":2950},"code":"    it(\"should encode CEASAR's C to S\", function () {\n      const encodings = doubleMetaphone.process('ceasar');\n      expect(encodings[0]).toMatch(/^S/);\n      expect(encodings[1]).toMatch(/S/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode chianti's C to K","suites":["double metaphone","C"],"line":75,"updatePoint":{"line":75,"column":38,"index":3155},"code":"    it(\"should encode chianti's C to K\", function () {\n      const encodings = doubleMetaphone.process('chianti');\n      expect(encodings[0]).toMatch(/^K/);\n      expect(encodings[1]).toMatch(/^K/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode CHAE to K,X","suites":["double metaphone","C"],"line":80,"updatePoint":{"line":80,"column":33,"index":3357},"code":"    it('should encode CHAE to K,X', function () {\n      const encodings = doubleMetaphone.process('archaeology');\n      expect(encodings[0]).toContain('K');\n      expect(encodings[1]).toContain('X');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode CHarac to K","suites":["double metaphone","C"],"line":85,"updatePoint":{"line":85,"column":33,"index":3565},"code":"    it('should encode CHarac to K', function () {\n      const encodings = doubleMetaphone.process('character');\n      expect(encodings[0]).toMatch(/^K/);\n      expect(encodings[1]).toMatch(/^K/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode C after von to K","suites":["double metaphone","C"],"line":90,"updatePoint":{"line":90,"column":38,"index":3774},"code":"    it('should encode C after von to K', function () {\n      const encodings = doubleMetaphone.process('von Chor');\n      expect(encodings[0]).toMatch(/^..K/);\n      expect(encodings[1]).toMatch(/^..K/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode some CH to K","suites":["double metaphone","C"],"line":95,"updatePoint":{"line":95,"column":34,"index":3982},"code":"    it('should encode some CH to K', function () {\n      const encodings = doubleMetaphone.process('orchestrational');\n      expect(encodings[0]).toMatch(/^..K/);\n      expect(encodings[1]).toMatch(/^..K/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode CH before some cons as K","suites":["double metaphone","C"],"line":100,"updatePoint":{"line":100,"column":46,"index":4209},"code":"    it('should encode CH before some cons as K', function () {\n      const encodings = doubleMetaphone.process('chthonic');\n      expect(encodings[0]).toMatch(/^K/);\n      expect(encodings[1]).toMatch(/^K/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode Irish mC to K","suites":["double metaphone","C"],"line":105,"updatePoint":{"line":105,"column":35,"index":4414},"code":"    it('should encode Irish mC to K', function () {\n      const encodings = doubleMetaphone.process('McHenry');\n      expect(encodings[0]).toMatch(/^.K/);\n      expect(encodings[1]).toMatch(/^.K/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode CH to X,K generally","suites":["double metaphone","C"],"line":110,"updatePoint":{"line":110,"column":41,"index":4626},"code":"    it('should encode CH to X,K generally', function () {\n      const encodings = doubleMetaphone.process('achievement');\n      expect(encodings[0]).toMatch(/^.X/);\n      expect(encodings[1]).toMatch(/^.K/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"shoud encode Polish WICZ to S,X","suites":["double metaphone","C"],"line":115,"updatePoint":{"line":115,"column":39,"index":4840},"code":"    it('shoud encode Polish WICZ to S,X', function () {\n      const encodings = doubleMetaphone.process('markiewicz');\n      expect(encodings[0]).toMatch(/S$/);\n      expect(encodings[1]).toMatch(/X$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode CIA to X","suites":["double metaphone","C"],"line":120,"updatePoint":{"line":120,"column":30,"index":5042},"code":"    it('should encode CIA to X', function () {\n      const encodings = doubleMetaphone.process('indicia');\n      expect(encodings[0]).toMatch(/X$/);\n      expect(encodings[1]).toMatch(/X$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode Italian CCI to X","suites":["double metaphone","C"],"line":125,"updatePoint":{"line":125,"column":38,"index":5249},"code":"    it('should encode Italian CCI to X', function () {\n      const encodings = doubleMetaphone.process('bacci');\n      expect(encodings[0]).toContain('X');\n      expect(encodings[1]).toContain('X');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode some CCes to K","suites":["double metaphone","C"],"line":130,"updatePoint":{"line":130,"column":36,"index":5454},"code":"    it('should encode some CCes to K', function () {\n      const encodings = doubleMetaphone.process('success');\n      expect(encodings[0]).toMatch(/^SKS/);\n      expect(encodings[1]).toMatch(/^SKS/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode some CC to K","suites":["double metaphone","C"],"line":135,"updatePoint":{"line":135,"column":34,"index":5661},"code":"    it('should encode some CC to K', function () {\n      const encodings = doubleMetaphone.process('yucca');\n      expect(encodings[0]).toMatch(/K$/);\n      expect(encodings[1]).toMatch(/K$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode CQ to K","suites":["double metaphone","C"],"line":140,"updatePoint":{"line":140,"column":29,"index":5857},"code":"    it('should encode CQ to K', function () {\n      const encodings = doubleMetaphone.process('racquetball');\n      expect(encodings[0]).toMatch(/^.K/);\n      expect(encodings[1]).toMatch(/^.K/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode CIO to S,X","suites":["double metaphone","C"],"line":145,"updatePoint":{"line":145,"column":32,"index":6064},"code":"    it('should encode CIO to S,X', function () {\n      const encodings = doubleMetaphone.process('sociopath');\n      expect(encodings[0]).toMatch(/^.S/);\n      expect(encodings[1]).toMatch(/^.X/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode CI to S","suites":["double metaphone","C"],"line":150,"updatePoint":{"line":150,"column":29,"index":6266},"code":"    it('should encode CI to S', function () {\n      const encodings = doubleMetaphone.process('city');\n      expect(encodings[0]).toMatch(/^S/);\n      expect(encodings[1]).toMatch(/^S/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode Scotch maC to K","suites":["double metaphone","C"],"line":155,"updatePoint":{"line":155,"column":37,"index":6469},"code":"    it('should encode Scotch maC to K', function () {\n      const encodings = doubleMetaphone.process('Mac Ghille dhuibh');\n      expect(encodings[0]).toMatch(/^MK/);\n      expect(encodings[1]).toMatch(/^MK/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode case Ç (French) to S","suites":["double metaphone","Ç"],"line":162,"updatePoint":{"line":162,"column":42,"index":6728},"code":"    it('should encode case Ç (French) to S', function () {\n      const encodings = doubleMetaphone.process('leçon');\n      expect(encodings[0]).toContain('S');\n      expect(encodings[1]).toContain('S');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode D to T","suites":["double metaphone","D"],"line":169,"updatePoint":{"line":169,"column":28,"index":6961},"code":"    it('should encode D to T', function () {\n      const encodings = doubleMetaphone.process('double');\n      expect(encodings[0]).toMatch(/^T/);\n      expect(encodings[1]).toMatch(/^T/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode DD to T","suites":["double metaphone","D"],"line":174,"updatePoint":{"line":174,"column":29,"index":7158},"code":"    it('should encode DD to T', function () {\n      const encodings = doubleMetaphone.process('fiddle');\n      expect(encodings[0]).toContain('T');\n      expect(encodings[0]).not.toContain('TT');\n      expect(encodings[0]).not.toContain('D');\n      expect(encodings[1]).toContain('T');\n      expect(encodings[1]).not.toContain('TT');\n      expect(encodings[1]).not.toContain('D');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode DG to J","suites":["double metaphone","D"],"line":183,"updatePoint":{"line":183,"column":29,"index":7547},"code":"    it('should encode DG to J', function () {\n      const encodings = doubleMetaphone.process('ledge');\n      expect(encodings[0]).toContain('J');\n      expect(encodings[0]).not.toContain('T');\n      expect(encodings[1]).toContain('J');\n      expect(encodings[1]).not.toContain('T');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode DT to T","suites":["double metaphone","D"],"line":190,"updatePoint":{"line":190,"column":29,"index":7839},"code":"    it('should encode DT to T', function () {\n      const encodings = doubleMetaphone.process('bundt');\n      expect(encodings[0]).toMatch(/[^D]T$/);\n      expect(encodings[1]).toMatch(/[^D]T$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode F","suites":["double metaphone","F"],"line":197,"updatePoint":{"line":197,"column":23,"index":8073},"code":"    it('should encode F', function () {\n      const encodings = doubleMetaphone.process('far');\n      expect(encodings[0]).toContain('F');\n      expect(encodings[1]).toContain('F');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode FF to F","suites":["double metaphone","F"],"line":202,"updatePoint":{"line":202,"column":29,"index":8269},"code":"    it('should encode FF to F', function () {\n      const encodings = doubleMetaphone.process('effect');\n      expect(encodings[0]).toContain('F');\n      expect(encodings[0]).not.toContain('FF');\n      expect(encodings[1]).toContain('F');\n      expect(encodings[1]).not.toContain('FF');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode G to F following U and 4 after certain cons","suites":["double metaphone","G"],"line":211,"updatePoint":{"line":211,"column":65,"index":8636},"code":"    it('should encode G to F following U and 4 after certain cons', function () {\n      const encodings = doubleMetaphone.process('tough');\n      expect(encodings[0]).toMatch('F$');\n      expect(encodings[1]).toMatch('F$');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode G to K","suites":["double metaphone","G"],"line":216,"updatePoint":{"line":216,"column":28,"index":8831},"code":"    it('should encode G to K', function () {\n      const encodings = doubleMetaphone.process('gift');\n      expect(encodings[0]).toMatch('^K');\n      expect(encodings[1]).toMatch('^K');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should ignore G a few letters after D, H, B","suites":["double metaphone","G"],"line":221,"updatePoint":{"line":221,"column":51,"index":9048},"code":"    it('should ignore G a few letters after D, H, B', function () {\n      const encodings = doubleMetaphone.process('fig');\n      expect(encodings[0]).toMatch('K$');\n      expect(encodings[1]).toMatch('K$');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should ignore G a few letters after D, H, B","suites":["double metaphone","G"],"line":226,"updatePoint":{"line":226,"column":51,"index":9264},"code":"    it('should ignore G a few letters after D, H, B', function () {\n      const encodings = doubleMetaphone.process('hugh');\n      expect(encodings[0]).toBe('H');\n      expect(encodings[1]).toBe('H');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode G to J when staring a word whose 3rd legger is I","suites":["double metaphone","G"],"line":231,"updatePoint":{"line":231,"column":70,"index":9492},"code":"    it('should encode G to J when staring a word whose 3rd legger is I', function () {\n      const encodings = doubleMetaphone.process('ghislaine');\n      expect(encodings[0]).toMatch(/^J/);\n      expect(encodings[1]).toMatch(/^J/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode G to K when staring words whose 3rd letter is not I","suites":["double metaphone","G"],"line":236,"updatePoint":{"line":236,"column":73,"index":9736},"code":"    it('should encode G to K when staring words whose 3rd letter is not I', function () {\n      const encodings = doubleMetaphone.process('consign');\n      expect(encodings[0]).toMatch(/N$/);\n      expect(encodings[1]).toMatch(/KN$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode GH to K if not succeeding a cons","suites":["double metaphone","G"],"line":241,"updatePoint":{"line":241,"column":54,"index":9960},"code":"    it('should encode GH to K if not succeeding a cons', function () {\n      const encodings = doubleMetaphone.process('Afghani');\n      expect(encodings[0]).toContain('K');\n      expect(encodings[1]).toContain('K');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode GN to N/KN generally","suites":["double metaphone","G"],"line":246,"updatePoint":{"line":246,"column":42,"index":10173},"code":"    it('should encode GN to N/KN generally', function () {\n      const encodings = doubleMetaphone.process('consign');\n      expect(encodings[0]).toMatch(/N$/);\n      expect(encodings[1]).toMatch(/KN$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode G to KN/N for the second letter following vowels","suites":["double metaphone","G"],"line":251,"updatePoint":{"line":251,"column":70,"index":10413},"code":"    it('should encode G to KN/N for the second letter following vowels', function () {\n      const encodings = doubleMetaphone.process('agnosia');\n      expect(encodings[0]).toMatch(/^.KN/);\n      expect(encodings[1]).toMatch(/^.N/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode G to K,J when starting some words","suites":["double metaphone","G"],"line":256,"updatePoint":{"line":256,"column":55,"index":10640},"code":"    it('should encode G to K,J when starting some words', function () {\n      const encodings = doubleMetaphone.process('germany');\n      expect(encodings[0]).toMatch(/^K/);\n      expect(encodings[1]).toMatch(/^J/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode GL to KL, L","suites":["double metaphone","G"],"line":261,"updatePoint":{"line":261,"column":33,"index":10842},"code":"    it('should encode GL to KL, L', function () {\n      const encodings = doubleMetaphone.process('taglianetti');\n      expect(encodings[0]).toMatch(/KL/);\n      expect(encodings[1]).toMatch(/[^K]L/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode GG to K","suites":["double metaphone","G"],"line":266,"updatePoint":{"line":266,"column":29,"index":11047},"code":"    it('should encode GG to K', function () {\n      const encodings = doubleMetaphone.process('daggers');\n      expect(encodings[0]).toContain('K');\n      expect(encodings[0]).not.toContain('KK');\n      expect(encodings[1]).toContain('K');\n      expect(encodings[1]).not.toContain('KK');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should keep initial Hs","suites":["double metaphone","H"],"line":275,"updatePoint":{"line":275,"column":30,"index":11380},"code":"    it('should keep initial Hs', function () {\n      const encodings = doubleMetaphone.process('hardly');\n      expect(encodings[0]).toMatch(/^H/);\n      expect(encodings[1]).toMatch(/^H/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should keep Hs between vowels","suites":["double metaphone","H"],"line":280,"updatePoint":{"line":280,"column":37,"index":11585},"code":"    it('should keep Hs between vowels', function () {\n      const encodings = doubleMetaphone.process('ahoi');\n      expect(encodings[0]).toContain('H');\n      expect(encodings[1]).toContain('H');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should drop Hs in words if not surrounded by vowels or starting","suites":["double metaphone","H"],"line":285,"updatePoint":{"line":285,"column":71,"index":11824},"code":"    it('should drop Hs in words if not surrounded by vowels or starting', function () {\n      const encodings = doubleMetaphone.process('charlie');\n      expect(encodings[0]).not.toContain('H');\n      expect(encodings[1]).not.toContain('H');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode spainish Js to Hs in the middle of words","suites":["double metaphone","J"],"line":292,"updatePoint":{"line":292,"column":62,"index":12101},"code":"    it('should encode spainish Js to Hs in the middle of words', function () {\n      const encodings = doubleMetaphone.process('bajador');\n      expect(encodings[0]).toMatch(/^.J/);\n      expect(encodings[1]).toMatch(/^.H/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode J to J,A","suites":["double metaphone","J"],"line":297,"updatePoint":{"line":297,"column":30,"index":12302},"code":"    it('should encode J to J,A', function () {\n      const encodings = doubleMetaphone.process('jumble');\n      expect(encodings[0]).toMatch(/^J/);\n      expect(encodings[1]).toMatch(/^A/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode J to J,\" \" at the end of words","suites":["double metaphone","J"],"line":302,"updatePoint":{"line":302,"column":52,"index":12522},"code":"    it('should encode J to J,\" \" at the end of words', function () {\n      const encodings = doubleMetaphone.process('hadj');\n      expect(encodings[0]).toMatch(/J$/);\n      expect(encodings[1]).toMatch(/\\s$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode JJ to J","suites":["double metaphone","J"],"line":307,"updatePoint":{"line":307,"column":29,"index":12718},"code":"    it('should encode JJ to J', function () {\n      const encodings = doubleMetaphone.process('hajj');\n      expect(encodings[0]).toMatch(/J$/);\n      expect(encodings[1]).toMatch(/J$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode J to H in saint names (cities)","suites":["double metaphone","J"],"line":312,"updatePoint":{"line":312,"column":52,"index":12936},"code":"    it('should encode J to H in saint names (cities)', function () {\n      const encodings = doubleMetaphone.process('san juan');\n      expect(encodings[0]).toContain('H');\n      expect(encodings[1]).toContain('H');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode L","suites":["double metaphone","L"],"line":319,"updatePoint":{"line":319,"column":23,"index":13167},"code":"    it('should encode L', function () {\n      const encodings = doubleMetaphone.process('last');\n      expect(encodings[0]).toMatch(/^L/);\n      expect(encodings[1]).toMatch(/^L/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode L to LL","suites":["double metaphone","L"],"line":324,"updatePoint":{"line":324,"column":29,"index":13362},"code":"    it('should encode L to LL', function () {\n      const encodings = doubleMetaphone.process('functionally');\n      expect(encodings[0]).toContain('L');\n      expect(encodings[0]).not.toContain('LL');\n      expect(encodings[1]).toContain('L');\n      expect(encodings[1]).not.toContain('LL');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode ignore spainish-style LL entirely in secondary","suites":["double metaphone","L"],"line":331,"updatePoint":{"line":331,"column":68,"index":13702},"code":"    it('should encode ignore spainish-style LL entirely in secondary', function () {\n      const encodings = doubleMetaphone.process('cabrillo');\n      expect(encodings[0]).toContain('L');\n      expect(encodings[1]).not.toContain('LL');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode M","suites":["double metaphone","M"],"line":338,"updatePoint":{"line":338,"column":23,"index":13938},"code":"    it('should encode M', function () {\n      const encodings = doubleMetaphone.process('meter');\n      expect(encodings[0]).toMatch(/^M/);\n      expect(encodings[1]).not.toContain(/^M/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should skip B after M","suites":["double metaphone","M"],"line":343,"updatePoint":{"line":343,"column":29,"index":14140},"code":"    it('should skip B after M', function () {\n      const encodings = doubleMetaphone.process('thumb');\n      expect(encodings[0]).toMatch(/M$/);\n      expect(encodings[1]).not.toContain(/M$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode Ns","suites":["double metaphone","N"],"line":350,"updatePoint":{"line":350,"column":24,"index":14373},"code":"    it('should encode Ns', function () {\n      const encodings = doubleMetaphone.process('natural');\n      expect(encodings[0]).toContain('N');\n      expect(encodings[1]).toContain('N');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode NN to N","suites":["double metaphone","N"],"line":355,"updatePoint":{"line":355,"column":29,"index":14573},"code":"    it('should encode NN to N', function () {\n      const encodings = doubleMetaphone.process('fanny');\n      expect(encodings[0]).toContain('N');\n      expect(encodings[1]).toContain('N');\n      expect(encodings[0]).not.toContain('NN');\n      expect(encodings[1]).not.toContain('NN');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should treat a spainish Ñ as a N","suites":["double metaphone","N"],"line":362,"updatePoint":{"line":362,"column":40,"index":14878},"code":"    it('should treat a spainish Ñ as a N', function () {\n      const encodings = doubleMetaphone.process('jalapeño');\n      expect(encodings[0]).toContain('N');\n      expect(encodings[1]).toContain('N');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode PH to F","suites":["double metaphone","P"],"line":369,"updatePoint":{"line":369,"column":29,"index":15115},"code":"    it('should encode PH to F', function () {\n      const encodings = doubleMetaphone.process('phone');\n      expect(encodings[0]).toMatch(/^F/);\n      expect(encodings[1]).toMatch(/^F/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode P","suites":["double metaphone","P"],"line":374,"updatePoint":{"line":374,"column":23,"index":15305},"code":"    it('should encode P', function () {\n      const encodings = doubleMetaphone.process('party');\n      expect(encodings[0]).toContain('P');\n      expect(encodings[1]).toContain('P');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode PP to P","suites":["double metaphone","P"],"line":379,"updatePoint":{"line":379,"column":29,"index":15503},"code":"    it('should encode PP to P', function () {\n      const encodings = doubleMetaphone.process('sappy');\n      expect(encodings[0]).toContain('P');\n      expect(encodings[0]).not.toContain('PP');\n      expect(encodings[1]).toContain('P');\n      expect(encodings[1]).not.toContain('PP');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should skip P before B i.e. raspberry","suites":["double metaphone","P"],"line":388,"updatePoint":{"line":388,"column":45,"index":15843},"code":"    it('should skip P before B i.e. raspberry', function () {\n      const encodings = doubleMetaphone.process('raspberry');\n      expect(encodings[0]).toContain('P');\n      expect(encodings[0]).not.toContain('PP');\n      expect(encodings[0]).not.toContain('PB');\n      expect(encodings[1]).toContain('P');\n      expect(encodings[1]).not.toContain('PP');\n      expect(encodings[1]).not.toContain('PB');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode Q to K","suites":["double metaphone","Q"],"line":401,"updatePoint":{"line":401,"column":28,"index":16300},"code":"    it('should encode Q to K', function () {\n      const encodings = doubleMetaphone.process('quarry');\n      expect(encodings[0]).toContain('K');\n      expect(encodings[1]).toContain('K');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode R","suites":["double metaphone","R"],"line":408,"updatePoint":{"line":408,"column":23,"index":16529},"code":"    it('should encode R', function () {\n      const encodings = doubleMetaphone.process('raspberry');\n      expect(encodings[0]).toMatch(/^R/);\n      expect(encodings[1]).toMatch(/^R/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should ignore trailing French Rs","suites":["double metaphone","R"],"line":413,"updatePoint":{"line":413,"column":40,"index":16740},"code":"    it('should ignore trailing French Rs', function () {\n      const encodings = doubleMetaphone.process('papier');\n      expect(encodings[0]).toMatch(/[^R]$/);\n      expect(encodings[1]).toMatch(/R$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should skip S between I and L","suites":["double metaphone","S"],"line":420,"updatePoint":{"line":420,"column":37,"index":16984},"code":"    it('should skip S between I and L', function () {\n      const encodings = doubleMetaphone.process('isle');\n      expect(encodings[0]).toMatch(/^AL/);\n      expect(encodings[1]).toMatch(/^AL/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode sugar's S to X","suites":["double metaphone","S"],"line":425,"updatePoint":{"line":425,"column":36,"index":17188},"code":"    it(\"should encode sugar's S to X\", function () {\n      const encodings = doubleMetaphone.process('sugar');\n      expect(encodings[0]).toMatch(/^X/);\n      expect(encodings[1]).toMatch(/^S/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode general SH to X","suites":["double metaphone","S"],"line":430,"updatePoint":{"line":430,"column":37,"index":17392},"code":"    it('should encode general SH to X', function () {\n      const encodings = doubleMetaphone.process('share');\n      expect(encodings[0]).toMatch(/^X/);\n      expect(encodings[1]).toMatch(/^X/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode certain germanic SHs to S","suites":["double metaphone","S"],"line":435,"updatePoint":{"line":435,"column":47,"index":17606},"code":"    it('should encode certain germanic SHs to S', function () {\n      const encodings = doubleMetaphone.process('Sholmer');\n      expect(encodings[0]).toMatch(/^S/);\n      expect(encodings[1]).toMatch(/^S/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode SION to S,X generally","suites":["double metaphone","S"],"line":440,"updatePoint":{"line":440,"column":43,"index":17818},"code":"    it('should encode SION to S,X generally', function () {\n      const encodings = doubleMetaphone.process('tension');\n      expect(encodings[0]).toContain('S');\n      expect(encodings[1]).toContain('X');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode SCHool to SK","suites":["double metaphone","S"],"line":445,"updatePoint":{"line":445,"column":34,"index":18023},"code":"    it('should encode SCHool to SK', function () {\n      const encodings = doubleMetaphone.process('school');\n      expect(encodings[0]).toMatch(/^SK/);\n      expect(encodings[1]).toMatch(/^SK/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode SCHER to X,SK","suites":["double metaphone","S"],"line":450,"updatePoint":{"line":450,"column":35,"index":18228},"code":"    it('should encode SCHER to X,SK', function () {\n      const encodings = doubleMetaphone.process('scherzando');\n      expect(encodings[0]).toMatch(/^X/);\n      expect(encodings[1]).toMatch(/^SK/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode SCHL to X,S","suites":["double metaphone","S"],"line":455,"updatePoint":{"line":455,"column":33,"index":18434},"code":"    it('should encode SCHL to X,S', function () {\n      const encodings = doubleMetaphone.process('schlump');\n      expect(encodings[0]).toMatch(/^X/);\n      expect(encodings[1]).toMatch(/^S/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode SC to SK generally","suites":["double metaphone","S"],"line":460,"updatePoint":{"line":460,"column":40,"index":18643},"code":"    it('should encode SC to SK generally', function () {\n      const encodings = doubleMetaphone.process('scumbag');\n      expect(encodings[0]).toMatch(/^SK/);\n      expect(encodings[1]).toMatch(/^SK/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode TION to XN","suites":["double metaphone","T"],"line":467,"updatePoint":{"line":467,"column":32,"index":18882},"code":"    it('should encode TION to XN', function () {\n      const encodings = doubleMetaphone.process('nation');\n      expect(encodings[0]).toMatch(/XN$/);\n      expect(encodings[1]).toMatch(/XN$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode CH sounds to X","suites":["double metaphone","T"],"line":472,"updatePoint":{"line":472,"column":36,"index":19088},"code":"    it('should encode CH sounds to X', function () {\n      const encodings = doubleMetaphone.process('thatch');\n      expect(encodings[0]).toMatch(/X$/);\n      expect(encodings[1]).toMatch(/X$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode hard TH to T","suites":["double metaphone","T"],"line":477,"updatePoint":{"line":477,"column":34,"index":19290},"code":"    it('should encode hard TH to T', function () {\n      const encodings = doubleMetaphone.process('thomas');\n      expect(encodings[0]).toMatch(/^T/);\n      expect(encodings[1]).toMatch(/^T/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode soft TH to 0,T","suites":["double metaphone","T"],"line":482,"updatePoint":{"line":482,"column":36,"index":19494},"code":"    it('should encode soft TH to 0,T', function () {\n      const encodings = doubleMetaphone.process('this');\n      expect(encodings[0]).toMatch(/^0/);\n      expect(encodings[1]).toMatch(/^T/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode TT to T","suites":["double metaphone","T"],"line":487,"updatePoint":{"line":487,"column":29,"index":19689},"code":"    it('should encode TT to T', function () {\n      const encodings = doubleMetaphone.process('matta');\n      expect(encodings[0]).toMatch(/[^T]T/);\n      expect(encodings[1]).toMatch(/[^T]T/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode TD to T","suites":["double metaphone","T"],"line":492,"updatePoint":{"line":492,"column":29,"index":19891},"code":"    it('should encode TD to T', function () {\n      const encodings = doubleMetaphone.process('countdown');\n      expect(encodings[0]).toContain('T');\n      expect(encodings[0]).not.toContain('D');\n      expect(encodings[1]).toContain('T');\n      expect(encodings[1]).not.toContain('D');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode V to F","suites":["double metaphone","V"],"line":501,"updatePoint":{"line":501,"column":28,"index":20222},"code":"    it('should encode V to F', function () {\n      const encodings = doubleMetaphone.process('very');\n      expect(encodings[0]).toContain('F');\n      expect(encodings[1]).toContain('F');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode VV to F","suites":["double metaphone","V"],"line":506,"updatePoint":{"line":506,"column":29,"index":20419},"code":"    it('should encode VV to F', function () {\n      const encodings = doubleMetaphone.process('savvy');\n      expect(encodings[0]).toContain('F');\n      expect(encodings[0]).not.toContain('FF');\n      expect(encodings[0]).not.toContain('FV');\n      expect(encodings[1]).toContain('F');\n      expect(encodings[1]).not.toContain('FF');\n      expect(encodings[1]).not.toContain('FV');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode WR to R","suites":["double metaphone","W"],"line":517,"updatePoint":{"line":517,"column":29,"index":20845},"code":"    it('should encode WR to R', function () {\n      const encodings = doubleMetaphone.process('wrong');\n      expect(encodings[0]).toMatch('^R');\n      expect(encodings[1]).toMatch('^R');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode WH to A at the start of a word","suites":["double metaphone","W"],"line":522,"updatePoint":{"line":522,"column":52,"index":21064},"code":"    it('should encode WH to A at the start of a word', function () {\n      const encodings = doubleMetaphone.process('wheat');\n      expect(encodings[0]).toMatch('^A');\n      expect(encodings[1]).toMatch('^A');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode WH to A,F if followed by a vowel at start","suites":["double metaphone","W"],"line":527,"updatePoint":{"line":527,"column":63,"index":21294},"code":"    it('should encode WH to A,F if followed by a vowel at start', function () {\n      const encodings = doubleMetaphone.process('wolfgang');\n      expect(encodings[0]).toMatch('^A');\n      expect(encodings[1]).toMatch('^F');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode OWSKY alternately to F(V)","suites":["double metaphone","W"],"line":532,"updatePoint":{"line":532,"column":47,"index":21511},"code":"    it('should encode OWSKY alternately to F(V)', function () {\n      const encodings = doubleMetaphone.process('lebowski');\n      expect(encodings[0]).not.toContain('F');\n      expect(encodings[1]).toContain('F');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode WICZ","suites":["double metaphone","W"],"line":537,"updatePoint":{"line":537,"column":26,"index":21713},"code":"    it('should encode WICZ', function () {\n      const encodings = doubleMetaphone.process('Lowicz');\n      expect(encodings[0]).toMatch('TS$');\n      expect(encodings[1]).toMatch('FX$');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode X as S at start","suites":["double metaphone","X"],"line":544,"updatePoint":{"line":544,"column":37,"index":21956},"code":"    it('should encode X as S at start', function () {\n      const encodings = doubleMetaphone.process('xenophobia');\n      expect(encodings[0]).toMatch(/^S/);\n      expect(encodings[1]).toMatch(/^S/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode X as KS at end for non-French words","suites":["double metaphone","X"],"line":549,"updatePoint":{"line":549,"column":57,"index":22185},"code":"    it('should encode X as KS at end for non-French words', function () {\n      const encodings = doubleMetaphone.process('box');\n      expect(encodings[0]).toMatch(/KS$/);\n      expect(encodings[1]).toMatch(/KS$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should skip X end for French words","suites":["double metaphone","X"],"line":554,"updatePoint":{"line":554,"column":42,"index":22394},"code":"    it('should skip X end for French words', function () {\n      const encodings = doubleMetaphone.process('lemieux');\n      expect(encodings[0]).not.toMatch(/KS$/);\n      expect(encodings[1]).not.toMatch(/KS$/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode Z to S","suites":["double metaphone","Z"],"line":561,"updatePoint":{"line":561,"column":28,"index":22637},"code":"    it('should encode Z to S', function () {\n      const encodings = doubleMetaphone.process('zookeeper');\n      expect(encodings[0]).toMatch(/^S/);\n      expect(encodings[1]).toMatch(/^S/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode Chinese ZH to J","suites":["double metaphone","Z"],"line":566,"updatePoint":{"line":566,"column":37,"index":22845},"code":"    it('should encode Chinese ZH to J', function () {\n      const encodings = doubleMetaphone.process('zheng');\n      expect(encodings[0]).toMatch(/^J/);\n      expect(encodings[1]).toMatch(/^J/);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode ZZA to S, TS","suites":["double metaphone","Z"],"line":571,"updatePoint":{"line":571,"column":34,"index":23046},"code":"    it('should encode ZZA to S, TS', function () {\n      const encodings = doubleMetaphone.process('pizza');\n      expect(encodings[0]).toContain('S');\n      expect(encodings[1]).toContain('TS');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should detect vowels","suites":["double metaphone","general"],"line":578,"updatePoint":{"line":578,"column":28,"index":23286},"code":"    it('should detect vowels', function () {\n      expect(doubleMetaphone.isVowel('a')).toBeTruthy();\n      expect(doubleMetaphone.isVowel('e')).toBeTruthy();\n      expect(doubleMetaphone.isVowel('b')).toBeFalsy();\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should detect Slavo-Germanic text","suites":["double metaphone","general"],"line":583,"updatePoint":{"line":583,"column":41,"index":23522},"code":"    it('should detect Slavo-Germanic text', function () {\n      doubleMetaphone.process('horowitz', function (spy) {\n        expect(spy.slavoGermanic).toBeTruthy();\n      });\n      doubleMetaphone.process('gumball', function (spy) {\n        expect(spy.slavoGermanic).toBeFalsy();\n      });\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should encode general words","suites":["double metaphone","general"],"line":591,"updatePoint":{"line":591,"column":35,"index":23814},"code":"    it('should encode general words', function () {\n      let encodings = doubleMetaphone.process('complete');\n      expect(encodings[0]).toMatch(/KMPLT/);\n      expect(encodings[1]).toMatch(/KMPLT/);\n      encodings = doubleMetaphone.process('Matrix');\n      expect(encodings[0]).toMatch(/MTRKS/);\n      expect(encodings[1]).toMatch(/MTRKS/);\n      encodings = doubleMetaphone.process('appropriate');\n      expect(encodings[0]).toMatch(/APRPRT/);\n      expect(encodings[1]).toMatch(/APRPRT/);\n      encodings = doubleMetaphone.process('intervention');\n      expect(encodings[0]).toBe('ANTRFNXN');\n      expect(encodings[1]).toBe('ANTRFNXN');\n      encodings = doubleMetaphone.process('Français');\n      expect(encodings[0]).toBe('FRNS');\n      expect(encodings[1]).toBe('FRNSS');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should truncate codes if specified","suites":["double metaphone","general"],"line":608,"updatePoint":{"line":608,"column":42,"index":24610},"code":"    it('should truncate codes if specified', function () {\n      let encodings = doubleMetaphone.process('Matrix', 4);\n      expect(encodings[0]).toBe('MTRK');\n      expect(encodings[1]).toBe('MTRK');\n      encodings = doubleMetaphone.process('Français', 4);\n      expect(encodings[0]).toBe('FRNS');\n      expect(encodings[1]).toBe('FRNS');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should not truncate code is shorter than specification","suites":["double metaphone","general"],"line":616,"updatePoint":{"line":616,"column":62,"index":24979},"code":"    it('should not truncate code is shorter than specification', function () {\n      let encodings = doubleMetaphone.process('Matrix', 32);\n      expect(encodings[0]).toBe('MTRKS');\n      expect(encodings[1]).toBe('MTRKS');\n      encodings = doubleMetaphone.process('Français', 5);\n      expect(encodings[0]).toBe('FRNS');\n      expect(encodings[1]).toBe('FRNSS');\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should compare","suites":["double metaphone","general"],"line":624,"updatePoint":{"line":624,"column":22,"index":25312},"code":"    it('should compare', function () {\n      expect(doubleMetaphone.compare('love', 'hate')).toBeFalsy();\n      expect(doubleMetaphone.compare('love', 'luv')).toBeTruthy();\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"terminate words ending in H","suites":["double metaphone","issue #34"],"line":630,"updatePoint":{"line":630,"column":35,"index":25550},"code":"    it('terminate words ending in H', function () {\n      expect(doubleMetaphone.process('ptah')).toEqual(['PT', 'PT']);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"handle initial cons correctly","suites":["double metaphone","issue #173"],"line":635,"updatePoint":{"line":635,"column":37,"index":25726},"code":"    it('handle initial cons correctly', function () {\n      expect(doubleMetaphone.process('ceasar')).toEqual(['SSR', 'SSR']);\n      expect(doubleMetaphone.process('ach')).toEqual(['AK', 'AK']);\n      expect(doubleMetaphone.process('chemical')).toEqual(['KMKL', 'KMKL']);\n      expect(doubleMetaphone.process('choral')).toEqual(['KRL', 'KRL']);\n    });","file":"double_metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should calculate the difference between two strings correctly","suites":["The Hamming distance function compares strings of equal length"],"line":29,"updatePoint":{"line":29,"column":69,"index":1642},"code":"    it('should calculate the difference between two strings correctly', function () {\n      expect(hammingDistance(values[0], values[1], values[2])).toEqual(values[3]);\n    });","file":"hamming_distance_spec.js","skipped":false,"dir":"spec"},{"name":"should evaluate string similarity","suites":["jaro-winkler"],"line":38,"updatePoint":{"line":38,"column":39,"index":1479},"code":"  it('should evaluate string similarity', function () {\n    expect(approxEql(jaroWinklerDistance('DIXON', 'DICKSONX'), 0.81333)).toBeTruthy();\n    expect(approxEql(jaroWinklerDistance('DWAYNE', 'DUANE'), 0.84)).toBeTruthy();\n  });","file":"jaro-winkler_spec.js","skipped":false,"dir":"spec"},{"name":"should handle exact matches","suites":["jaro-winkler"],"line":42,"updatePoint":{"line":42,"column":33,"index":1704},"code":"  it('should handle exact matches', function () {\n    expect(jaroWinklerDistance('RICK', 'RICK')).toBe(1);\n    expect(jaroWinklerDistance('abc', 'abc')).toBe(1);\n    expect(jaroWinklerDistance('abcd', 'abcd')).toBe(1);\n    expect(jaroWinklerDistance('seddon', 'seddon')).toBe(1);\n  });","file":"jaro-winkler_spec.js","skipped":false,"dir":"spec"},{"name":"should handle total mis-matches","suites":["jaro-winkler"],"line":48,"updatePoint":{"line":48,"column":37,"index":1994},"code":"  it('should handle total mis-matches', function () {\n    expect(jaroWinklerDistance('NOT', 'SAME')).toBe(0);\n  });","file":"jaro-winkler_spec.js","skipped":false,"dir":"spec"},{"name":"should handle partial mis-matches","suites":["jaro-winkler"],"line":51,"updatePoint":{"line":51,"column":39,"index":2112},"code":"  it('should handle partial mis-matches', function () {\n    expect(approxEql(jaroWinklerDistance('aaa', 'abcd'), 0.575)).toBeTruthy();\n  });","file":"jaro-winkler_spec.js","skipped":false,"dir":"spec"},{"name":"should handle transpositions","suites":["jaro-winkler"],"line":54,"updatePoint":{"line":54,"column":34,"index":2248},"code":"  it('should handle transpositions', function () {\n    expect(approxEql(jaroWinklerDistance('MARTHA', 'MARHTA'), 0.96111)).toBeTruthy();\n  });","file":"jaro-winkler_spec.js","skipped":false,"dir":"spec"},{"name":"should handle transpositions regardless of string order","suites":["jaro-winkler"],"line":57,"updatePoint":{"line":57,"column":61,"index":2418},"code":"  it('should handle transpositions regardless of string order', function () {\n    expect(approxEql(jaroWinklerDistance('class', 'clams'), 0.90666)).toBeTruthy();\n    expect(approxEql(jaroWinklerDistance('clams', 'class'), 0.90666)).toBeTruthy();\n  });","file":"jaro-winkler_spec.js","skipped":false,"dir":"spec"},{"name":"should ignore case when asked to","suites":["jaro-winkler"],"line":61,"updatePoint":{"line":61,"column":38,"index":2647},"code":"  it('should ignore case when asked to', function () {\n    expect(jaroWinklerDistance('aaa', 'aAa', undefined, true)).toEqual(jaroWinklerDistance('aaa', 'aaa'));\n    expect(jaroWinklerDistance('aaa', 'aAa')).not.toEqual(jaroWinklerDistance('aaa', 'aaa'));\n    expect(jaroWinklerDistance('dixon', 'DICKSONX', undefined, true)).toEqual(jaroWinklerDistance('DIXON', 'DICKSONX'));\n    expect(jaroWinklerDistance('seddon', 'SEDDON', undefined, true)).toEqual(jaroWinklerDistance('SEDDON', 'SEDDON'));\n    expect(approxEql(jaroWinklerDistance('MARTHA', 'MARHTA', undefined, true), 0.96111)).toBeTruthy();\n    expect(jaroWinklerDistance('abcd', 'ABCD', undefined, true)).toBe(1);\n  });","file":"jaro-winkler_spec.js","skipped":false,"dir":"spec"},{"name":"should stem","suites":["lancaster_stemmer"],"line":27,"updatePoint":{"line":27,"column":17,"index":1212},"code":"  it('should stem', function () {\n    // stemmer.attach();\n    expect(stemmer.stem('marks')).toBe('mark');\n    expect(stemmer.stem('MARKs')).toBe('mark');\n  });","file":"lancaster_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should stop running rules where appropriate","suites":["lancaster_stemmer"],"line":32,"updatePoint":{"line":32,"column":49,"index":1405},"code":"  it('should stop running rules where appropriate', function () {\n    // stemmer.attach();\n    expect(stemmer.stem('living')).toBe('liv');\n    expect(stemmer.stem('thing')).toBe('thing');\n    expect(stemmer.stem('ear')).toBe('ear');\n    expect(stemmer.stem('string')).toBe('string');\n  });","file":"lancaster_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should only pop the size specified by the rule","suites":["lancaster_stemmer"],"line":39,"updatePoint":{"line":39,"column":52,"index":1698},"code":"  it('should only pop the size specified by the rule', function () {\n    // stemmer.attach();\n    expect(stemmer.stem('triplicate')).toBe('triply');\n    expect(stemmer.stem('triPlicAte')).toBe('triply');\n  });","file":"lancaster_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should stem and append and recurse","suites":["lancaster_stemmer"],"line":44,"updatePoint":{"line":44,"column":40,"index":1896},"code":"  it('should stem and append and recurse', function () {\n    // stemmer.attach();\n    expect(stemmer.stem('classified')).toBe('class');\n    expect(stemmer.stem('ClaSsiFied')).toBe('class');\n  });","file":"lancaster_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should apply intact rules only to intact string","suites":["lancaster_stemmer"],"line":49,"updatePoint":{"line":49,"column":53,"index":2105},"code":"  it('should apply intact rules only to intact string', function () {\n    // stemmer.attach();\n    expect(stemmer.stem('maximum')).toBe('maxim');\n    expect(stemmer.stem('presumably')).toBe('presum');\n    expect(stemmer.stem('MAXIMUM')).toBe('maxim');\n    expect(stemmer.stem('PRESUMABLY')).toBe('presum');\n  });","file":"lancaster_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"#171 and #174, exceed, anguish, affluxion, discept","suites":["lancaster_stemmer"],"line":56,"updatePoint":{"line":56,"column":56,"index":2421},"code":"  it('#171 and #174, exceed, anguish, affluxion, discept', function () {\n    // stemmer.attach();\n    expect(stemmer.stem('exceed')).toBe('excess');\n    expect(stemmer.stem('anguish')).toBe('anct');\n    expect(stemmer.stem('affluxion')).toBe('affluct');\n    expect(stemmer.stem('discept')).toBe('disceiv');\n  });","file":"lancaster_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should find cheapest substring","suites":["levenshtein_distance","options.search = true"],"line":29,"updatePoint":{"line":29,"column":38,"index":1436},"code":"    it('should find cheapest substring', function () {\n      expect(levenshteinDistanceSearch('kitten', 'sitting')).toEqual({\n        substring: 'sittin',\n        distance: 2,\n        offset: 0\n      });\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should find 0 cost substring in target","suites":["levenshtein_distance","options.search = true"],"line":36,"updatePoint":{"line":36,"column":46,"index":1656},"code":"    it('should find 0 cost substring in target', function () {\n      expect(levenshteinDistanceSearch('doctor', 'the doctor is in')).toEqual({\n        substring: 'doctor',\n        distance: 0,\n        offset: 4\n      });\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should find 1 cost substring in target","suites":["levenshtein_distance","options.search = true"],"line":43,"updatePoint":{"line":43,"column":46,"index":1885},"code":"    it('should find 1 cost substring in target', function () {\n      expect(levenshteinDistanceSearch('doctor', 'the doktor is in')).toEqual({\n        substring: 'doktor',\n        distance: 1,\n        offset: 4\n      });\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should return empty substring when that is cleapest match","suites":["levenshtein_distance","options.search = true"],"line":50,"updatePoint":{"line":50,"column":65,"index":2133},"code":"    it('should return empty substring when that is cleapest match', function () {\n      expect(levenshteinDistanceSearch('doctor', '000000000000')).toEqual({\n        substring: '',\n        distance: 6,\n        offset: 0\n      });\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"different insertion costs should work","suites":["levenshtein_distance","options.search = true"],"line":57,"updatePoint":{"line":57,"column":45,"index":2351},"code":"    it('different insertion costs should work', function () {\n      // delete 10 0's at cost 1 and insert the letters for doctor at cost -1\n      expect(levenshteinDistanceSearch('0000000000', 'doctor', {\n        insertion_cost: -1\n      })).toEqual({\n        substring: 'doctor',\n        distance: 4,\n        offset: 0\n      });\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"different deletion costs should work","suites":["levenshtein_distance","options.search = true"],"line":67,"updatePoint":{"line":67,"column":44,"index":2688},"code":"    it('different deletion costs should work', function () {\n      // delete 10 0's at cost -10\n      expect(levenshteinDistanceSearch('0000000000', 'doctor', {\n        deletion_cost: -1\n      })).toEqual({\n        substring: '',\n        distance: -10,\n        offset: 0\n      });\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should replace 2","suites":["levenshtein_distance","default / options.search = false"],"line":79,"updatePoint":{"line":79,"column":24,"index":3024},"code":"    it('should replace 2', function () {\n      expect(levenshteinDistance('doctor', 'doktor')).toBe(1);\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should allow altering replacement value","suites":["levenshtein_distance","default / options.search = false"],"line":82,"updatePoint":{"line":82,"column":47,"index":3159},"code":"    it('should allow altering replacement value', function () {\n      expect(levenshteinDistance('doctor', 'doktor', {\n        substitution_cost: 1\n      })).toBe(1);\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should delete 1","suites":["levenshtein_distance","default / options.search = false"],"line":87,"updatePoint":{"line":87,"column":23,"index":3310},"code":"    it('should delete 1', function () {\n      expect(levenshteinDistance('doctor', 'docto')).toBe(1);\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should insert 1","suites":["levenshtein_distance","default / options.search = false"],"line":90,"updatePoint":{"line":90,"column":23,"index":3420},"code":"    it('should insert 1', function () {\n      expect(levenshteinDistance('flat', 'flats')).toBe(1);\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should combine operations","suites":["levenshtein_distance","default / options.search = false"],"line":93,"updatePoint":{"line":93,"column":33,"index":3538},"code":"    it('should combine operations', function () {\n      expect(levenshteinDistance('flad', 'flaten')).toBe(3);\n      expect(levenshteinDistance('flaten', 'flad')).toBe(3);\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should consider perfect matches 0","suites":["levenshtein_distance","default / options.search = false"],"line":97,"updatePoint":{"line":97,"column":41,"index":3726},"code":"    it('should consider perfect matches 0', function () {\n      expect(levenshteinDistance('one', 'one')).toBe(0);\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"different deletion cost should work","suites":["levenshtein_distance","default / options.search = false"],"line":100,"updatePoint":{"line":100,"column":43,"index":3851},"code":"    it('different deletion cost should work', function () {\n      expect(levenshteinDistance('ones', 'one', {\n        deletion_cost: 3\n      })).toBe(3);\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"different insertion cost should work","suites":["levenshtein_distance","default / options.search = false"],"line":105,"updatePoint":{"line":105,"column":44,"index":4014},"code":"    it('different insertion cost should work', function () {\n      expect(levenshteinDistance('one', 'ones', {\n        deletion_cost: 3,\n        insertion_cost: 5\n      })).toBe(5);\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"delete all characters with -ve cost","suites":["levenshtein_distance","default / options.search = false"],"line":111,"updatePoint":{"line":111,"column":43,"index":4203},"code":"    it('delete all characters with -ve cost', function () {\n      expect(levenshteinDistance('delete', '', {\n        deletion_cost: -1\n      })).toBe(-6);\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"insert all characters","suites":["levenshtein_distance","default / options.search = false"],"line":116,"updatePoint":{"line":116,"column":29,"index":4352},"code":"    it('insert all characters', function () {\n      expect(levenshteinDistance('', 'insert')).toBe(6);\n    });","file":"levenshtein_spec.js","skipped":false,"dir":"spec"},{"name":"should classify with individually trained documents","suites":["logistic regression"],"line":28,"updatePoint":{"line":28,"column":57,"index":1300},"code":"  it('should classify with individually trained documents', function () {\n    const classifier = new LogisticRegressionClassifier();\n    classifier.addDocument(['have', 'computer'], 'IT');\n    classifier.addDocument(['have', 'phone'], 'IT');\n    classifier.addDocument(['computer', 'suck'], 'IT');\n    classifier.addDocument(['field', 'goal'], 'sports');\n    classifier.addDocument(['score', 'goal'], 'sports');\n    classifier.addDocument(['great', 'speed'], 'sports');\n    classifier.train();\n    expect(classifier.classify(['hate', 'computer'])).toBe('IT');\n    expect(classifier.classify(['score', 'please'])).toBe('sports');\n  });","file":"logistic_regression_classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should provide all classification scores","suites":["logistic regression"],"line":40,"updatePoint":{"line":40,"column":46,"index":1924},"code":"  it('should provide all classification scores', function () {\n    const classifier = new natural.LogisticRegressionClassifier();\n    classifier.addDocument(['fix', 'box'], 'computing');\n    classifier.addDocument(['write', 'code'], 'computing');\n    classifier.addDocument(['script', 'code'], 'computing');\n    classifier.addDocument(['write', 'book'], 'literature');\n    classifier.addDocument(['read', 'book'], 'literature');\n    classifier.addDocument(['study', 'book'], 'literature');\n    classifier.train();\n    expect(classifier.getClassifications('i write code')[0].label).toBe('computing');\n    expect(classifier.getClassifications('i write code')[1].label).toBe('literature');\n  });","file":"logistic_regression_classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should classify with arrays","suites":["logistic regression"],"line":62,"updatePoint":{"line":62,"column":33,"index":3085},"code":"  it('should classify with arrays', function () {\n    const classifier = createClassifier();\n    classifier.train();\n    expect(classifier.classify('a bug in the code')).toBe('computing');\n    expect(classifier.classify('read all the books')).toBe('literature');\n  });","file":"logistic_regression_classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should serialize and deserialize a working classifier","suites":["logistic regression"],"line":68,"updatePoint":{"line":68,"column":59,"index":3380},"code":"  it('should serialize and deserialize a working classifier', function () {\n    const classifier = createClassifier();\n    const obj = JSON.stringify(classifier);\n    const newClassifier = natural.LogisticRegressionClassifier.restore(JSON.parse(obj));\n    newClassifier.addDocument('kick a ball', 'sports');\n    newClassifier.addDocument('hit some balls', 'sports');\n    newClassifier.addDocument('kick and punch', 'sports');\n    newClassifier.train();\n    expect(newClassifier.classify('a bug in the code')).toBe('computing');\n    expect(newClassifier.classify('read all the books')).toBe('literature');\n    expect(newClassifier.classify('kick butt')).toBe('sports');\n  });","file":"logistic_regression_classifier_spec.js","skipped":false,"dir":"spec"},{"name":"should get the numbers of vertexs","suites":["shortest path tree","edge weighted digraph normal operations"],"line":44,"updatePoint":{"line":44,"column":41,"index":1836},"code":"    it('should get the numbers of vertexs', function () {\n      expect(digraph.v()).toBe(8);\n    });","file":"longest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should get the numbers of edges","suites":["shortest path tree","edge weighted digraph normal operations"],"line":47,"updatePoint":{"line":47,"column":39,"index":1935},"code":"    it('should get the numbers of edges', function () {\n      expect(digraph.e()).toBe(13);\n    });","file":"longest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should print all item in digraph","suites":["shortest path tree","edge weighted digraph normal operations"],"line":50,"updatePoint":{"line":50,"column":40,"index":2036},"code":"    it('should print all item in digraph', function () {\n      expect(digraph.toString()).toBe('0 -> 2, 0.26\\n' + '1 -> 3, 0.29\\n' + '3 -> 7, 0.39\\n' + '3 -> 6, 0.52\\n' + '4 -> 7, 0.37\\n' + '4 -> 0, 0.38\\n' + '5 -> 4, 0.35\\n' + '5 -> 7, 0.28\\n' + '5 -> 1, 0.32\\n' + '6 -> 2, 0.4\\n' + '6 -> 0, 0.58\\n' + '6 -> 4, 0.93\\n' + '7 -> 2, 0.34');\n    });","file":"longest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should sort all the vertexs","suites":["shortest path tree","topo sort for digraph"],"line":55,"updatePoint":{"line":55,"column":35,"index":2434},"code":"    it('should sort all the vertexs', function () {\n      const topoSort = new Topological(digraph);\n      expect(topoSort.isDAG()).toBe(true);\n      expect(topoSort.order()).toEqual([5, 1, 3, 6, 4, 7, 0, 2]);\n    });","file":"longest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should determine existence of paths","suites":["shortest path tree","shortest path tree normal operations"],"line":63,"updatePoint":{"line":63,"column":43,"index":2768},"code":"    it('should determine existence of paths', function () {\n      expect(lpt.hasPathTo(0)).toBe(true);\n      expect(lpt.hasPathTo(1)).toBe(true);\n      expect(lpt.hasPathTo(2)).toBe(true);\n      expect(lpt.hasPathTo(3)).toBe(true);\n      expect(lpt.hasPathTo(4)).toBe(true);\n      expect(lpt.hasPathTo(5)).toBe(false);\n      expect(lpt.hasPathTo(6)).toBe(true);\n      expect(lpt.hasPathTo(7)).toBe(true);\n    });","file":"longest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should determine paths","suites":["shortest path tree","shortest path tree normal operations"],"line":73,"updatePoint":{"line":73,"column":30,"index":3168},"code":"    it('should determine paths', function () {\n      expect(lpt.pathTo(0)).toEqual([5, 1, 3, 6, 4, 0]);\n      expect(lpt.pathTo(1)).toEqual([5, 1]);\n      expect(lpt.pathTo(2)).toEqual([5, 1, 3, 6, 4, 7, 2]);\n      expect(lpt.pathTo(3)).toEqual([5, 1, 3]);\n      expect(lpt.pathTo(4)).toEqual([5, 1, 3, 6, 4]);\n      expect(lpt.pathTo(5)).toEqual([]);\n      expect(lpt.pathTo(6)).toEqual([5, 1, 3, 6]);\n      expect(lpt.pathTo(7)).toEqual([5, 1, 3, 6, 4, 7]);\n    });","file":"longest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should calculate distances","suites":["shortest path tree","shortest path tree normal operations"],"line":83,"updatePoint":{"line":83,"column":34,"index":3640},"code":"    it('should calculate distances', function () {\n      expect(lpt.getDistTo(0)).toBe(2.44);\n      expect(lpt.getDistTo(1)).toBe(0.32);\n      expect(lpt.getDistTo(2)).toBe(2.77);\n      expect(lpt.getDistTo(3)).toBe(0.61);\n      expect(lpt.getDistTo(4)).toBe(2.06);\n      expect(lpt.getDistTo(5)).toBe(0);\n      expect(lpt.getDistTo(6)).toBe(1.13);\n      expect(lpt.getDistTo(7)).toBe(2.43);\n    });","file":"longest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"generates a sample from a corpus","suites":["Maximum Entropy Classifier applied to POS tagging"],"line":145,"updatePoint":{"line":145,"column":38,"index":5777},"code":"  it('generates a sample from a corpus', function () {\n    sample = trainCorpus.generateSample();\n    expect(sample.size()).toBeGreaterThan(0);\n    DEBUG && console.log('Size of the sample: ' + sample.size());\n  });","file":"MaxEntAppliedToPOSTagging_spec.js","skipped":false,"dir":"spec"},{"name":"generates a set of features from the sample","suites":["Maximum Entropy Classifier applied to POS tagging"],"line":150,"updatePoint":{"line":150,"column":49,"index":6004},"code":"  it('generates a set of features from the sample', function () {\n    featureSet = new FeatureSet();\n    DEBUG && console.log(sample);\n    sample.generateFeatures(featureSet);\n    expect(featureSet.size()).toBeGreaterThan(0);\n    DEBUG && console.log('Number of features: ' + featureSet.size());\n    DEBUG && console.log(featureSet.prettyPrint());\n  });","file":"MaxEntAppliedToPOSTagging_spec.js","skipped":false,"dir":"spec"},{"name":"analyses the sample","suites":["Maximum Entropy Classifier applied to POS tagging"],"line":158,"updatePoint":{"line":158,"column":25,"index":6334},"code":"  it('analyses the sample', function () {\n    trainCorpus.analyse();\n    lexicon = trainCorpus.buildLexicon();\n    expect(lexicon.size()).toBeGreaterThan(0);\n  });","file":"MaxEntAppliedToPOSTagging_spec.js","skipped":false,"dir":"spec"},{"name":"trains the maximum entropy classifier","suites":["Maximum Entropy Classifier applied to POS tagging"],"line":163,"updatePoint":{"line":163,"column":43,"index":6516},"code":"  it('trains the maximum entropy classifier', function () {\n    classifier = new Classifier(featureSet, sample);\n    DEBUG && console.log('Classifier created');\n    classifier.train(nrIterations, minImprovement);\n    DEBUG && console.log('Checksum: ' + classifier.p.checkSum());\n  });","file":"MaxEntAppliedToPOSTagging_spec.js","skipped":false,"dir":"spec"},{"name":"compares maximum entropy based POS tagger to lexicon-based tagger","suites":["Maximum Entropy Classifier applied to POS tagging"],"line":169,"updatePoint":{"line":169,"column":71,"index":6829},"code":"  it('compares maximum entropy based POS tagger to lexicon-based tagger', function () {\n    // Test the classifier against the test corpus\n    // lexicon.setDefaultCategories('NN', 'NP');\n    tagger = new Tagger(lexicon);\n    applyClassifierToTestCorpus(testCorpus, tagger, classifier);\n  });","file":"MaxEntAppliedToPOSTagging_spec.js","skipped":false,"dir":"spec"},{"name":"The Sample class creates a sample","suites":["The MaxEnt module"],"line":40,"updatePoint":{"line":40,"column":39,"index":1607},"code":"  it('The Sample class creates a sample', function () {\n    sample = new Sample();\n    sample.addElement(new SEElement('x', new Context('0')));\n    sample.addElement(new SEElement('x', new Context('0')));\n    sample.addElement(new SEElement('x', new Context('0')));\n    sample.addElement(new SEElement('y', new Context('0')));\n    sample.addElement(new SEElement('y', new Context('0')));\n    sample.addElement(new SEElement('y', new Context('0')));\n    sample.addElement(new SEElement('x', new Context('1')));\n    sample.addElement(new SEElement('y', new Context('1')));\n    sample.addElement(new SEElement('y', new Context('1')));\n    sample.addElement(new SEElement('y', new Context('1')));\n    expect(sample.size()).toBe(10);\n  });","file":"MaxEntClassifier_spec.js","skipped":false,"dir":"spec"},{"name":"The FeatureSet class creates a feature set","suites":["The MaxEnt module"],"line":54,"updatePoint":{"line":54,"column":48,"index":2351},"code":"  it('The FeatureSet class creates a feature set', function () {\n    featureSet = new FeatureSet();\n    sample.generateFeatures(featureSet);\n    expect(featureSet.size()).toBe(2);\n  });","file":"MaxEntClassifier_spec.js","skipped":false,"dir":"spec"},{"name":"The Classifier class creates a classifier","suites":["The MaxEnt module"],"line":59,"updatePoint":{"line":59,"column":47,"index":2536},"code":"  it('The Classifier class creates a classifier', function () {\n    // Create a classifier\n    classifier = new Classifier(featureSet, sample);\n    expect(classifier).not.toBe(undefined);\n  });","file":"MaxEntClassifier_spec.js","skipped":false,"dir":"spec"},{"name":"Classifier does not need a correction feature","suites":["The MaxEnt module"],"line":64,"updatePoint":{"line":64,"column":51,"index":2734},"code":"  it('Classifier does not need a correction feature', function () {});","file":"MaxEntClassifier_spec.js","skipped":false,"dir":"spec"},{"name":"Save classifer to a file","suites":["The MaxEnt module"],"line":72,"updatePoint":{"line":72,"column":30,"index":3201},"code":"  it('Save classifer to a file', function (done) {\n    classifier.save(classifierFilename, function (err, c) {\n      if (err) {\n        console.log(err);\n      } else {\n        DEBUG && console.log('Classifier saved to ' + classifierFilename);\n      }\n      done();\n    });\n  });","file":"MaxEntClassifier_spec.js","skipped":false,"dir":"spec"},{"name":"Load classifer","suites":["The MaxEnt module"],"line":83,"updatePoint":{"line":83,"column":20,"index":3499},"code":"  it('Load classifer', function (done) {\n    classifier.load(classifierFilename, SEElement, function (err, c) {\n      if (err) {\n        console.log(err);\n      } else {\n        DEBUG && console.log('Classifier loaded from ' + classifierFilename);\n        newClassifier = c;\n      }\n      done();\n    });\n    if (newClassifier) {\n      classifier = newClassifier;\n    }\n  });","file":"MaxEntClassifier_spec.js","skipped":false,"dir":"spec"},{"name":"The classifier classifies events","suites":["The MaxEnt module"],"line":97,"updatePoint":{"line":97,"column":38,"index":3893},"code":"  it('The classifier classifies events', function () {\n    let context = new Context('0');\n    DEBUG && console.log('Classes plus scores ' + JSON.stringify(classifier.getClassifications(context)));\n    let classification = classifier.classify(context);\n    expect(classification).toBe('x');\n    context = new Context('1');\n    DEBUG && console.log('Classes plus scores ' + JSON.stringify(classifier.getClassifications(context)));\n    classification = classifier.classify(context);\n    expect(classification).toBe('y');\n  });","file":"MaxEntClassifier_spec.js","skipped":false,"dir":"spec"},{"name":"should drop duplicate adjacent letters, except C","suites":["metaphone"],"line":27,"updatePoint":{"line":27,"column":54,"index":1234},"code":"  it('should drop duplicate adjacent letters, except C', function () {\n    expect(metaphone.dedup('dropping')).toBe('droping');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should not drop duplicat C","suites":["metaphone"],"line":30,"updatePoint":{"line":30,"column":32,"index":1346},"code":"  it('should not drop duplicat C', function () {\n    expect(metaphone.dedup('accelerate')).toBe('accelerate');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should drop some initial letters","suites":["metaphone"],"line":33,"updatePoint":{"line":33,"column":38,"index":1469},"code":"  it('should drop some initial letters', function () {\n    expect(metaphone.dropInitialLetters('knuth')).toBe('nuth');\n    expect(metaphone.dropInitialLetters('gnat')).toBe('nat');\n    expect(metaphone.dropInitialLetters('aegis')).toBe('egis');\n    expect(metaphone.dropInitialLetters('pneumatic')).toBe('neumatic');\n    expect(metaphone.dropInitialLetters('wrack')).toBe('rack');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should not drop other initial letters","suites":["metaphone"],"line":40,"updatePoint":{"line":40,"column":43,"index":1861},"code":"  it('should not drop other initial letters', function () {\n    expect(metaphone.dropInitialLetters('garbage')).toBe('garbage');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should b if if words end with mb","suites":["metaphone"],"line":43,"updatePoint":{"line":43,"column":38,"index":1991},"code":"  it('should b if if words end with mb', function () {\n    expect(metaphone.dropBafterMAtEnd('dumb')).toBe('dum');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should not drop b after m if not at end of word","suites":["metaphone"],"line":46,"updatePoint":{"line":46,"column":53,"index":2127},"code":"  it('should not drop b after m if not at end of word', function () {\n    expect(metaphone.dropBafterMAtEnd('dumbo')).toBe('dumbo');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should replace CH to X","suites":["metaphone"],"line":49,"updatePoint":{"line":49,"column":28,"index":2241},"code":"  it('should replace CH to X', function () {\n    expect(metaphone.cTransform('change')).toBe('xhange');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should not replace CH to X if part of SCH","suites":["metaphone"],"line":52,"updatePoint":{"line":52,"column":47,"index":2370},"code":"  it('should not replace CH to X if part of SCH', function () {\n    expect(metaphone.cTransform('discharger')).toBe('diskharger');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should replace CIA to X","suites":["metaphone"],"line":55,"updatePoint":{"line":55,"column":29,"index":2489},"code":"  it('should replace CIA to X', function () {\n    expect(metaphone.cTransform('aesthetician')).toBe('aesthetixian');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"C should become S if followed by I, E, or Y","suites":["metaphone"],"line":58,"updatePoint":{"line":58,"column":49,"index":2632},"code":"  it('C should become S if followed by I, E, or Y', function () {\n    expect(metaphone.cTransform('cieling')).toBe('sieling');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform other C's to K","suites":["metaphone"],"line":61,"updatePoint":{"line":61,"column":38,"index":2754},"code":"  it('should transform other C\\'s to K', function () {\n    expect(metaphone.cTransform('cuss')).toBe('kuss');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform D to J if followed by GE, GY, GI","suites":["metaphone"],"line":64,"updatePoint":{"line":64,"column":55,"index":2887},"code":"  it('should transform D to J if followed by GE, GY, GI', function () {\n    expect(metaphone.dTransform('abridge')).toBe('abrijge');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform D to T if not followed by GE, GY, GI","suites":["metaphone"],"line":67,"updatePoint":{"line":67,"column":59,"index":3030},"code":"  it('should transform D to T if not followed by GE, GY, GI', function () {\n    expect(metaphone.dTransform('bid')).toBe('bit');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should drop G before H if not at the end or before vowell","suites":["metaphone"],"line":70,"updatePoint":{"line":70,"column":63,"index":3169},"code":"  it('should drop G before H if not at the end or before vowell', function () {\n    expect(metaphone.dropG('alight')).toBe('aliht');\n    expect(metaphone.dropG('fright')).toBe('friht');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should drop G if followed by N or NED at the end","suites":["metaphone"],"line":74,"updatePoint":{"line":74,"column":54,"index":3352},"code":"  it('should drop G if followed by N or NED at the end', function () {\n    expect(metaphone.dropG('aligned')).toBe('alined');\n    expect(metaphone.dropG('align')).toBe('alin');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform G to J if followed by I, E or Y and not preceeded by G","suites":["metaphone"],"line":78,"updatePoint":{"line":78,"column":77,"index":3558},"code":"  it('should transform G to J if followed by I, E or Y and not preceeded by G', function () {\n    expect(metaphone.transformG('age')).toBe('aje');\n    expect(metaphone.transformG('gin')).toBe('jin');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform G to K","suites":["metaphone"],"line":82,"updatePoint":{"line":82,"column":29,"index":3716},"code":"  it('should transform G to K', function () {\n    expect(metaphone.transformG('august')).toBe('aukust');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should reduce GG to G before turning to K","suites":["metaphone"],"line":85,"updatePoint":{"line":85,"column":47,"index":3845},"code":"  it('should reduce GG to G before turning to K', function () {\n    expect(metaphone.transformG('aggrade')).toBe('akrade');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should drop H if after vowell and not before vowell","suites":["metaphone"],"line":88,"updatePoint":{"line":88,"column":57,"index":3985},"code":"  it('should drop H if after vowell and not before vowell', function () {\n    expect(metaphone.dropH('alriht')).toBe('alrit');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should not drop H if after vowell","suites":["metaphone"],"line":91,"updatePoint":{"line":91,"column":39,"index":4100},"code":"  it('should not drop H if after vowell', function () {\n    expect(metaphone.dropH('that')).toBe('that');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should not drop H if not before vowell","suites":["metaphone"],"line":94,"updatePoint":{"line":94,"column":44,"index":4217},"code":"  it('should not drop H if not before vowell', function () {\n    expect(metaphone.dropH('chump')).toBe('chump');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform CK to K","suites":["metaphone"],"line":97,"updatePoint":{"line":97,"column":30,"index":4322},"code":"  it('should transform CK to K', function () {\n    expect(metaphone.transformCK('check')).toBe('chek');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform PH to F","suites":["metaphone"],"line":100,"updatePoint":{"line":100,"column":30,"index":4432},"code":"  it('should transform PH to F', function () {\n    expect(metaphone.transformPH('phone')).toBe('fone');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform Q to K","suites":["metaphone"],"line":103,"updatePoint":{"line":103,"column":29,"index":4541},"code":"  it('should transform Q to K', function () {\n    expect(metaphone.transformQ('quack')).toBe('kuack');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform S to X if followed by H, IO, or IA","suites":["metaphone"],"line":106,"updatePoint":{"line":106,"column":57,"index":4678},"code":"  it('should transform S to X if followed by H, IO, or IA', function () {\n    expect(metaphone.transformS('shack')).toBe('xhack');\n    expect(metaphone.transformS('sialagogues')).toBe('xialagogues');\n    expect(metaphone.transformS('asia')).toBe('axia');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should not transform S to X if not followed by H, IO, or IA","suites":["metaphone"],"line":111,"updatePoint":{"line":111,"column":65,"index":4947},"code":"  it('should not transform S to X if not followed by H, IO, or IA', function () {\n    expect(metaphone.transformS('substance')).toBe('substance');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform T to X if followed by IA or IO","suites":["metaphone"],"line":114,"updatePoint":{"line":114,"column":53,"index":5088},"code":"  it('should transform T to X if followed by IA or IO', function () {\n    expect(metaphone.transformT('dementia')).toBe('demenxia');\n    expect(metaphone.transformT('abbreviation')).toBe('abbreviaxion');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform TH to 0","suites":["metaphone"],"line":118,"updatePoint":{"line":118,"column":30,"index":5275},"code":"  it('should transform TH to 0', function () {\n    expect(metaphone.transformT('that')).toBe('0at');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should drop T if followed by CH","suites":["metaphone"],"line":121,"updatePoint":{"line":121,"column":37,"index":5389},"code":"  it('should drop T if followed by CH', function () {\n    expect(metaphone.dropT('backstitch')).toBe('backstich');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform V to F","suites":["metaphone"],"line":124,"updatePoint":{"line":124,"column":29,"index":5502},"code":"  it('should transform V to F', function () {\n    expect(metaphone.transformV('vestige')).toBe('festige');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform WH to W if at beginning","suites":["metaphone"],"line":127,"updatePoint":{"line":127,"column":46,"index":5632},"code":"  it('should transform WH to W if at beginning', function () {\n    expect(metaphone.transformWH('whisper')).toBe('wisper');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should drop W if not followed by vowell","suites":["metaphone"],"line":130,"updatePoint":{"line":130,"column":45,"index":5761},"code":"  it('should drop W if not followed by vowell', function () {\n    expect(metaphone.dropW('bowl')).toBe('bol');\n    expect(metaphone.dropW('warsaw')).toBe('warsa');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform X to S if at beginning","suites":["metaphone"],"line":134,"updatePoint":{"line":134,"column":45,"index":5931},"code":"  it('should transform X to S if at beginning', function () {\n    expect(metaphone.transformX('xenophile')).toBe('senophile');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform X to KS if not at beginning","suites":["metaphone"],"line":137,"updatePoint":{"line":137,"column":50,"index":6069},"code":"  it('should transform X to KS if not at beginning', function () {\n    expect(metaphone.transformX('admixed')).toBe('admiksed');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should drop Y of not followed by a vowell","suites":["metaphone"],"line":140,"updatePoint":{"line":140,"column":47,"index":6201},"code":"  it('should drop Y of not followed by a vowell', function () {\n    expect(metaphone.dropY('analyzer')).toBe('analzer');\n    expect(metaphone.dropY('specify')).toBe('specif');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should not drop Y of followed by a vowell","suites":["metaphone"],"line":144,"updatePoint":{"line":144,"column":47,"index":6383},"code":"  it('should not drop Y of followed by a vowell', function () {\n    expect(metaphone.dropY('allying')).toBe('allying');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should transform Z to S","suites":["metaphone"],"line":147,"updatePoint":{"line":147,"column":29,"index":6491},"code":"  it('should transform Z to S', function () {\n    expect(metaphone.transformZ('blaze')).toBe('blase');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should drop all vowels except initial","suites":["metaphone"],"line":150,"updatePoint":{"line":150,"column":43,"index":6614},"code":"  it('should drop all vowels except initial', function () {\n    expect(metaphone.dropVowels('ablaze')).toBe('ablz');\n    expect(metaphone.dropVowels('adamantium')).toBe('admntm');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should do all","suites":["metaphone"],"line":154,"updatePoint":{"line":154,"column":19,"index":6776},"code":"  it('should do all', function () {\n    expect(metaphone.process('ablaze')).toBe('ABLS');\n    expect(metaphone.process('transition')).toBe('TRNSXN');\n    expect(metaphone.process('astronomical')).toBe('ASTRNMKL');\n    expect(metaphone.process('buzzard')).toBe('BSRT');\n    expect(metaphone.process('wonderer')).toBe('WNTRR');\n    expect(metaphone.process('district')).toBe('TSTRKT');\n    expect(metaphone.process('hockey')).toBe('HK');\n    expect(metaphone.process('capital')).toBe('KPTL');\n    expect(metaphone.process('penguin')).toBe('PNKN');\n    expect(metaphone.process('garbonzo')).toBe('KRBNS');\n    expect(metaphone.process('lightning')).toBe('LTNNK');\n    expect(metaphone.process('light')).toBe('LT');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should compare strings","suites":["metaphone"],"line":168,"updatePoint":{"line":168,"column":28,"index":7503},"code":"  it('should compare strings', function () {\n    expect(metaphone.compare('phonetics', 'fonetix')).toBeTruthy();\n    expect(metaphone.compare('phonetics', 'garbonzo')).toBeFalsy();\n    expect(metaphone.compare('PHONETICS', 'fonetix')).toBeTruthy();\n    expect(metaphone.compare('PHONETICS', 'garbonzo')).toBeFalsy();\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should truncate to length specified if code exceeds","suites":["metaphone"],"line":207,"updatePoint":{"line":207,"column":57,"index":9146},"code":"  it('should truncate to length specified if code exceeds', function () {\n    expect(metaphone.process('phonetics', 4)).toBe('FNTK');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should not truncate to length specified if code does not exceed","suites":["metaphone"],"line":210,"updatePoint":{"line":210,"column":69,"index":9298},"code":"  it('should not truncate to length specified if code does not exceed', function () {\n    expect(metaphone.process('phonetics', 8)).toBe('FNTKS');\n  });","file":"metaphone_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram a string via ngrams","suites":["ngrams"],"line":28,"updatePoint":{"line":28,"column":39,"index":1358},"code":"  it('should bigram a string via ngrams', function () {\n    expect(NGrams.ngrams('these are some words', 2)).toEqual([['these', 'are'], ['are', 'some'], ['some', 'words']]);\n  });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram an array via ngrams","suites":["ngrams"],"line":31,"updatePoint":{"line":31,"column":39,"index":1538},"code":"  it('should bigram an array via ngrams', function () {\n    expect(NGrams.ngrams(['these', 'are', 'some', 'words'], 2)).toEqual([['these', 'are'], ['are', 'some'], ['some', 'words']]);\n  });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should trigram a string via ngrams","suites":["ngrams"],"line":34,"updatePoint":{"line":34,"column":40,"index":1730},"code":"  it('should trigram a string via ngrams', function () {\n    expect(NGrams.ngrams('these are some words', 3)).toEqual([['these', 'are', 'some'], ['are', 'some', 'words']]);\n  });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should trigram an array via ngrams","suites":["ngrams"],"line":37,"updatePoint":{"line":37,"column":40,"index":1909},"code":"  it('should trigram an array via ngrams', function () {\n    expect(NGrams.ngrams(['these', 'are', 'some', 'words'], 3)).toEqual([['these', 'are', 'some'], ['are', 'some', 'words']]);\n  });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram a string","suites":["ngrams","bigrams"],"line":41,"updatePoint":{"line":41,"column":30,"index":2125},"code":"    it('should bigram a string', function () {\n      expect(NGrams.bigrams('these are some words')).toEqual([['these', 'are'], ['are', 'some'], ['some', 'words']]);\n    });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram an array","suites":["ngrams","bigrams"],"line":44,"updatePoint":{"line":44,"column":30,"index":2298},"code":"    it('should bigram an array', function () {\n      expect(NGrams.bigrams(['these', 'are', 'some', 'words'])).toEqual([['these', 'are'], ['are', 'some'], ['some', 'words']]);\n    });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram a string","suites":["ngrams","trigrams"],"line":49,"updatePoint":{"line":49,"column":30,"index":2525},"code":"    it('should bigram a string', function () {\n      expect(NGrams.trigrams('these are some words')).toEqual([['these', 'are', 'some'], ['are', 'some', 'words']]);\n    });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram an array","suites":["ngrams","trigrams"],"line":52,"updatePoint":{"line":52,"column":30,"index":2697},"code":"    it('should bigram an array', function () {\n      expect(NGrams.trigrams(['these', 'are', 'some', 'words'])).toEqual([['these', 'are', 'some'], ['are', 'some', 'words']]);\n    });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram a string with start and end symbols","suites":["ngrams","trigrams"],"line":58,"updatePoint":{"line":58,"column":55,"index":2947},"code":"  it('should bigram a string with start and end symbols', function () {\n    expect(NGrams.ngrams('these are some words', 2, '[start]', '[end]')).toEqual([['[start]', 'these'], ['these', 'are'], ['are', 'some'], ['some', 'words'], ['words', '[end]']]);\n  });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram a string with start symbols only","suites":["ngrams","trigrams"],"line":61,"updatePoint":{"line":61,"column":52,"index":3202},"code":"  it('should bigram a string with start symbols only', function () {\n    expect(NGrams.ngrams('these are some words', 2, '[start]')).toEqual([['[start]', 'these'], ['these', 'are'], ['are', 'some'], ['some', 'words']]);\n  });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram a string with end symbols only","suites":["ngrams","trigrams"],"line":64,"updatePoint":{"line":64,"column":50,"index":3426},"code":"  it('should bigram a string with end symbols only', function () {\n    expect(NGrams.ngrams('these are some words', 2, null, '[end]')).toEqual([['these', 'are'], ['are', 'some'], ['some', 'words'], ['words', '[end]']]);\n  });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should trigram a string with start and end symbols","suites":["ngrams","trigrams"],"line":67,"updatePoint":{"line":67,"column":56,"index":3658},"code":"  it('should trigram a string with start and end symbols', function () {\n    expect(NGrams.ngrams('these are some words', 3, '[start]', '[end]')).toEqual([['[start]', '[start]', 'these'], ['[start]', 'these', 'are'], ['these', 'are', 'some'], ['are', 'some', 'words'], ['some', 'words', '[end]'], ['words', '[end]', '[end]']]);\n  });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should 4-gram a string with start and end symbols","suites":["ngrams","trigrams"],"line":70,"updatePoint":{"line":70,"column":55,"index":3991},"code":"  it('should 4-gram a string with start and end symbols', function () {\n    expect(NGrams.ngrams('these are some words', 4, '[start]', '[end]')).toEqual([['[start]', '[start]', '[start]', 'these'], ['[start]', '[start]', 'these', 'are'], ['[start]', 'these', 'are', 'some'], ['these', 'are', 'some', 'words'], ['are', 'some', 'words', '[end]'], ['some', 'words', '[end]', '[end]'], ['words', '[end]', '[end]', '[end]']]);\n  });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should use french tokenizer","suites":["ngrams","trigrams"],"line":73,"updatePoint":{"line":73,"column":33,"index":4397},"code":"  it('should use french tokenizer', function () {\n    const T = require('../lib/natural/tokenizers/aggressive_tokenizer_fr');\n    const t = new T();\n    NGrams.setTokenizer(t);\n    expect(NGrams.ngrams('Un Éléphant rouge', 2)).toEqual([['Un', 'Éléphant'], ['Éléphant', 'rouge']]);\n  });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should trigram a string via ngrams including statistics","suites":["ngrams","trigrams"],"line":79,"updatePoint":{"line":79,"column":61,"index":4712},"code":"  it('should trigram a string via ngrams including statistics', function () {\n    const T = require('../lib/natural/tokenizers/aggressive_tokenizer');\n    const t = new T();\n    NGrams.setTokenizer(t);\n    text.sentences.forEach(sentence => {\n      const result = NGrams.ngrams(sentence, 3, null, null, true);\n      let nrNgrams = 0;\n      Object.keys(result.Nr).forEach(function (f) {\n        nrNgrams += result.Nr[f] * f;\n      });\n      expect(nrNgrams).toEqual(result.numberOfNgrams);\n    });\n  });","file":"ngram_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram a string via ngrams","suites":["ngrams"],"line":27,"updatePoint":{"line":27,"column":39,"index":1211},"code":"  it('should bigram a string via ngrams', function () {\n    expect(NGramsZH.ngrams('中文文本测试', 2)).toEqual([['中', '文'], ['文', '文'], ['文', '本'], ['本', '测'], ['测', '试']]);\n  });","file":"ngram_zh_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram an array via ngrams","suites":["ngrams"],"line":30,"updatePoint":{"line":30,"column":39,"index":1385},"code":"  it('should bigram an array via ngrams', function () {\n    expect(NGramsZH.ngrams(['中', '文', '文', '本', '测', '试'], 2)).toEqual([['中', '文'], ['文', '文'], ['文', '本'], ['本', '测'], ['测', '试']]);\n  });","file":"ngram_zh_spec.js","skipped":false,"dir":"spec"},{"name":"should trigram a string via ngrams","suites":["ngrams"],"line":33,"updatePoint":{"line":33,"column":40,"index":1582},"code":"  it('should trigram a string via ngrams', function () {\n    expect(NGramsZH.ngrams('中文文本测试', 3)).toEqual([['中', '文', '文'], ['文', '文', '本'], ['文', '本', '测'], ['本', '测', '试']]);\n  });","file":"ngram_zh_spec.js","skipped":false,"dir":"spec"},{"name":"should trigram an array via ngrams","suites":["ngrams"],"line":36,"updatePoint":{"line":36,"column":40,"index":1765},"code":"  it('should trigram an array via ngrams', function () {\n    expect(NGramsZH.ngrams(['中', '文', '文', '本', '测', '试'], 3)).toEqual([['中', '文', '文'], ['文', '文', '本'], ['文', '本', '测'], ['本', '测', '试']]);\n  });","file":"ngram_zh_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram a string","suites":["ngrams","bigrams"],"line":40,"updatePoint":{"line":40,"column":30,"index":1996},"code":"    it('should bigram a string', function () {\n      expect(NGramsZH.bigrams('中文文本测试')).toEqual([['中', '文'], ['文', '文'], ['文', '本'], ['本', '测'], ['测', '试']]);\n    });","file":"ngram_zh_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram an array","suites":["ngrams","bigrams"],"line":43,"updatePoint":{"line":43,"column":30,"index":2163},"code":"    it('should bigram an array', function () {\n      expect(NGramsZH.bigrams(['中', '文', '文', '本', '测', '试'])).toEqual([['中', '文'], ['文', '文'], ['文', '本'], ['本', '测'], ['测', '试']]);\n    });","file":"ngram_zh_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram an array","suites":["ngrams","trigrams"],"line":55,"updatePoint":{"line":55,"column":30,"index":2594},"code":"    it('should bigram an array', function () {\n      expect(NGramsZH.ngrams(['中', '文', '文', '本', '测', '试'], 3)).toEqual([['中', '文', '文'], ['文', '文', '本'], ['文', '本', '测'], ['本', '测', '试']]);\n    });","file":"ngram_zh_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram a string with start and end symbols","suites":["ngrams","trigrams"],"line":59,"updatePoint":{"line":59,"column":55,"index":2824},"code":"  it('should bigram a string with start and end symbols', function () {\n    expect(NGramsZH.ngrams('中文测试', 2, '[start]', '[end]')).toEqual([['[start]', '中'], ['中', '文'], ['文', '测'], ['测', '试'], ['试', '[end]']]);\n  });","file":"ngram_zh_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram a string with start symbols only","suites":["ngrams","trigrams"],"line":62,"updatePoint":{"line":62,"column":52,"index":3039},"code":"  it('should bigram a string with start symbols only', function () {\n    expect(NGramsZH.ngrams('中文测试', 2, '[start]')).toEqual([['[start]', '中'], ['中', '文'], ['文', '测'], ['测', '试']]);\n  });","file":"ngram_zh_spec.js","skipped":false,"dir":"spec"},{"name":"should bigram a string with end symbols only","suites":["ngrams","trigrams"],"line":65,"updatePoint":{"line":65,"column":50,"index":3227},"code":"  it('should bigram a string with end symbols only', function () {\n    expect(NGramsZH.ngrams('中文测试', 2, null, '[end]')).toEqual([['中', '文'], ['文', '测'], ['测', '试'], ['试', '[end]']]);\n  });","file":"ngram_zh_spec.js","skipped":false,"dir":"spec"},{"name":"should trigram a string with start and end symbols","suites":["ngrams","trigrams"],"line":68,"updatePoint":{"line":68,"column":56,"index":3423},"code":"  it('should trigram a string with start and end symbols', function () {\n    expect(NGramsZH.ngrams('中文测试', 3, '[start]', '[end]')).toEqual([['[start]', '[start]', '中'], ['[start]', '中', '文'], ['中', '文', '测'], ['文', '测', '试'], ['测', '试', '[end]'], ['试', '[end]', '[end]']]);\n  });","file":"ngram_zh_spec.js","skipped":false,"dir":"spec"},{"name":"should 4-gram a string with start and end symbols","suites":["ngrams","trigrams"],"line":71,"updatePoint":{"line":71,"column":55,"index":3703},"code":"  it('should 4-gram a string with start and end symbols', function () {\n    expect(NGramsZH.ngrams('中文测试', 4, '[start]', '[end]')).toEqual([['[start]', '[start]', '[start]', '中'], ['[start]', '[start]', '中', '文'], ['[start]', '中', '文', '测'], ['中', '文', '测', '试'], ['文', '测', '试', '[end]'], ['测', '试', '[end]', '[end]'], ['试', '[end]', '[end]', '[end]']]);\n  });","file":"ngram_zh_spec.js","skipped":false,"dir":"spec"},{"name":"should fix badly formed hiragana","suites":["normalizeJa"],"line":28,"updatePoint":{"line":28,"column":38,"index":1328},"code":"  it('should fix badly formed hiragana', function () {\n    expect(normalizeJa('う゛か゛き゛く゛は゜ひ゜ふ゜')).toEqual('ゔがぎぐぱぴぷ');\n    expect(normalizeJa('うﾞかﾞきﾞくﾞはﾟひﾟふﾟ')).toEqual('ゔがぎぐぱぴぷ');\n    expect(normalizeJa('まっなか')).toEqual('まんなか');\n  });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should fix badly formed fullwidth katakana","suites":["normalizeJa"],"line":33,"updatePoint":{"line":33,"column":48,"index":1572},"code":"  it('should fix badly formed fullwidth katakana', function () {\n    expect(normalizeJa('ウ゛カ゛キ゛ク゛ハ゜ヒ゜フ゜')).toEqual('ヴガギグパピプ');\n    expect(normalizeJa('ウﾞカﾞキﾞクﾞハﾟヒﾟフﾟ')).toEqual('ヴガギグパピプ');\n  });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should fix badly formed halfwidth katakana","suites":["normalizeJa"],"line":37,"updatePoint":{"line":37,"column":48,"index":1767},"code":"  it('should fix badly formed halfwidth katakana', function () {\n    expect(normalizeJa('ｳ゛ｶ゛ｷ゛ｸ゛ﾊ゜ﾋ゜ﾌ゜')).toEqual('ヴガギグパピプ');\n    expect(normalizeJa('ｳﾞｶﾞｷﾞｸﾞﾊﾟﾋﾟﾌﾟ')).toEqual('ヴガギグパピプ');\n  });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform halfwidth katakana to fullwidth","suites":["normalizeJa"],"line":41,"updatePoint":{"line":41,"column":54,"index":1968},"code":"  it('should transform halfwidth katakana to fullwidth', function () {\n    expect(normalizeJa('ｶﾀｶﾅ')).toEqual('カタカナ');\n  });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform fullwidth alphanumerical characters to halfwidth","suites":["normalizeJa"],"line":44,"updatePoint":{"line":44,"column":71,"index":2111},"code":"  it('should transform fullwidth alphanumerical characters to halfwidth', function () {\n    expect(normalizeJa('ＡＢＣ１２３')).toEqual('ABC123');\n  });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform fullwidth spaces to halfwidth","suites":["normalizeJa"],"line":47,"updatePoint":{"line":47,"column":52,"index":2239},"code":"  it('should transform fullwidth spaces to halfwidth', function () {\n    expect(normalizeJa('空　空　空')).toEqual('空 空 空');\n  });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform halfwidth punctuation signs to fullwidth","suites":["normalizeJa"],"line":50,"updatePoint":{"line":50,"column":63,"index":2376},"code":"  it('should transform halfwidth punctuation signs to fullwidth', function () {\n    // Taken from http://unicode.org/cldr/trac/browser/trunk/common/main/ja.xml\n    expect(normalizeJa('〜 ・ ･ 、､ 。｡ 「｢ 」｣')).toEqual('〜 ・ ・ 、、 。。 「「 」」');\n  });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform fullwidth symbols to halfwidth","suites":["normalizeJa"],"line":54,"updatePoint":{"line":54,"column":53,"index":2607},"code":"  it('should transform fullwidth symbols to halfwidth', function () {\n    // Taken from http://unicode.org/cldr/trac/browser/trunk/common/main/ja.xml\n    expect(normalizeJa('‾ _＿ -－ ‐ — ― ,， ;； :： !！ ?？ .． ‥ … ＇＼ ‘ ’ \"＂ “ ” (（ )） [［ ]］ {｛ }｝ 〈 〉 《 》 『 』 【 】 〔 〕 ‖ § ¶ @＠ +＋ ^＾ $＄ *＊ /／ ＼\\\\ &＆ #＃ %％ ‰ † ‡ ′ ″ 〃 ※')).toEqual('‾ __ -- ‐ — ― ,, ;; :: !! ?? .. ‥ … ＇\\\\ ‘ ’ \"\" “ ” (( )) [[ ]] {{ }} 〈 〉 《 》 『 』 【 】 〔 〕 ‖ § ¶ @@ ++ ^^ $$ ** // \\\\\\\\ && ## %% ‰ † ‡ ′ ″ 〃 ※');\n  });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should replace repeat characters","suites":["normalizeJa"],"line":58,"updatePoint":{"line":58,"column":38,"index":3067},"code":"  it('should replace repeat characters', function () {\n    expect(normalizeJa('時々刻々')).toEqual('時時刻刻');\n    expect(normalizeJa('甲斐々々しい')).toEqual('甲斐甲斐しい');\n  });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should replace composite symbols","suites":["normalizeJa"],"line":62,"updatePoint":{"line":62,"column":38,"index":3230},"code":"  it('should replace composite symbols', function () {\n    expect(normalizeJa('㍼54年㋃㏪')).toEqual('昭和54年4月11日');\n    expect(normalizeJa('㍧〜㍬')).toEqual('15点〜20点');\n    expect(normalizeJa('カンパニー㍿')).toEqual('カンパニー株式会社');\n    expect(normalizeJa('100㌫')).toEqual('100パーセント');\n    expect(normalizeJa('70㌔')).toEqual('70キロ');\n    expect(normalizeJa('㍇')).toEqual('マンション');\n  });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should all be reversible","suites":["converters"],"line":73,"updatePoint":{"line":73,"column":30,"index":3703},"code":"  it('should all be reversible', function () {\n    const sample = '半角カナ（はんかくカナ）とは、JIS X 0208など片仮名を含む他の文字集合と同時に運用される場合におけるJIS X 0201の片仮名文字集合の通称である。漢字を含む文字集合で定義された片仮名に対して、半分の文字幅で表示されることが一般的であったためこのように呼ばれる。JIS X 0201で規定される8ビット符号化およびShift_JISにおいて0xA1-0xDFの範囲の1バイト文字がこれにあたる。また、Shift_JISやEUC-JPなどの符号化方式やUnicodeでも互換性の目的でこの文字集合をもっている。';\n    expect(converters.halfwidthToFullwidth.alphabet(converters.fullwidthToHalfwidth.alphabet(sample))).toEqual(converters.halfwidthToFullwidth.alphabet(sample));\n    expect(converters.fullwidthToHalfwidth.alphabet(converters.halfwidthToFullwidth.alphabet(sample))).toEqual(converters.fullwidthToHalfwidth.alphabet(sample));\n    expect(converters.halfwidthToFullwidth.numbers(converters.fullwidthToHalfwidth.numbers(sample))).toEqual(converters.halfwidthToFullwidth.numbers(sample));\n    expect(converters.fullwidthToHalfwidth.numbers(converters.halfwidthToFullwidth.numbers(sample))).toEqual(converters.fullwidthToHalfwidth.numbers(sample));\n    expect(converters.halfwidthToFullwidth.punctuation(converters.fullwidthToHalfwidth.punctuation(sample))).toEqual(converters.halfwidthToFullwidth.punctuation(sample));\n    expect(converters.fullwidthToHalfwidth.punctuation(converters.halfwidthToFullwidth.punctuation(sample))).toEqual(converters.fullwidthToHalfwidth.punctuation(sample));\n    expect(converters.halfwidthToFullwidth.katakana(converters.fullwidthToHalfwidth.katakana(sample))).toEqual(converters.halfwidthToFullwidth.katakana(sample));\n    expect(converters.fullwidthToHalfwidth.katakana(converters.halfwidthToFullwidth.katakana(sample))).toEqual(converters.fullwidthToHalfwidth.katakana(sample));\n  });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform fullwidth roman characters and space to halfwidth","suites":["converters",".fullwidthToHalfwidth",".alphabet"],"line":86,"updatePoint":{"line":86,"column":76,"index":5482},"code":"      it('should transform fullwidth roman characters and space to halfwidth', function () {\n        expect(converters.fullwidthToHalfwidth.alphabet(sample)).toEqual('ABC ABC 123１２３.,-．，-ゔあいうえおはばぱｶｷｸｹｺﾊﾊﾞﾊﾟヴカキクケコハバパ');\n      });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform fullwidth numerical characters to halfwidth","suites":["converters",".fullwidthToHalfwidth",".numbers"],"line":91,"updatePoint":{"line":91,"column":70,"index":5752},"code":"      it('should transform fullwidth numerical characters to halfwidth', function () {\n        expect(converters.fullwidthToHalfwidth.numbers(sample)).toEqual('ABC ＡＢＣ　123123.,-．，-ゔあいうえおはばぱｶｷｸｹｺﾊﾊﾞﾊﾟヴカキクケコハバパ');\n      });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform fullwidth punctuation signs to halfwidth","suites":["converters",".fullwidthToHalfwidth",".punctuation"],"line":96,"updatePoint":{"line":96,"column":67,"index":6022},"code":"      it('should transform fullwidth punctuation signs to halfwidth', function () {\n        expect(converters.fullwidthToHalfwidth.punctuation(sample)).toEqual('ABC ＡＢＣ　123１２３.,-.,-ゔあいうえおはばぱｶｷｸｹｺﾊﾊﾞﾊﾟヴカキクケコハバパ');\n      });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform fullwidth katakana to halfwidth","suites":["converters",".fullwidthToHalfwidth",".katakana"],"line":101,"updatePoint":{"line":101,"column":58,"index":6284},"code":"      it('should transform fullwidth katakana to halfwidth', function () {\n        expect(converters.fullwidthToHalfwidth.katakana(sample)).toEqual('ABC ＡＢＣ　123１２３.,-．，-ゔあいうえおはばぱｶｷｸｹｺﾊﾊﾞﾊﾟｳﾞｶｷｸｹｺﾊﾊﾞﾊﾟ');\n      });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform halfwidth roman characters and space to fullwidth","suites":["converters",".halfwidthToFullwidth",".alphabet"],"line":108,"updatePoint":{"line":108,"column":76,"index":6620},"code":"      it('should transform halfwidth roman characters and space to fullwidth', function () {\n        expect(converters.halfwidthToFullwidth.alphabet(sample)).toEqual('ＡＢＣ　ＡＢＣ　123１２３.,-．，-ゔあいうえおはばぱｶｷｸｹｺﾊﾊﾞﾊﾟヴカキクケコハバパ');\n      });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform halfwidth numerical characters to fullwidth","suites":["converters",".halfwidthToFullwidth",".numbers"],"line":113,"updatePoint":{"line":113,"column":70,"index":6890},"code":"      it('should transform halfwidth numerical characters to fullwidth', function () {\n        expect(converters.halfwidthToFullwidth.numbers(sample)).toEqual('ABC ＡＢＣ　１２３１２３.,-．，-ゔあいうえおはばぱｶｷｸｹｺﾊﾊﾞﾊﾟヴカキクケコハバパ');\n      });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform halfwidth punctuation signs to fullwidth","suites":["converters",".halfwidthToFullwidth",".punctuation"],"line":118,"updatePoint":{"line":118,"column":67,"index":7160},"code":"      it('should transform halfwidth punctuation signs to fullwidth', function () {\n        expect(converters.halfwidthToFullwidth.punctuation(sample)).toEqual('ABC ＡＢＣ　123１２３．，─．，─ゔあいうえおはばぱｶｷｸｹｺﾊﾊﾞﾊﾟヴカキクケコハバパ');\n      });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform halfwidth katakana to fullwidth","suites":["converters",".halfwidthToFullwidth",".katakana"],"line":123,"updatePoint":{"line":123,"column":58,"index":7422},"code":"      it('should transform halfwidth katakana to fullwidth', function () {\n        expect(converters.halfwidthToFullwidth.katakana(sample)).toEqual('ABC ＡＢＣ　123１２３.,-．，-ゔあいうえおはばぱカキクケコハバパヴカキクケコハバパ');\n      });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform hiragana to katakana","suites":["converters",".hiraganaToKatakana"],"line":129,"updatePoint":{"line":129,"column":45,"index":7680},"code":"    it('should transform hiragana to katakana', function () {\n      expect(converters.hiraganaToKatakana(sample)).toEqual('ABC ＡＢＣ　123１２３.,-．，-ヴアイウエオハバパカキクケコハバパヴカキクケコハバパ');\n    });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transform katakana to hiragana","suites":["converters",".katakanaToHiragana"],"line":134,"updatePoint":{"line":134,"column":45,"index":7915},"code":"    it('should transform katakana to hiragana', function () {\n      expect(converters.katakanaToHiragana(sample)).toEqual('ABC ＡＢＣ　123１２３.,-．，-ゔあいうえおはばぱかきくけこはばぱゔかきくけこはばぱ');\n    });","file":"normalizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should leave uppercase and lowercase ä's untouched","suites":["normalizer_no","Normalization of diacritical marks"],"line":31,"updatePoint":{"line":31,"column":58,"index":1374},"code":"    it(\"should leave uppercase and lowercase ä's untouched\", function () {\n      expect(normalizer.removeDiacritics('ä')).toBe('ä');\n      expect(normalizer.removeDiacritics('Ä')).toBe('Ä');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should leave uppercase and lowercase ö's untouched","suites":["normalizer_no","Normalization of diacritical marks"],"line":35,"updatePoint":{"line":35,"column":58,"index":1573},"code":"    it(\"should leave uppercase and lowercase ö's untouched\", function () {\n      expect(normalizer.removeDiacritics('ö')).toBe('ö');\n      expect(normalizer.removeDiacritics('Ö')).toBe('Ö');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should leave uppercase and lowercase ü's untouched","suites":["normalizer_no","Normalization of diacritical marks"],"line":39,"updatePoint":{"line":39,"column":58,"index":1772},"code":"    it(\"should leave uppercase and lowercase ü's untouched\", function () {\n      expect(normalizer.removeDiacritics('ü')).toBe('ü');\n      expect(normalizer.removeDiacritics('Ü')).toBe('Ü');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize uppercase and lowercase a's with grave accents","suites":["normalizer_no","Normalization of diacritical marks"],"line":43,"updatePoint":{"line":43,"column":81,"index":1994},"code":"    it(\"should correctly normalize uppercase and lowercase a's with grave accents\", function () {\n      expect(normalizer.removeDiacritics('à')).toBe('a');\n      expect(normalizer.removeDiacritics('À')).toBe('A');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize uppercase and lowercase a's with acute accents","suites":["normalizer_no","Normalization of diacritical marks"],"line":47,"updatePoint":{"line":47,"column":81,"index":2216},"code":"    it(\"should correctly normalize uppercase and lowercase a's with acute accents\", function () {\n      expect(normalizer.removeDiacritics('á')).toBe('a');\n      expect(normalizer.removeDiacritics('Á')).toBe('A');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize uppercase and lowercase a's with circumflex accents","suites":["normalizer_no","Normalization of diacritical marks"],"line":51,"updatePoint":{"line":51,"column":86,"index":2443},"code":"    it(\"should correctly normalize uppercase and lowercase a's with circumflex accents\", function () {\n      expect(normalizer.removeDiacritics('â')).toBe('a');\n      expect(normalizer.removeDiacritics('Â')).toBe('A');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize uppercase and lowercase c's with cedillas","suites":["normalizer_no","Normalization of diacritical marks"],"line":55,"updatePoint":{"line":55,"column":76,"index":2660},"code":"    it(\"should correctly normalize uppercase and lowercase c's with cedillas\", function () {\n      expect(normalizer.removeDiacritics('ç')).toBe('c');\n      expect(normalizer.removeDiacritics('Ç')).toBe('C');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize uppercase and lowercase e's with grave accents","suites":["normalizer_no","Normalization of diacritical marks"],"line":59,"updatePoint":{"line":59,"column":81,"index":2882},"code":"    it(\"should correctly normalize uppercase and lowercase e's with grave accents\", function () {\n      expect(normalizer.removeDiacritics('è')).toBe('e');\n      expect(normalizer.removeDiacritics('È')).toBe('E');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize uppercase and lowercase e's with acute accents","suites":["normalizer_no","Normalization of diacritical marks"],"line":63,"updatePoint":{"line":63,"column":81,"index":3104},"code":"    it(\"should correctly normalize uppercase and lowercase e's with acute accents\", function () {\n      expect(normalizer.removeDiacritics('é')).toBe('e');\n      expect(normalizer.removeDiacritics('É')).toBe('E');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize uppercase and lowercase e's with circumflex accents","suites":["normalizer_no","Normalization of diacritical marks"],"line":67,"updatePoint":{"line":67,"column":86,"index":3331},"code":"    it(\"should correctly normalize uppercase and lowercase e's with circumflex accents\", function () {\n      expect(normalizer.removeDiacritics('ê')).toBe('e');\n      expect(normalizer.removeDiacritics('Ê')).toBe('E');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize uppercase and lowercase i's with circumflex accents","suites":["normalizer_no","Normalization of diacritical marks"],"line":71,"updatePoint":{"line":71,"column":86,"index":3558},"code":"    it(\"should correctly normalize uppercase and lowercase i's with circumflex accents\", function () {\n      expect(normalizer.removeDiacritics('î')).toBe('i');\n      expect(normalizer.removeDiacritics('Î')).toBe('I');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize uppercase and lowercase n's with tildes","suites":["normalizer_no","Normalization of diacritical marks"],"line":75,"updatePoint":{"line":75,"column":74,"index":3773},"code":"    it(\"should correctly normalize uppercase and lowercase n's with tildes\", function () {\n      expect(normalizer.removeDiacritics('ñ')).toBe('n');\n      expect(normalizer.removeDiacritics('Ñ')).toBe('N');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize uppercase and lowercase o's with acute accents","suites":["normalizer_no","Normalization of diacritical marks"],"line":79,"updatePoint":{"line":79,"column":81,"index":3995},"code":"    it(\"should correctly normalize uppercase and lowercase o's with acute accents\", function () {\n      expect(normalizer.removeDiacritics('ó')).toBe('o');\n      expect(normalizer.removeDiacritics('Ó')).toBe('O');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize uppercase and lowercase o's with circumflex accents","suites":["normalizer_no","Normalization of diacritical marks"],"line":83,"updatePoint":{"line":83,"column":86,"index":4222},"code":"    it(\"should correctly normalize uppercase and lowercase o's with circumflex accents\", function () {\n      expect(normalizer.removeDiacritics('ô')).toBe('o');\n      expect(normalizer.removeDiacritics('Ô')).toBe('O');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize uppercase and lowercase u's with circumflex accents","suites":["normalizer_no","Normalization of diacritical marks"],"line":87,"updatePoint":{"line":87,"column":86,"index":4449},"code":"    it(\"should correctly normalize uppercase and lowercase u's with circumflex accents\", function () {\n      expect(normalizer.removeDiacritics('û')).toBe('u');\n      expect(normalizer.removeDiacritics('Û')).toBe('U');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize uppercase and lowercase s's with caron/wedge accents","suites":["normalizer_no","Normalization of diacritical marks"],"line":91,"updatePoint":{"line":91,"column":87,"index":4677},"code":"    it(\"should correctly normalize uppercase and lowercase s's with caron/wedge accents\", function () {\n      expect(normalizer.removeDiacritics('š')).toBe('s');\n      expect(normalizer.removeDiacritics('Š')).toBe('S');\n    });","file":"normalizer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize n't as not","suites":["normalizer","normal operations (rule execution)"],"line":31,"updatePoint":{"line":31,"column":45,"index":1417},"code":"    it(\"should correctly normalize n't as not\", function () {\n      expect(JSON.stringify(normalizer.normalizeTokens([\"hasn't\"]))).toBe(JSON.stringify(['has', 'not']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"hadn't\"]))).toBe(JSON.stringify(['had', 'not']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"haven't\"]))).toBe(JSON.stringify(['have', 'not']));\n    });","file":"normalizer_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize 's as is","suites":["normalizer","normal operations (rule execution)"],"line":36,"updatePoint":{"line":36,"column":43,"index":1808},"code":"    it(\"should correctly normalize 's as is\", function () {\n      expect(JSON.stringify(normalizer.normalizeTokens([\"it's\"]))).toBe(JSON.stringify(['it', 'is']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"he's\"]))).toBe(JSON.stringify(['he', 'is']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"here's\"]))).toBe(JSON.stringify(['here', 'is']));\n    });","file":"normalizer_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize 'll as will","suites":["normalizer","normal operations (rule execution)"],"line":41,"updatePoint":{"line":41,"column":46,"index":2192},"code":"    it(\"should correctly normalize 'll as will\", function () {\n      expect(JSON.stringify(normalizer.normalizeTokens([\"we'll\"]))).toBe(JSON.stringify(['we', 'will']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"he'll\"]))).toBe(JSON.stringify(['he', 'will']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"I'll\"]))).toBe(JSON.stringify(['I', 'will']));\n    });","file":"normalizer_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize 're as are","suites":["normalizer","normal operations (rule execution)"],"line":46,"updatePoint":{"line":46,"column":45,"index":2578},"code":"    it(\"should correctly normalize 're as are\", function () {\n      expect(JSON.stringify(normalizer.normalizeTokens([\"we're\"]))).toBe(JSON.stringify(['we', 'are']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"how're\"]))).toBe(JSON.stringify(['how', 'are']));\n    });","file":"normalizer_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize 'd as would","suites":["normalizer","normal operations (rule execution)"],"line":50,"updatePoint":{"line":50,"column":46,"index":2861},"code":"    it(\"should correctly normalize 'd as would\", function () {\n      expect(JSON.stringify(normalizer.normalizeTokens([\"he'd\"]))).toBe(JSON.stringify(['he', 'would']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"I'd\"]))).toBe(JSON.stringify(['I', 'would']));\n    });","file":"normalizer_spec.js","skipped":false,"dir":"spec"},{"name":"should convert can't to cannot","suites":["normalizer","special cases"],"line":60,"updatePoint":{"line":60,"column":38,"index":3292},"code":"    it(\"should convert can't to cannot\", function () {\n      expect(JSON.stringify(normalizer.normalizeTokens([\"can't\"]))).toBe(JSON.stringify(['can', 'not']));\n    });","file":"normalizer_spec.js","skipped":false,"dir":"spec"},{"name":"should convert couldn't've as could not have","suites":["normalizer","special cases"],"line":63,"updatePoint":{"line":63,"column":52,"index":3475},"code":"    it(\"should convert couldn't've as could not have\", function () {\n      expect(JSON.stringify(normalizer.normalizeTokens([\"couldn't've\"]))).toBe(JSON.stringify(['could', 'not', 'have']));\n    });","file":"normalizer_spec.js","skipped":false,"dir":"spec"},{"name":"should convert how'd to how did","suites":["normalizer","special cases"],"line":66,"updatePoint":{"line":66,"column":39,"index":3661},"code":"    it(\"should convert how'd to how did\", function () {\n      expect(JSON.stringify(normalizer.normalizeTokens([\"how'd\"]))).toBe(JSON.stringify(['how', 'did']));\n    });","file":"normalizer_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly normalize I'm as I am","suites":["normalizer","special cases"],"line":69,"updatePoint":{"line":69,"column":46,"index":3838},"code":"    it(\"should correctly normalize I'm as I am\", function () {\n      expect(JSON.stringify(normalizer.normalizeTokens([\"I'm\"]))).toBe(JSON.stringify(['I', 'am']));\n    });","file":"normalizer_spec.js","skipped":false,"dir":"spec"},{"name":"should handle different cases on special case conversion","suites":["normalizer","basic properties"],"line":78,"updatePoint":{"line":78,"column":64,"index":4152},"code":"    it('should handle different cases on special case conversion', function () {\n      expect(JSON.stringify(normalizer.normalizeTokens([\"I'm\"]))).toBe(JSON.stringify(['I', 'am']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"i'm\"]))).toBe(JSON.stringify(['I', 'am']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"can't\"]))).toBe(JSON.stringify(['can', 'not']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"Can't\"]))).toBe(JSON.stringify(['can', 'not']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"CaN'T\"]))).toBe(JSON.stringify(['can', 'not']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"how'd\"]))).toBe(JSON.stringify(['how', 'did']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"how'D\"]))).toBe(JSON.stringify(['how', 'did']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"HOw'd\"]))).toBe(JSON.stringify(['how', 'did']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"COULDN't've\"]))).toBe(JSON.stringify(['could', 'not', 'have']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"couldn'T've\"]))).toBe(JSON.stringify(['could', 'not', 'have']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"couldn't'VE\"]))).toBe(JSON.stringify(['could', 'not', 'have']));\n    });","file":"normalizer_spec.js","skipped":false,"dir":"spec"},{"name":"should handle different cases on rule-based conversion","suites":["normalizer","basic properties"],"line":93,"updatePoint":{"line":93,"column":62,"index":5524},"code":"    it('should handle different cases on rule-based conversion', function () {\n      expect(JSON.stringify(normalizer.normalizeTokens([\"Hasn't\"]))).toBe(JSON.stringify(['Has', 'not']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"HAsn't\"]))).toBe(JSON.stringify(['HAs', 'not']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"hasn'T\"]))).toBe(JSON.stringify(['has', 'not']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"It's\"]))).toBe(JSON.stringify(['It', 'is']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"IT's\"]))).toBe(JSON.stringify(['IT', 'is']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"it'S\"]))).toBe(JSON.stringify(['it', 'is']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"We'll\"]))).toBe(JSON.stringify(['We', 'will']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"WE'll\"]))).toBe(JSON.stringify(['WE', 'will']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"we'Ll\"]))).toBe(JSON.stringify(['we', 'will']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"How're\"]))).toBe(JSON.stringify(['How', 'are']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"hOW're\"]))).toBe(JSON.stringify(['hOW', 'are']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"how'RE\"]))).toBe(JSON.stringify(['how', 'are']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"I'd\"]))).toBe(JSON.stringify(['I', 'would']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"i'd\"]))).toBe(JSON.stringify(['i', 'would']));\n      expect(JSON.stringify(normalizer.normalizeTokens([\"I'D\"]))).toBe(JSON.stringify(['I', 'would']));\n    });","file":"normalizer_spec.js","skipped":false,"dir":"spec"},{"name":"should convert a string to an array for normalization","suites":["normalizer","basic properties"],"line":110,"updatePoint":{"line":110,"column":61,"index":7191},"code":"    it('should convert a string to an array for normalization', function () {\n      expect(JSON.stringify(normalizer.normalizeTokens(\"I'D\"))).toBe(JSON.stringify(['I', 'would']));\n    });","file":"normalizer_spec.js","skipped":false,"dir":"spec"},{"name":"should simply tokenize a string that does not match a rule","suites":["normalizer","basic properties"],"line":113,"updatePoint":{"line":113,"column":66,"index":7384},"code":"    it('should simply tokenize a string that does not match a rule', function () {\n      expect(JSON.stringify(normalizer.normalizeTokens(['Has', 'not']))).toBe(JSON.stringify(['Has', 'not']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['HAs', 'not']))).toBe(JSON.stringify(['HAs', 'not']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['has', 'noT']))).toBe(JSON.stringify(['has', 'noT']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['It', 'is']))).toBe(JSON.stringify(['It', 'is']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['IT', 'is']))).toBe(JSON.stringify(['IT', 'is']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['it', 'iS']))).toBe(JSON.stringify(['it', 'iS']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['We', 'will']))).toBe(JSON.stringify(['We', 'will']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['WE', 'will']))).toBe(JSON.stringify(['WE', 'will']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['we', 'wiLl']))).toBe(JSON.stringify(['we', 'wiLl']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['How', 'are']))).toBe(JSON.stringify(['How', 'are']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['hOW', 'are']))).toBe(JSON.stringify(['hOW', 'are']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['how', 'aRE']))).toBe(JSON.stringify(['how', 'aRE']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['I', 'would']))).toBe(JSON.stringify(['I', 'would']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['i', 'would']))).toBe(JSON.stringify(['i', 'would']));\n      expect(JSON.stringify(normalizer.normalizeTokens(['I', 'woulD']))).toBe(JSON.stringify(['I', 'woulD']));\n    });","file":"normalizer_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns ending by -al","suites":["NounInflector",".pluralize()"],"line":29,"updatePoint":{"line":29,"column":54,"index":1336},"code":"    it('should pluralize exception nouns ending by -al', function () {\n      expect(inflector.pluralize('carnaval')).toBe('carnavals');\n      expect(inflector.pluralize('narval')).toBe('narvals');\n      expect(inflector.pluralize('récital')).toBe('récitals');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -al","suites":["NounInflector",".pluralize()"],"line":34,"updatePoint":{"line":34,"column":52,"index":1602},"code":"    it('should pluralize regular nouns ending by -al', function () {\n      expect(inflector.pluralize('amiral')).toBe('amiraux');\n      expect(inflector.pluralize('cheval')).toBe('chevaux');\n      expect(inflector.pluralize('général')).toBe('généraux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns ending by -ail","suites":["NounInflector",".pluralize()"],"line":39,"updatePoint":{"line":39,"column":55,"index":1867},"code":"    it('should pluralize exception nouns ending by -ail', function () {\n      expect(inflector.pluralize('bail')).toBe('baux');\n      expect(inflector.pluralize('vitrail')).toBe('vitraux');\n      expect(inflector.pluralize('émail')).toBe('émaux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -ail","suites":["NounInflector",".pluralize()"],"line":44,"updatePoint":{"line":44,"column":53,"index":2121},"code":"    it('should pluralize regular nouns ending by -ail', function () {\n      expect(inflector.pluralize('détail')).toBe('détails');\n      expect(inflector.pluralize('poitrail')).toBe('poitrails');\n      expect(inflector.pluralize('chandail')).toBe('chandails');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns ending by -il","suites":["NounInflector",".pluralize()"],"line":49,"updatePoint":{"line":49,"column":54,"index":2391},"code":"    it('should pluralize exception nouns ending by -il', function () {\n      expect(inflector.pluralize('ciel')).toBe('cieux');\n      expect(inflector.pluralize('œil')).toBe('yeux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns ending by -ou","suites":["NounInflector",".pluralize()"],"line":53,"updatePoint":{"line":53,"column":54,"index":2582},"code":"    it('should pluralize exception nouns ending by -ou', function () {\n      expect(inflector.pluralize('bijou')).toBe('bijoux');\n      expect(inflector.pluralize('joujou')).toBe('joujoux');\n      expect(inflector.pluralize('hibou')).toBe('hiboux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -ou","suites":["NounInflector",".pluralize()"],"line":58,"updatePoint":{"line":58,"column":52,"index":2838},"code":"    it('should pluralize regular nouns ending by -ou', function () {\n      expect(inflector.pluralize('trou')).toBe('trous');\n      expect(inflector.pluralize('bambou')).toBe('bambous');\n      expect(inflector.pluralize('toutou')).toBe('toutous');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns ending by -au","suites":["NounInflector",".pluralize()"],"line":63,"updatePoint":{"line":63,"column":54,"index":3096},"code":"    it('should pluralize exception nouns ending by -au', function () {\n      expect(inflector.pluralize('berimbau')).toBe('berimbaus');\n      expect(inflector.pluralize('landau')).toBe('landaus');\n      expect(inflector.pluralize('pilau')).toBe('pilaus');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -au","suites":["NounInflector",".pluralize()"],"line":68,"updatePoint":{"line":68,"column":52,"index":3358},"code":"    it('should pluralize regular nouns ending by -au', function () {\n      expect(inflector.pluralize('cadeau')).toBe('cadeaux');\n      expect(inflector.pluralize('beau')).toBe('beaux');\n      expect(inflector.pluralize('étau')).toBe('étaux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns ending by -eu","suites":["NounInflector",".pluralize()"],"line":73,"updatePoint":{"line":73,"column":54,"index":3612},"code":"    it('should pluralize exception nouns ending by -eu', function () {\n      expect(inflector.pluralize('bleu')).toBe('bleus');\n      expect(inflector.pluralize('émeu')).toBe('émeus');\n      expect(inflector.pluralize('pneu')).toBe('pneus');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -eu","suites":["NounInflector",".pluralize()"],"line":78,"updatePoint":{"line":78,"column":52,"index":3860},"code":"    it('should pluralize regular nouns ending by -eu', function () {\n      expect(inflector.pluralize('pieu')).toBe('pieux');\n      expect(inflector.pluralize('lieu')).toBe('lieux');\n      expect(inflector.pluralize('feu')).toBe('feux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -eau","suites":["NounInflector",".pluralize()"],"line":83,"updatePoint":{"line":83,"column":53,"index":4107},"code":"    it('should pluralize regular nouns ending by -eau', function () {\n      expect(inflector.pluralize('eau')).toBe('eaux');\n      expect(inflector.pluralize('manteau')).toBe('manteaux');\n      expect(inflector.pluralize('arbrisseau')).toBe('arbrisseaux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -œu","suites":["NounInflector",".pluralize()"],"line":88,"updatePoint":{"line":88,"column":52,"index":4371},"code":"    it('should pluralize regular nouns ending by -œu', function () {\n      expect(inflector.pluralize('vœu')).toBe('vœux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -s, -x or -z","suites":["NounInflector",".pluralize()"],"line":91,"updatePoint":{"line":91,"column":61,"index":4512},"code":"    it('should pluralize regular nouns ending by -s, -x or -z', function () {\n      expect(inflector.pluralize('os')).toBe('os');\n      expect(inflector.pluralize('cas')).toBe('cas');\n      expect(inflector.pluralize('rhinocéros')).toBe('rhinocéros');\n      expect(inflector.pluralize('houx')).toBe('houx');\n      expect(inflector.pluralize('lynx')).toBe('lynx');\n      expect(inflector.pluralize('roux')).toBe('roux');\n      expect(inflector.pluralize('gaz')).toBe('gaz');\n      expect(inflector.pluralize('quartz')).toBe('quartz');\n      expect(inflector.pluralize('quiz')).toBe('quiz');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns","suites":["NounInflector",".pluralize()"],"line":102,"updatePoint":{"line":102,"column":40,"index":5089},"code":"    it('should pluralize exception nouns', function () {\n      expect(inflector.pluralize('ail')).toBe('aulx');\n      expect(inflector.pluralize('bétail')).toBe('bestiaux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns","suites":["NounInflector",".pluralize()"],"line":106,"updatePoint":{"line":106,"column":38,"index":5269},"code":"    it('should pluralize regular nouns', function () {\n      expect(inflector.pluralize('chai')).toBe('chais');\n      expect(inflector.pluralize('vérité')).toBe('vérités');\n      expect(inflector.pluralize('orange')).toBe('oranges');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -aux","suites":["NounInflector",".singularize()"],"line":113,"updatePoint":{"line":113,"column":55,"index":5577},"code":"    it('should singularize regular nouns ending by -aux', function () {\n      expect(inflector.singularize('amiraux')).toBe('amiral');\n      expect(inflector.singularize('chevaux')).toBe('cheval');\n      expect(inflector.singularize('généraux')).toBe('général');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize exception nouns ending by -aux","suites":["NounInflector",".singularize()"],"line":118,"updatePoint":{"line":118,"column":57,"index":5850},"code":"    it('should singularize exception nouns ending by -aux', function () {\n      expect(inflector.singularize('baux')).toBe('bail');\n      expect(inflector.singularize('vitraux')).toBe('vitrail');\n      expect(inflector.singularize('émaux')).toBe('émail');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -aux","suites":["NounInflector",".singularize()"],"line":123,"updatePoint":{"line":123,"column":55,"index":6112},"code":"    it('should singularize regular nouns ending by -aux', function () {\n      expect(inflector.singularize('cadeaux')).toBe('cadeau');\n      expect(inflector.singularize('beaux')).toBe('beau');\n      expect(inflector.singularize('étaux')).toBe('étau');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize nouns with multiple plural forms","suites":["NounInflector",".singularize()"],"line":128,"updatePoint":{"line":128,"column":59,"index":6377},"code":"    it('should singularize nouns with multiple plural forms', function () {\n      expect(inflector.singularize('ails')).toBe('ail');\n      expect(inflector.singularize('aulx')).toBe('ail');\n      expect(inflector.singularize('ciels')).toBe('ciel');\n      expect(inflector.singularize('cieux')).toBe('ciel');\n      expect(inflector.singularize('œils')).toBe('œil');\n      expect(inflector.singularize('yeux')).toBe('œil');\n      expect(inflector.singularize('aïeuls')).toBe('aïeul'); // Regular\n      expect(inflector.singularize('aïeux')).toBe('aïeul');\n      expect(inflector.singularize('bisaïeuls')).toBe('bisaïeul'); // Regular\n      expect(inflector.singularize('bisaïeux')).toBe('bisaïeul');\n      expect(inflector.singularize('craus')).toBe('crau');\n      expect(inflector.singularize('craux')).toBe('crau');\n      expect(inflector.singularize('graus')).toBe('grau');\n      expect(inflector.singularize('graux')).toBe('grau');\n      expect(inflector.singularize('sénaus')).toBe('sénau');\n      expect(inflector.singularize('sénaux')).toBe('sénau');\n      expect(inflector.singularize('tussaus')).toBe('tussau');\n      expect(inflector.singularize('tussaux')).toBe('tussau');\n      expect(inflector.singularize('emposieus')).toBe('emposieu');\n      expect(inflector.singularize('emposieux')).toBe('emposieu');\n      expect(inflector.singularize('richelieus')).toBe('richelieu');\n      expect(inflector.singularize('richelieux')).toBe('richelieu');\n      // expect(inflector.singularize('feus')).toBe('feu'); // This one is an adjective.\n      expect(inflector.singularize('feux')).toBe('feu');\n      expect(inflector.singularize('lieus')).toBe('lieu'); // Fish\n      expect(inflector.singularize('lieux')).toBe('lieu');\n      expect(inflector.singularize('corails')).toBe('corail');\n      expect(inflector.singularize('coraux')).toBe('corail');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize exception nouns ending by -oux","suites":["NounInflector",".singularize()"],"line":158,"updatePoint":{"line":158,"column":57,"index":8234},"code":"    it('should singularize exception nouns ending by -oux', function () {\n      expect(inflector.singularize('bijoux')).toBe('bijou');\n      expect(inflector.singularize('joujoux')).toBe('joujou');\n      expect(inflector.singularize('hiboux')).toBe('hibou');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize exception nouns ending by -eus","suites":["NounInflector",".singularize()"],"line":163,"updatePoint":{"line":163,"column":57,"index":8501},"code":"    it('should singularize exception nouns ending by -eus', function () {\n      expect(inflector.singularize('bleus')).toBe('bleu');\n      expect(inflector.singularize('émeus')).toBe('émeu');\n      expect(inflector.singularize('pneus')).toBe('pneu');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -eux","suites":["NounInflector",".singularize()"],"line":168,"updatePoint":{"line":168,"column":55,"index":8758},"code":"    it('should singularize regular nouns ending by -eux', function () {\n      expect(inflector.singularize('pieux')).toBe('pieu');\n      expect(inflector.singularize('lieux')).toBe('lieu');\n      expect(inflector.singularize('feux')).toBe('feu');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -eaux","suites":["NounInflector",".singularize()"],"line":173,"updatePoint":{"line":173,"column":56,"index":9014},"code":"    it('should singularize regular nouns ending by -eaux', function () {\n      expect(inflector.singularize('eaux')).toBe('eau');\n      expect(inflector.singularize('manteaux')).toBe('manteau');\n      expect(inflector.singularize('arbrisseaux')).toBe('arbrisseau');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -œux","suites":["NounInflector",".singularize()"],"line":178,"updatePoint":{"line":178,"column":55,"index":9287},"code":"    it('should singularize regular nouns ending by -œux', function () {\n      expect(inflector.singularize('vœux')).toBe('vœu');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -s, -x or -z","suites":["NounInflector",".singularize()"],"line":181,"updatePoint":{"line":181,"column":63,"index":9432},"code":"    it('should singularize regular nouns ending by -s, -x or -z', function () {\n      expect(inflector.singularize('cas')).toBe('cas');\n      expect(inflector.singularize('os')).toBe('os');\n      expect(inflector.singularize('rhinocéros')).toBe('rhinocéros');\n      expect(inflector.singularize('houx')).toBe('houx');\n      expect(inflector.singularize('lynx')).toBe('lynx');\n      expect(inflector.singularize('roux')).toBe('roux');\n      expect(inflector.singularize('gaz')).toBe('gaz');\n      expect(inflector.singularize('quartz')).toBe('quartz');\n      expect(inflector.singularize('quiz')).toBe('quiz');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize exception nouns","suites":["NounInflector",".singularize()"],"line":192,"updatePoint":{"line":192,"column":42,"index":10029},"code":"    it('should singularize exception nouns', function () {\n      expect(inflector.singularize('bestiaux')).toBe('bétail');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns","suites":["NounInflector",".singularize()"],"line":195,"updatePoint":{"line":195,"column":40,"index":10158},"code":"    it('should singularize regular nouns', function () {\n      expect(inflector.singularize('chais')).toBe('chai');\n      expect(inflector.singularize('vérités')).toBe('vérité');\n      expect(inflector.singularize('oranges')).toBe('orange');\n\n      // Exception nouns ending by -als\n      expect(inflector.singularize('carnavals')).toBe('carnaval');\n      expect(inflector.singularize('narvals')).toBe('narval');\n      expect(inflector.singularize('récitals')).toBe('récital');\n\n      // Regular nouns ending by -ails\n      expect(inflector.singularize('détails')).toBe('détail');\n      expect(inflector.singularize('poitrails')).toBe('poitrail');\n      expect(inflector.singularize('chandails')).toBe('chandail');\n\n      // Regular nouns ending by -ous\n      expect(inflector.singularize('trous')).toBe('trou');\n      expect(inflector.singularize('bambous')).toBe('bambou');\n      expect(inflector.singularize('toutous')).toBe('toutou');\n\n      // Exception nouns ending by -aus\n      expect(inflector.singularize('berimbaus')).toBe('berimbau');\n      expect(inflector.singularize('landaus')).toBe('landau');\n      expect(inflector.singularize('pilaus')).toBe('pilau');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns ending by -al","suites":["NounInflector","pluralize()"],"line":227,"updatePoint":{"line":227,"column":54,"index":11512},"code":"    it('should pluralize exception nouns ending by -al', function () {\n      expect(inflector.pluralize('carnavals')).toBe('carnavals');\n      expect(inflector.pluralize('narvals')).toBe('narvals');\n      expect(inflector.pluralize('récitals')).toBe('récitals');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -al","suites":["NounInflector","pluralize()"],"line":232,"updatePoint":{"line":232,"column":52,"index":11781},"code":"    it('should pluralize regular nouns ending by -al', function () {\n      expect(inflector.pluralize('amiraux')).toBe('amiraux');\n      expect(inflector.pluralize('chevaux')).toBe('chevaux');\n      expect(inflector.pluralize('généraux')).toBe('généraux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns ending by -ail","suites":["NounInflector","pluralize()"],"line":237,"updatePoint":{"line":237,"column":55,"index":12049},"code":"    it('should pluralize exception nouns ending by -ail', function () {\n      expect(inflector.pluralize('baux')).toBe('baux');\n      expect(inflector.pluralize('vitraux')).toBe('vitraux');\n      expect(inflector.pluralize('émaux')).toBe('émaux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -ail","suites":["NounInflector","pluralize()"],"line":242,"updatePoint":{"line":242,"column":53,"index":12303},"code":"    it('should pluralize regular nouns ending by -ail', function () {\n      expect(inflector.pluralize('détails')).toBe('détails');\n      expect(inflector.pluralize('poitrails')).toBe('poitrails');\n      expect(inflector.pluralize('chandails')).toBe('chandails');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns ending by -il","suites":["NounInflector","pluralize()"],"line":247,"updatePoint":{"line":247,"column":54,"index":12576},"code":"    it('should pluralize exception nouns ending by -il', function () {\n      expect(inflector.pluralize('cieux')).toBe('cieux');\n      expect(inflector.pluralize('yeux')).toBe('yeux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns ending by -ou","suites":["NounInflector","pluralize()"],"line":251,"updatePoint":{"line":251,"column":54,"index":12769},"code":"    it('should pluralize exception nouns ending by -ou', function () {\n      expect(inflector.pluralize('bijoux')).toBe('bijoux');\n      expect(inflector.pluralize('joujoux')).toBe('joujoux');\n      expect(inflector.pluralize('hiboux')).toBe('hiboux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -ou","suites":["NounInflector","pluralize()"],"line":256,"updatePoint":{"line":256,"column":52,"index":13028},"code":"    it('should pluralize regular nouns ending by -ou', function () {\n      expect(inflector.pluralize('trous')).toBe('trous');\n      expect(inflector.pluralize('bambous')).toBe('bambous');\n      expect(inflector.pluralize('toutous')).toBe('toutous');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns ending by -au","suites":["NounInflector","pluralize()"],"line":261,"updatePoint":{"line":261,"column":54,"index":13289},"code":"    it('should pluralize exception nouns ending by -au', function () {\n      expect(inflector.pluralize('berimbaus')).toBe('berimbaus');\n      expect(inflector.pluralize('landaus')).toBe('landaus');\n      expect(inflector.pluralize('pilaus')).toBe('pilaus');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -au","suites":["NounInflector","pluralize()"],"line":266,"updatePoint":{"line":266,"column":52,"index":13554},"code":"    it('should pluralize regular nouns ending by -au', function () {\n      expect(inflector.pluralize('cadeaux')).toBe('cadeaux');\n      expect(inflector.pluralize('beaux')).toBe('beaux');\n      expect(inflector.pluralize('étaux')).toBe('étaux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns ending by -eu","suites":["NounInflector","pluralize()"],"line":271,"updatePoint":{"line":271,"column":54,"index":13811},"code":"    it('should pluralize exception nouns ending by -eu', function () {\n      expect(inflector.pluralize('bleus')).toBe('bleus');\n      expect(inflector.pluralize('émeus')).toBe('émeus');\n      expect(inflector.pluralize('pneus')).toBe('pneus');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -eu","suites":["NounInflector","pluralize()"],"line":276,"updatePoint":{"line":276,"column":52,"index":14062},"code":"    it('should pluralize regular nouns ending by -eu', function () {\n      expect(inflector.pluralize('pieux')).toBe('pieux');\n      expect(inflector.pluralize('lieux')).toBe('lieux');\n      expect(inflector.pluralize('feux')).toBe('feux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -eau","suites":["NounInflector","pluralize()"],"line":281,"updatePoint":{"line":281,"column":53,"index":14312},"code":"    it('should pluralize regular nouns ending by -eau', function () {\n      expect(inflector.pluralize('eaux')).toBe('eaux');\n      expect(inflector.pluralize('manteaux')).toBe('manteaux');\n      expect(inflector.pluralize('arbrisseaux')).toBe('arbrisseaux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns ending by -œu","suites":["NounInflector","pluralize()"],"line":286,"updatePoint":{"line":286,"column":52,"index":14579},"code":"    it('should pluralize regular nouns ending by -œu', function () {\n      expect(inflector.pluralize('vœux')).toBe('vœux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize exception nouns","suites":["NounInflector","pluralize()"],"line":304,"updatePoint":{"line":304,"column":40,"index":15306},"code":"    it('should pluralize exception nouns', function () {\n      expect(inflector.pluralize('aulx')).toBe('aulx');\n      expect(inflector.pluralize('bestiaux')).toBe('bestiaux');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular nouns","suites":["NounInflector","pluralize()"],"line":308,"updatePoint":{"line":308,"column":38,"index":15489},"code":"    it('should pluralize regular nouns', function () {\n      expect(inflector.pluralize('chais')).toBe('chais');\n      expect(inflector.pluralize('vérités')).toBe('vérités');\n      expect(inflector.pluralize('oranges')).toBe('oranges');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -aux","suites":["NounInflector",".singularize()"],"line":315,"updatePoint":{"line":315,"column":55,"index":15800},"code":"    it('should singularize regular nouns ending by -aux', function () {\n      expect(inflector.singularize('amiral')).toBe('amiral');\n      expect(inflector.singularize('cheval')).toBe('cheval');\n      expect(inflector.singularize('général')).toBe('général');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize exception nouns ending by -aux","suites":["NounInflector",".singularize()"],"line":320,"updatePoint":{"line":320,"column":57,"index":16070},"code":"    it('should singularize exception nouns ending by -aux', function () {\n      expect(inflector.singularize('bail')).toBe('bail');\n      expect(inflector.singularize('vitrail')).toBe('vitrail');\n      expect(inflector.singularize('émail')).toBe('émail');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -aux","suites":["NounInflector",".singularize()"],"line":325,"updatePoint":{"line":325,"column":55,"index":16332},"code":"    it('should singularize regular nouns ending by -aux', function () {\n      expect(inflector.singularize('cadeau')).toBe('cadeau');\n      expect(inflector.singularize('beau')).toBe('beau');\n      expect(inflector.singularize('étau')).toBe('étau');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize nouns with multiple plural forms","suites":["NounInflector",".singularize()"],"line":330,"updatePoint":{"line":330,"column":59,"index":16594},"code":"    it('should singularize nouns with multiple plural forms', function () {\n      expect(inflector.singularize('ail')).toBe('ail');\n      expect(inflector.singularize('ciel')).toBe('ciel');\n      expect(inflector.singularize('œil')).toBe('œil');\n      expect(inflector.singularize('aïeul')).toBe('aïeul');\n      expect(inflector.singularize('bisaïeul')).toBe('bisaïeul');\n      expect(inflector.singularize('crau')).toBe('crau');\n      expect(inflector.singularize('grau')).toBe('grau');\n      expect(inflector.singularize('sénau')).toBe('sénau');\n      expect(inflector.singularize('tussau')).toBe('tussau');\n      expect(inflector.singularize('emposieu')).toBe('emposieu');\n      expect(inflector.singularize('richelieu')).toBe('richelieu');\n      expect(inflector.singularize('feu')).toBe('feu');\n      expect(inflector.singularize('lieu')).toBe('lieu'); // Fish\n      expect(inflector.singularize('corail')).toBe('corail');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize exception nouns ending by -oux","suites":["NounInflector",".singularize()"],"line":346,"updatePoint":{"line":346,"column":57,"index":17528},"code":"    it('should singularize exception nouns ending by -oux', function () {\n      expect(inflector.singularize('bijou')).toBe('bijou');\n      expect(inflector.singularize('joujou')).toBe('joujou');\n      expect(inflector.singularize('hibou')).toBe('hibou');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize exception nouns ending by -eus","suites":["NounInflector",".singularize()"],"line":351,"updatePoint":{"line":351,"column":57,"index":17792},"code":"    it('should singularize exception nouns ending by -eus', function () {\n      expect(inflector.singularize('bleu')).toBe('bleu');\n      expect(inflector.singularize('émeu')).toBe('émeu');\n      expect(inflector.singularize('pneu')).toBe('pneu');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -eux","suites":["NounInflector",".singularize()"],"line":356,"updatePoint":{"line":356,"column":55,"index":18046},"code":"    it('should singularize regular nouns ending by -eux', function () {\n      expect(inflector.singularize('pieu')).toBe('pieu');\n      expect(inflector.singularize('lieu')).toBe('lieu');\n      expect(inflector.singularize('feu')).toBe('feu');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -eaux","suites":["NounInflector",".singularize()"],"line":361,"updatePoint":{"line":361,"column":56,"index":18299},"code":"    it('should singularize regular nouns ending by -eaux', function () {\n      expect(inflector.singularize('eau')).toBe('eau');\n      expect(inflector.singularize('manteau')).toBe('manteau');\n      expect(inflector.singularize('arbrisseau')).toBe('arbrisseau');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -œux","suites":["NounInflector",".singularize()"],"line":366,"updatePoint":{"line":366,"column":55,"index":18569},"code":"    it('should singularize regular nouns ending by -œux', function () {\n      expect(inflector.singularize('vœu')).toBe('vœu');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize exception nouns","suites":["NounInflector",".singularize()"],"line":384,"updatePoint":{"line":384,"column":42,"index":19318},"code":"    it('should singularize exception nouns', function () {\n      expect(inflector.singularize('bétail')).toBe('bétail');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns","suites":["NounInflector",".singularize()"],"line":387,"updatePoint":{"line":387,"column":40,"index":19445},"code":"    it('should singularize regular nouns', function () {\n      expect(inflector.singularize('chai')).toBe('chai');\n      expect(inflector.singularize('vérité')).toBe('vérité');\n      expect(inflector.singularize('orange')).toBe('orange');\n\n      // Exception nouns ending by -als\n      expect(inflector.singularize('carnaval')).toBe('carnaval');\n      expect(inflector.singularize('narval')).toBe('narval');\n      expect(inflector.singularize('récital')).toBe('récital');\n\n      // Regular nouns ending by -ails\n      expect(inflector.singularize('détail')).toBe('détail');\n      expect(inflector.singularize('poitrail')).toBe('poitrail');\n      expect(inflector.singularize('chandail')).toBe('chandail');\n\n      // Regular nouns ending by -ous\n      expect(inflector.singularize('trou')).toBe('trou');\n      expect(inflector.singularize('bambou')).toBe('bambou');\n      expect(inflector.singularize('toutou')).toBe('toutou');\n\n      // Exception nouns ending by -aus\n      expect(inflector.singularize('berimbau')).toBe('berimbau');\n      expect(inflector.singularize('landau')).toBe('landau');\n      expect(inflector.singularize('pilau')).toBe('pilau');\n    });","file":"noun_inflector_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize nouns","suites":["NounInflector",".pluralize()"],"line":29,"updatePoint":{"line":29,"column":30,"index":1312},"code":"    it('should pluralize nouns', function () {\n      expect(inflector.pluralize('ひと')).toBe('ひとたち');\n      expect(inflector.pluralize('わたし')).toBe('わたしたち');\n      expect(inflector.pluralize('私')).toBe('私たち');\n    });","file":"noun_inflector_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should not pluralize exceptions","suites":["NounInflector",".pluralize()"],"line":34,"updatePoint":{"line":34,"column":39,"index":1538},"code":"    it('should not pluralize exceptions', function () {\n      expect(inflector.pluralize('ともだち')).toBe('ともだち');\n      expect(inflector.pluralize('友だち')).toBe('友だち');\n      expect(inflector.pluralize('友達')).toBe('友達');\n    });","file":"noun_inflector_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize archaic forms","suites":["NounInflector",".pluralize()"],"line":39,"updatePoint":{"line":39,"column":38,"index":1763},"code":"    it('should pluralize archaic forms', function () {\n      expect(inflector.pluralize('神')).toBe('神神');\n      expect(inflector.pluralize('人')).toBe('人人');\n      expect(inflector.pluralize('我')).toBe('我我');\n    });","file":"noun_inflector_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -tachi in Hiragana","suites":["NounInflector",".singularize()"],"line":46,"updatePoint":{"line":46,"column":69,"index":2059},"code":"    it('should singularize regular nouns ending by -tachi in Hiragana', function () {\n      expect(inflector.singularize('わたしたち')).toBe('わたし');\n      expect(inflector.singularize('人たち')).toBe('人');\n      expect(inflector.singularize('りかたち')).toBe('りか');\n    });","file":"noun_inflector_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should not singularize exception nouns ending by -tachi in Hiragana","suites":["NounInflector",".singularize()"],"line":51,"updatePoint":{"line":51,"column":75,"index":2327},"code":"    it('should not singularize exception nouns ending by -tachi in Hiragana', function () {\n      expect(inflector.singularize('ついたち')).toBe('ついたち');\n      expect(inflector.singularize('かたち')).toBe('かたち');\n      expect(inflector.singularize('はたち')).toBe('はたち');\n    });","file":"noun_inflector_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -tachi in Kanji","suites":["NounInflector",".singularize()"],"line":56,"updatePoint":{"line":56,"column":66,"index":2588},"code":"    it('should singularize regular nouns ending by -tachi in Kanji', function () {\n      expect(inflector.singularize('わたし達')).toBe('わたし');\n      expect(inflector.singularize('人達')).toBe('人');\n      expect(inflector.singularize('日伊達')).toBe('日伊');\n    });","file":"noun_inflector_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should not singularize exception nouns ending by -tachi in Kanji","suites":["NounInflector",".singularize()"],"line":61,"updatePoint":{"line":61,"column":72,"index":2850},"code":"    it('should not singularize exception nouns ending by -tachi in Kanji', function () {\n      expect(inflector.singularize('上達')).toBe('上達');\n      expect(inflector.singularize('配達')).toBe('配達');\n      expect(inflector.singularize('発達')).toBe('発達');\n    });","file":"noun_inflector_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -ra in Kanji","suites":["NounInflector",".singularize()"],"line":66,"updatePoint":{"line":66,"column":63,"index":3100},"code":"    it('should singularize regular nouns ending by -ra in Kanji', function () {\n      expect(inflector.singularize('僕等')).toBe('僕');\n      expect(inflector.singularize('貴様等')).toBe('貴様');\n      expect(inflector.singularize('圭一等')).toBe('圭一');\n    });","file":"noun_inflector_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should not singularize exception nouns ending by -ra in Kanji","suites":["NounInflector",".singularize()"],"line":71,"updatePoint":{"line":71,"column":69,"index":3357},"code":"    it('should not singularize exception nouns ending by -ra in Kanji', function () {\n      expect(inflector.singularize('下等')).toBe('下等');\n      expect(inflector.singularize('初等')).toBe('初等');\n      expect(inflector.singularize('一等')).toBe('一等');\n    });","file":"noun_inflector_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -gata","suites":["NounInflector",".singularize()"],"line":76,"updatePoint":{"line":76,"column":56,"index":3600},"code":"    it('should singularize regular nouns ending by -gata', function () {\n      expect(inflector.singularize('神様方')).toBe('神様');\n      expect(inflector.singularize('先生方')).toBe('先生');\n      expect(inflector.singularize('あなたがた')).toBe('あなた');\n    });","file":"noun_inflector_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular nouns ending by -domo","suites":["NounInflector",".singularize()"],"line":81,"updatePoint":{"line":81,"column":56,"index":3849},"code":"    it('should singularize regular nouns ending by -domo', function () {\n      expect(inflector.singularize('人間共')).toBe('人間');\n      expect(inflector.singularize('野郎共')).toBe('野郎');\n      expect(inflector.singularize('ガキども')).toBe('ガキ');\n    });","file":"noun_inflector_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize archaic forms","suites":["NounInflector",".singularize()"],"line":86,"updatePoint":{"line":86,"column":38,"index":4078},"code":"    it('should pluralize archaic forms', function () {\n      expect(inflector.singularize('神神')).toBe('神');\n      expect(inflector.singularize('人人')).toBe('人');\n      expect(inflector.singularize('我我')).toBe('我');\n    });","file":"noun_inflector_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should drop an S by default","suites":["noun inflector","singularization"],"line":29,"updatePoint":{"line":29,"column":35,"index":1313},"code":"    it('should drop an S by default', function () {\n      expect(inflector.singularize('rrrs')).toBe('rrr');\n      expect(inflector.singularize('hackers')).toBe('hacker');\n      expect(inflector.singularize('movies')).toBe('movie');\n\n      // MAN cases that don't pluralize to MEN\n      expect(inflector.singularize('talismans')).toBe('talisman');\n      expect(inflector.singularize('humans')).toBe('human');\n      expect(inflector.singularize('prehumans')).toBe('prehuman');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle ambiguous form","suites":["noun inflector","singularization"],"line":39,"updatePoint":{"line":39,"column":36,"index":1798},"code":"    it('should handle ambiguous form', function () {\n      expect(inflector.singularize('deer')).toBe('deer');\n      expect(inflector.singularize('fish')).toBe('fish');\n      expect(inflector.singularize('series')).toBe('series');\n      expect(inflector.singularize('sheep')).toBe('sheep');\n      expect(inflector.singularize('trout')).toBe('trout');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should convert plurals ending SES to S","suites":["noun inflector","singularization"],"line":46,"updatePoint":{"line":46,"column":46,"index":2167},"code":"    it('should convert plurals ending SES to S', function () {\n      expect(inflector.singularize('statuses')).toBe('status');\n      expect(inflector.singularize('buses')).toBe('bus');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should match irregulars","suites":["noun inflector","singularization"],"line":50,"updatePoint":{"line":50,"column":31,"index":2345},"code":"    it('should match irregulars', function () {\n      expect(inflector.singularize('people')).toBe('person');\n      expect(inflector.singularize('children')).toBe('child');\n      expect(inflector.singularize('oxen')).toBe('ox');\n      expect(inflector.singularize('clothes')).toBe('cloth');\n      expect(inflector.singularize('heroes')).toBe('hero');\n      expect(inflector.singularize('torsi')).toBe('torso');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle IX cases","suites":["noun inflector","singularization"],"line":58,"updatePoint":{"line":58,"column":30,"index":2763},"code":"    it('should handle IX cases', function () {\n      expect(inflector.singularize('matrices')).toBe('matrix');\n      expect(inflector.singularize('indices')).toBe('index');\n      expect(inflector.singularize('cortices')).toBe('cortex');\n\n      // our pluralizer won''t cause this form of appendix (appendicies)\n      // but we should handle it\n      expect(inflector.singularize('appendices')).toBe('appendix');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should regulars to ES","suites":["noun inflector","singularization"],"line":67,"updatePoint":{"line":67,"column":29,"index":3182},"code":"    it('should regulars to ES', function () {\n      expect(inflector.singularize('churches')).toBe('church');\n      expect(inflector.singularize('appendixes')).toBe('appendix');\n      expect(inflector.singularize('messes')).toBe('mess');\n      expect(inflector.singularize('quizes')).toBe('quiz');\n      expect(inflector.singularize('shoes')).toBe('shoe');\n      expect(inflector.singularize('funguses')).toBe('fungus');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle SIS cases","suites":["noun inflector","singularization"],"line":75,"updatePoint":{"line":75,"column":31,"index":3613},"code":"    it('should handle SIS cases', function () {\n      expect(inflector.singularize('synopses')).toBe('synopsis');\n      expect(inflector.singularize('parentheses')).toBe('parenthesis');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle special OES cases","suites":["noun inflector","singularization"],"line":79,"updatePoint":{"line":79,"column":39,"index":3815},"code":"    it('should handle special OES cases', function () {\n      expect(inflector.singularize('tomatoes')).toBe('tomato');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle I cases","suites":["noun inflector","singularization"],"line":82,"updatePoint":{"line":82,"column":29,"index":3933},"code":"    it('should handle I cases', function () {\n      expect(inflector.singularize('octopi')).toBe('octopus');\n      expect(inflector.singularize('stimuli')).toBe('stimulus');\n      expect(inflector.singularize('radii')).toBe('radius');\n      expect(inflector.singularize('nuclei')).toBe('nucleus');\n      expect(inflector.singularize('fungi')).toBe('fungus');\n      expect(inflector.singularize('cacti')).toBe('cactus');\n      expect(inflector.singularize('alumni')).toBe('alumnus');\n      expect(inflector.singularize('calculi')).toBe('calculus');\n      expect(inflector.singularize('hippopotami')).toBe('hippopotamus');\n      expect(inflector.singularize('macrofungi')).toBe('macrofungus');\n      expect(inflector.singularize('phoeti')).toBe('phoetus');\n      expect(inflector.singularize('syllabi')).toBe('syllabus');\n      expect(inflector.singularize('trophi')).toBe('trophus');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle IVES cases","suites":["noun inflector","singularization"],"line":97,"updatePoint":{"line":97,"column":32,"index":4827},"code":"    it('should handle IVES cases', function () {\n      expect(inflector.singularize('lives')).toBe('life');\n      expect(inflector.singularize('knives')).toBe('knife');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle Y cases","suites":["noun inflector","singularization"],"line":101,"updatePoint":{"line":101,"column":29,"index":5001},"code":"    it('should handle Y cases', function () {\n      expect(inflector.singularize('parties')).toBe('party');\n      expect(inflector.singularize('flies')).toBe('fly');\n      expect(inflector.singularize('victories')).toBe('victory');\n      expect(inflector.singularize('monstrosities')).toBe('monstrosity');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle SS cases","suites":["noun inflector","singularization"],"line":107,"updatePoint":{"line":107,"column":30,"index":5316},"code":"    it('should handle SS cases', function () {\n      expect(inflector.singularize('dresses')).toBe('dress');\n      expect(inflector.singularize('dress')).toBe('dress');\n      expect(inflector.singularize('messes')).toBe('mess');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle MAN->MAN cases","suites":["noun inflector","singularization"],"line":112,"updatePoint":{"line":112,"column":36,"index":5559},"code":"    it('should handle MAN->MAN cases', function () {\n      expect(inflector.singularize('men')).toBe('man');\n      expect(inflector.singularize('women')).toBe('woman');\n      expect(inflector.singularize('workmen')).toBe('workman');\n      expect(inflector.singularize('riflemen')).toBe('rifleman');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle irregular cases","suites":["noun inflector","singularization"],"line":118,"updatePoint":{"line":118,"column":37,"index":5867},"code":"    it('should handle irregular cases', function () {\n      expect(inflector.singularize('feet')).toBe('foot');\n      expect(inflector.singularize('geese')).toBe('goose');\n      expect(inflector.singularize('teeth')).toBe('tooth');\n      expect(inflector.singularize('ephemerides')).toBe('ephemeris');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle AE cases","suites":["noun inflector","singularization"],"line":124,"updatePoint":{"line":124,"column":30,"index":6170},"code":"    it('should handle AE cases', function () {\n      expect(inflector.singularize('antennae')).toBe('antenna');\n      expect(inflector.singularize('formulae')).toBe('formula');\n      expect(inflector.singularize('nebulae')).toBe('nebula');\n      expect(inflector.singularize('vertebrae')).toBe('vertebra');\n      expect(inflector.singularize('vitae')).toBe('vita');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should allow AE cases to be S","suites":["noun inflector","singularization"],"line":131,"updatePoint":{"line":131,"column":37,"index":6551},"code":"    it('should allow AE cases to be S', function () {\n      expect(inflector.singularize('antennas')).toBe('antenna');\n      expect(inflector.singularize('formulas')).toBe('formula');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should append an S by default","suites":["noun inflector","pluralization"],"line":137,"updatePoint":{"line":137,"column":37,"index":6791},"code":"    it('should append an S by default', function () {\n      expect(inflector.pluralize('rrr')).toBe('rrrs');\n      expect(inflector.pluralize('hacker')).toBe('hackers');\n      expect(inflector.pluralize('movie')).toBe('movies');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle ambiguous form","suites":["noun inflector","pluralization"],"line":142,"updatePoint":{"line":142,"column":36,"index":7027},"code":"    it('should handle ambiguous form', function () {\n      expect(inflector.pluralize('deer')).toBe('deer');\n      expect(inflector.pluralize('fish')).toBe('fish');\n      expect(inflector.pluralize('series')).toBe('series');\n      expect(inflector.pluralize('sheep')).toBe('sheep');\n      expect(inflector.pluralize('trout')).toBe('trout');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should convert singulars ending s to ses","suites":["noun inflector","pluralization"],"line":149,"updatePoint":{"line":149,"column":48,"index":7388},"code":"    it('should convert singulars ending s to ses', function () {\n      expect(inflector.pluralize('status')).toBe('statuses');\n      expect(inflector.pluralize('bus')).toBe('buses');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should match irregulars","suites":["noun inflector","pluralization"],"line":153,"updatePoint":{"line":153,"column":31,"index":7562},"code":"    it('should match irregulars', function () {\n      expect(inflector.pluralize('person')).toBe('people');\n      expect(inflector.pluralize('child')).toBe('children');\n      expect(inflector.pluralize('ox')).toBe('oxen');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should maintain case of irregulars","suites":["noun inflector","pluralization"],"line":158,"updatePoint":{"line":158,"column":42,"index":7804},"code":"    it('should maintain case of irregulars', function () {\n      expect(inflector.pluralize('OX')).toBe('OXEN');\n      expect(inflector.pluralize('Person')).toBe('People');\n      expect(inflector.pluralize('child')).toBe('children');\n      expect(inflector.pluralize('cloth')).toBe('clothes');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle IX cases","suites":["noun inflector","pluralization"],"line":164,"updatePoint":{"line":164,"column":30,"index":8094},"code":"    it('should handle IX cases', function () {\n      expect(inflector.pluralize('matrix')).toBe('matrices');\n      expect(inflector.pluralize('index')).toBe('indices');\n      expect(inflector.pluralize('cortex')).toBe('cortices');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should regulars to ES","suites":["noun inflector","pluralization"],"line":169,"updatePoint":{"line":169,"column":29,"index":8332},"code":"    it('should regulars to ES', function () {\n      expect(inflector.pluralize('church')).toBe('churches');\n      expect(inflector.pluralize('appendix')).toBe('appendixes');\n      expect(inflector.pluralize('mess')).toBe('messes');\n      expect(inflector.pluralize('quiz')).toBe('quizes');\n      expect(inflector.pluralize('shoe')).toBe('shoes');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle SIS cases","suites":["noun inflector","pluralization"],"line":176,"updatePoint":{"line":176,"column":31,"index":8689},"code":"    it('should handle SIS cases', function () {\n      expect(inflector.pluralize('synopsis')).toBe('synopses');\n      expect(inflector.pluralize('parenthesis')).toBe('parentheses');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle special OES cases","suites":["noun inflector","pluralization"],"line":180,"updatePoint":{"line":180,"column":39,"index":8887},"code":"    it('should handle special OES cases', function () {\n      expect(inflector.pluralize('tomato')).toBe('tomatoes');\n      expect(inflector.pluralize('buffalo')).toBe('buffaloes');\n      expect(inflector.pluralize('tornado')).toBe('tornadoes');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle I cases","suites":["noun inflector","pluralization"],"line":185,"updatePoint":{"line":185,"column":29,"index":9131},"code":"    it('should handle I cases', function () {\n      expect(inflector.pluralize('radius')).toBe('radii');\n      expect(inflector.pluralize('octopus')).toBe('octopi');\n      expect(inflector.pluralize('stimulus')).toBe('stimuli');\n      expect(inflector.pluralize('nucleus')).toBe('nuclei');\n      expect(inflector.pluralize('fungus')).toBe('fungi');\n      expect(inflector.pluralize('cactus')).toBe('cacti');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle IVES cases","suites":["noun inflector","pluralization"],"line":193,"updatePoint":{"line":193,"column":32,"index":9550},"code":"    it('should handle IVES cases', function () {\n      expect(inflector.pluralize('knife')).toBe('knives');\n      expect(inflector.pluralize('life')).toBe('lives');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle Y cases","suites":["noun inflector","pluralization"],"line":197,"updatePoint":{"line":197,"column":29,"index":9720},"code":"    it('should handle Y cases', function () {\n      expect(inflector.pluralize('party')).toBe('parties');\n      expect(inflector.pluralize('fly')).toBe('flies');\n      expect(inflector.pluralize('victory')).toBe('victories');\n      expect(inflector.pluralize('monstrosity')).toBe('monstrosities');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle [aeiou]Y cases","suites":["noun inflector","pluralization"],"line":203,"updatePoint":{"line":203,"column":36,"index":10033},"code":"    it('should handle [aeiou]Y cases', function () {\n      expect(inflector.pluralize('day')).toBe('days');\n      expect(inflector.pluralize('toy')).toBe('toys');\n      expect(inflector.pluralize('journey')).toBe('journeys');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle SS cases","suites":["noun inflector","pluralization"],"line":208,"updatePoint":{"line":208,"column":30,"index":10261},"code":"    it('should handle SS cases', function () {\n      expect(inflector.pluralize('dress')).toBe('dresses');\n      expect(inflector.pluralize('dresses')).toBe('dresses');\n      expect(inflector.pluralize('mess')).toBe('messes');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle MAN->MEN cases","suites":["noun inflector","pluralization"],"line":213,"updatePoint":{"line":213,"column":36,"index":10502},"code":"    it('should handle MAN->MEN cases', function () {\n      expect(inflector.pluralize('man')).toBe('men');\n      expect(inflector.pluralize('woman')).toBe('women');\n      expect(inflector.pluralize('workman')).toBe('workmen');\n      expect(inflector.pluralize('rifleman')).toBe('riflemen');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle irregular cases","suites":["noun inflector","pluralization"],"line":219,"updatePoint":{"line":219,"column":37,"index":10802},"code":"    it('should handle irregular cases', function () {\n      expect(inflector.pluralize('foot')).toBe('feet');\n      expect(inflector.pluralize('goose')).toBe('geese');\n      expect(inflector.pluralize('tooth')).toBe('teeth');\n      expect(inflector.pluralize('ephemeris')).toBe('ephemerides');\n\n      // MAN cases that don't pluralize to MEN\n      expect(inflector.pluralize('talisman')).toBe('talismans');\n      expect(inflector.pluralize('human')).toBe('humans');\n      expect(inflector.pluralize('prehuman')).toBe('prehumans');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle AE cases","suites":["noun inflector","pluralization"],"line":230,"updatePoint":{"line":230,"column":30,"index":11334},"code":"    it('should handle AE cases', function () {\n      expect(inflector.pluralize('antenna')).toBe('antennae');\n      expect(inflector.pluralize('formula')).toBe('formulae');\n      expect(inflector.pluralize('nebula')).toBe('nebulae');\n      expect(inflector.pluralize('vertebra')).toBe('vertebrae');\n      expect(inflector.pluralize('vita')).toBe('vitae');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize and singularize string from patch","suites":["noun inflector","pluralization"],"line":238,"updatePoint":{"line":238,"column":56,"index":11730},"code":"  it('should pluralize and singularize string from patch', function () {\n    expect(inflector.pluralize('synopsis')).toBe('synopses');\n    expect(inflector.singularize('synopses')).toBe('synopsis');\n    expect(inflector.pluralize('mess')).toBe('messes');\n    expect(inflector.singularize('messes')).toBe('mess');\n  });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize and singularize custom forms","suites":["noun inflector","custom inflections"],"line":245,"updatePoint":{"line":245,"column":53,"index":12093},"code":"    it('should pluralize and singularize custom forms', function () {\n      const myInflector = new NounInflector();\n      myInflector.addPlural(/(code|ware)/i, '$1z');\n      myInflector.addSingular(/(code|ware)z/i, '$1');\n      expect(myInflector.pluralize('code')).toBe('codez');\n      expect(myInflector.pluralize('ware')).toBe('warez');\n      expect(myInflector.singularize('codez')).toBe('code');\n      expect(myInflector.singularize('warez')).toBe('ware');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should not break regular forms","suites":["noun inflector","custom inflections"],"line":254,"updatePoint":{"line":254,"column":38,"index":12549},"code":"    it('should not break regular forms', function () {\n      const myInflector = new NounInflector();\n      myInflector.addPlural(/(code|ware)/i, '$1z');\n      myInflector.addSingular(/(code|ware)z/i, '$1');\n      expect(myInflector.pluralize('bus')).toBe('buses');\n      expect(myInflector.singularize('buses')).toBe('bus');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle words ending in ff and ffe","suites":["noun inflector","should handle words ending in f, ff, ffe"],"line":263,"updatePoint":{"line":263,"column":48,"index":12968},"code":"    it('should handle words ending in ff and ffe', function () {\n      const myInflector = new NounInflector();\n      expect(myInflector.pluralize('sherriff')).toBe('sherriffs');\n      expect(myInflector.pluralize('giraffe')).toBe('giraffes');\n      expect(myInflector.singularize('sherriffs')).toBe('sherriff');\n      expect(myInflector.singularize('giraffes')).toBe('giraffe');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle words ending in f","suites":["noun inflector","should handle words ending in f, ff, ffe"],"line":270,"updatePoint":{"line":270,"column":39,"index":13347},"code":"    it('should handle words ending in f', function () {\n      const myInflector = new NounInflector();\n      expect(myInflector.pluralize('roof')).toBe('roofs');\n      expect(myInflector.pluralize('chief')).toBe('chiefs');\n      expect(myInflector.pluralize('oaf')).toBe('oafs');\n      expect(myInflector.singularize('roofs')).toBe('roof');\n      expect(myInflector.singularize('chiefs')).toBe('chief');\n      expect(myInflector.singularize('oafs')).toBe('oaf');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle words ending in f or fe","suites":["noun inflector","should handle words ending in f, ff, ffe"],"line":279,"updatePoint":{"line":279,"column":45,"index":13824},"code":"    it('should handle words ending in f or fe', function () {\n      const myInflector = new NounInflector();\n      expect(myInflector.pluralize('leaf')).toBe('leaves');\n      expect(myInflector.pluralize('wolf')).toBe('wolves');\n      expect(myInflector.pluralize('calf')).toBe('calves');\n      expect(myInflector.pluralize('half')).toBe('halves');\n      expect(myInflector.pluralize('knife')).toBe('knives');\n      expect(myInflector.pluralize('loaf')).toBe('loaves');\n      expect(myInflector.pluralize('life')).toBe('lives');\n      expect(myInflector.pluralize('wife')).toBe('wives');\n      expect(myInflector.pluralize('shelf')).toBe('shelves');\n      expect(myInflector.pluralize('thief')).toBe('thieves');\n      expect(myInflector.pluralize('yourself')).toBe('yourselves');\n      expect(myInflector.singularize('leaves')).toBe('leaf');\n      expect(myInflector.singularize('wolves')).toBe('wolf');\n      expect(myInflector.singularize('calves')).toBe('calf');\n      expect(myInflector.singularize('halves')).toBe('half');\n      expect(myInflector.singularize('knives')).toBe('knife');\n      expect(myInflector.singularize('loaves')).toBe('loaf');\n      expect(myInflector.singularize('lives')).toBe('life');\n      expect(myInflector.singularize('wives')).toBe('wife');\n      expect(myInflector.singularize('shelves')).toBe('shelf');\n      expect(myInflector.singularize('thieves')).toBe('thief');\n      expect(myInflector.singularize('yourselves')).toBe('yourself');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle words ending in rf","suites":["noun inflector","should handle words ending in f, ff, ffe"],"line":304,"updatePoint":{"line":304,"column":40,"index":15300},"code":"    it('should handle words ending in rf', function () {\n      const myInflector = new NounInflector();\n      expect(myInflector.pluralize('scarf')).toBe('scarfs');\n      expect(myInflector.pluralize('dwarf')).toBe('dwarfs');\n      expect(myInflector.pluralize('handkerchief')).toBe('handkerchiefs');\n      expect(myInflector.pluralize('wharf')).toBe('wharfs');\n      expect(myInflector.singularize('scarfs')).toBe('scarf');\n      expect(myInflector.singularize('dwarfs')).toBe('dwarf');\n      expect(myInflector.singularize('wharfs')).toBe('wharf');\n      expect(myInflector.singularize('handkerchiefs')).toBe('handkerchief');\n      expect(myInflector.singularize('scarves')).toBe('scarf');\n      expect(myInflector.singularize('dwarves')).toBe('dwarf');\n      expect(myInflector.singularize('wharves')).toBe('wharf');\n      expect(myInflector.singularize('handkerchieves')).toBe('handkerchief');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle words ending in rf","suites":["noun inflector","should handle words ending in f, ff, ffe"],"line":319,"updatePoint":{"line":319,"column":40,"index":16206},"code":"    it('should handle words ending in rf', function () {\n      const myInflector = new NounInflector();\n      expect(myInflector.singularize('expenses')).toBe('expense');\n      expect(myInflector.singularize('defenses')).toBe('defense');\n      expect(myInflector.pluralize('expense')).toBe('expenses');\n      expect(myInflector.pluralize('defense')).toBe('defenses');\n    });","file":"noun_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should perform stemming on a lot of words","suites":["porter_stemmer_es"],"line":28,"updatePoint":{"line":28,"column":47,"index":1365},"code":"  it('should perform stemming on a lot of words', function () {\n    const errors = [];\n    Object.keys(snowBallDict).forEach(word => {\n      const stemmed = PorterStemmer.stem(word);\n      const expectedStem = snowBallDict[word];\n      if (stemmed !== snowBallDict[word]) {\n        console.log('Error:', word, 'Expected:', expectedStem, 'Got:', stemmed);\n        errors.push({\n          word: word,\n          expected: expectedStem,\n          actual: stemmed\n        });\n      }\n    });\n    expect(errors.length).toBe(0);\n  });","file":"porter_stemmer_es_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize a piece of text","suites":["porter_stemmer_es"],"line":44,"updatePoint":{"line":44,"column":37,"index":1883},"code":"  it('should tokenize a piece of text', function () {\n    expect(PorterStemmer.tokenizeAndStem('Pues, en efecto, es solo el plano y lo que muestra aquello que interesa a Zemeckis.', true)).toEqual(['pues', 'en', 'efect', 'es', 'sol', 'el', 'plan', 'y', 'lo', 'que', 'muestr', 'aquell', 'que', 'interes', 'a', 'zemeckis']);\n  });","file":"porter_stemmer_es_spec.js","skipped":false,"dir":"spec"},{"name":"should prelude","suites":["porter_stemmer"],"line":28,"updatePoint":{"line":28,"column":20,"index":1296},"code":"  it('should prelude', function () {\n    expect(stemmer.prelude('JOUER')).toBe('joUer');\n    expect(stemmer.prelude('ennuie')).toBe('ennuIe');\n    expect(stemmer.prelude('yeux')).toBe('Yeux');\n    expect(stemmer.prelude('quand')).toBe('qUand');\n  });","file":"porter_stemmer_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should compute regions","suites":["porter_stemmer"],"line":34,"updatePoint":{"line":34,"column":28,"index":1555},"code":"  it('should compute regions', function () {\n    expect(stemmer.regions('fameusement').r1).toBe(3);\n    expect(stemmer.regions('fameusement').r2).toBe(6);\n    expect(stemmer.regions('taii').r1).toBe(4);\n    expect(stemmer.regions('taii').r2).toBe(4);\n    expect(stemmer.regions('parade').rv).toBe(3);\n    expect(stemmer.regions('colet').rv).toBe(3);\n    expect(stemmer.regions('tapis').rv).toBe(3);\n    expect(stemmer.regions('aimer').rv).toBe(3);\n    expect(stemmer.regions('adorer').rv).toBe(3);\n    expect(stemmer.regions('voler').rv).toBe(2);\n    expect(stemmer.regions('tue').rv).toBe(2);\n  });","file":"porter_stemmer_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should compute longest suffix ends in Arr","suites":["porter_stemmer"],"line":47,"updatePoint":{"line":47,"column":47,"index":2174},"code":"  it('should compute longest suffix ends in Arr', function () {\n    expect(stemmer.endsinArr('voudriez', ['ez', 'iez', 'z'])).toBe('iez');\n  });","file":"porter_stemmer_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should stem some word","suites":["porter_stemmer"],"line":50,"updatePoint":{"line":50,"column":27,"index":2299},"code":"  it('should stem some word', function () {\n    expect(stemmer.stem('volera')).toBe('vol');\n    expect(stemmer.stem('volerait')).toBe('vol');\n    expect(stemmer.stem('subitement')).toBe('subit');\n    expect(stemmer.stem('tempérament')).toBe('temper');\n    expect(stemmer.stem('voudriez')).toBe('voudr');\n    expect(stemmer.stem('vengeait')).toBe('veng');\n    expect(stemmer.stem('saisissement')).toBe('sais');\n    expect(stemmer.stem('transatlantique')).toBe('transatlant');\n    expect(stemmer.stem('premièrement')).toBe('premi');\n    expect(stemmer.stem('instruments')).toBe('instrument');\n    expect(stemmer.stem('trouverions')).toBe('trouv');\n    expect(stemmer.stem('voyiez')).toBe('voi');\n    expect(stemmer.stem('publicité')).toBe('publiqu');\n    expect(stemmer.stem('pitoyable')).toBe('pitoi');\n  });","file":"porter_stemmer_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should perform stemming on a lot of words","suites":["porter_stemmer"],"line":66,"updatePoint":{"line":66,"column":47,"index":3127},"code":"  it('should perform stemming on a lot of words', function () {\n    const ok = [];\n    const ko = [];\n    Object.keys(snowBallDict).forEach(word => {\n      const stemmed = stemmer.stem(word);\n      const expectedStem = snowBallDict[word];\n      const regs = stemmer.regions(word);\n      const txtRegions = {\n        r1: word.substring(regs.r1),\n        r2: word.substring(regs.r2),\n        rv: word.substring(regs.rv)\n      };\n      if (stemmed === expectedStem) {\n        ok.push(word);\n      } else {\n        ko.push({\n          word: word,\n          expected: expectedStem,\n          actual: stemmed,\n          regions: txtRegions\n        });\n      }\n    });\n    expect(ko.length).toBe(0);\n  });","file":"porter_stemmer_fr_spec.js","skipped":false,"dir":"spec"},{"name":"should perform stem","suites":["porter_stemmer_it"],"line":28,"updatePoint":{"line":28,"column":25,"index":1319},"code":"  it('should perform stem', function () {\n    const errors = [];\n    Object.keys(snowBallDict).forEach(word => {\n      const stemmed = stemmer.stem(word);\n      const expectedStem = snowBallDict[word];\n      if (stemmed !== snowBallDict[word]) {\n        console.log('Error:', word, 'Expected:', expectedStem, 'Got:', stemmed);\n        errors.push({\n          word: word,\n          expected: expectedStem,\n          actual: stemmed\n        });\n      }\n    });\n    expect(errors.length).toEqual(0);\n  });","file":"porter_stemmer_it_spec.js","skipped":false,"dir":"spec"},{"name":"should perform stemming on a lot of words","suites":["porter_stemmer_nl"],"line":31,"updatePoint":{"line":31,"column":47,"index":1551},"code":"  it('should perform stemming on a lot of words', function () {\n    const errors = [];\n    Object.keys(snowBallDict).forEach(word => {\n      const stemmed = stemmer.stem(word);\n      const expectedStem = snowBallDict[word];\n      if (stemmed !== snowBallDict[word]) {\n        DEBUG && console.log('Error:', word, 'Expected:', expectedStem, 'Got:', stemmed);\n        errors.push({\n          word: word,\n          expected: expectedStem,\n          actual: stemmed\n        });\n      }\n    });\n\n    // The stemmer has an error count of 237 against the snowball list for nl that has 45669 entries\n    expect(errors.length).toEqual(237);\n  });","file":"porter_stemmer_nl_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize a piece of text","suites":["porter_stemmer_nl"],"line":49,"updatePoint":{"line":49,"column":37,"index":2179},"code":"  it('should tokenize a piece of text', function () {\n    dutchSentences.sentences.forEach((sentence, index) => {\n      const result = stemmer.tokenizeAndStem(sentence, true);\n      DEBUG && console.log(result);\n      expect(result).toEqual(dutchStemResults.results[index]);\n    });\n  });","file":"porter_stemmer_nl_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step 1a","suites":["porter_stemmer_no"],"line":28,"updatePoint":{"line":28,"column":28,"index":1315},"code":"  it('should perform step 1a', function () {\n    expect(stemmer.step1a('forenkla')).toBe('forenkl');\n    expect(stemmer.step1a('aase')).toBe('aas');\n    expect(stemmer.step1a('allerede')).toBe('aller');\n    expect(stemmer.step1a('aukande')).toBe('auk');\n    expect(stemmer.step1a('avbøtende')).toBe('avbøt');\n    expect(stemmer.step1a('avdelingane')).toBe('avdeling');\n    expect(stemmer.step1a('avgiftene')).toBe('avgift');\n    expect(stemmer.step1a('havnevirksomhetene')).toBe('havnevirksom');\n    expect(stemmer.step1a('heimelen')).toBe('heimel');\n    expect(stemmer.step1a('hovedvirksomheten')).toBe('hovedvirksom');\n    expect(stemmer.step1a('hovudreglar')).toBe('hovudregl');\n    expect(stemmer.step1a('hugger')).toBe('hugg');\n    expect(stemmer.step1a('importvirksomheter')).toBe('importvirksom');\n    expect(stemmer.step1a('ivaretas')).toBe('ivaret');\n    expect(stemmer.step1a('iverksettes')).toBe('iverksett');\n    expect(stemmer.step1a('konsekvensutredes')).toBe('konsekvensutr');\n    expect(stemmer.step1a('oversendes')).toBe('overs');\n    expect(stemmer.step1a('pensjonenes')).toBe('pensjon');\n    expect(stemmer.step1a('myndighetenes')).toBe('myndig');\n    expect(stemmer.step1a('møteleiarens')).toBe('møteleiar');\n    expect(stemmer.step1a('virksomhetens')).toBe('virksom');\n    expect(stemmer.step1a('aktørers')).toBe('aktør');\n    expect(stemmer.step1a('arbeidslivets')).toBe('arbeidsliv');\n    expect(stemmer.step1a('arbeidskapasitet')).toBe('arbeidskapasit');\n    expect(stemmer.step1a('arealknapphet')).toBe('arealknapp');\n    expect(stemmer.step1a('attgjevast')).toBe('attgjev');\n  });","file":"porter_stemmer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step 1b","suites":["porter_stemmer_no"],"line":56,"updatePoint":{"line":56,"column":28,"index":2922},"code":"  it('should perform step 1b', function () {\n    expect(stemmer.step1b('hinder')).toBe('hinder');\n    expect(stemmer.step1b('erwerbs')).toBe('erwerb');\n    expect(stemmer.step1b('alltids')).toBe('alltid');\n  });","file":"porter_stemmer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step 1c","suites":["porter_stemmer_no"],"line":61,"updatePoint":{"line":61,"column":28,"index":3134},"code":"  it('should perform step 1c', function () {\n    expect(stemmer.step1c('akkumulerte')).toBe('akkumuler');\n    expect(stemmer.step1c('akseptert')).toBe('aksepter');\n  });","file":"porter_stemmer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step 1 (a-c)","suites":["porter_stemmer_no"],"line":65,"updatePoint":{"line":65,"column":33,"index":3309},"code":"  it('should perform step 1 (a-c)', function () {\n    expect(stemmer.step1('andelar')).toBe('andel');\n    expect(stemmer.step1('andeleigar')).toBe('andeleig');\n    expect(stemmer.step1('andeleigarane')).toBe('andeleigar');\n    expect(stemmer.step1('andeleigarbok')).toBe('andeleigarbok');\n    expect(stemmer.step1('andelen')).toBe('andel');\n    expect(stemmer.step1('andelene')).toBe('andel');\n    expect(stemmer.step1('andeler')).toBe('andel');\n    expect(stemmer.step1('andelsbevis')).toBe('andelsbevis');\n    expect(stemmer.step1('andelsbok')).toBe('andelsbok');\n    expect(stemmer.step1('andelsboka')).toBe('andelsbok');\n    expect(stemmer.step1('andelsboligforening')).toBe('andelsboligforening');\n    expect(stemmer.step1('andelsboligforeninger')).toBe('andelsboligforening');\n    expect(stemmer.step1('andelsboligorganisation')).toBe('andelsboligorganisation');\n    expect(stemmer.step1('andelsboligorganisationer')).toBe('andelsboligorganisation');\n    expect(stemmer.step1('andelsbrev')).toBe('andelsbrev');\n  });","file":"porter_stemmer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step 2","suites":["porter_stemmer_no"],"line":82,"updatePoint":{"line":82,"column":27,"index":4326},"code":"  it('should perform step 2', function () {\n    expect(stemmer.step2('hvorvidt')).toBe('hvorvid');\n    expect(stemmer.step2('innovativt')).toBe('innovativ');\n  });","file":"porter_stemmer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step 3","suites":["porter_stemmer_no"],"line":86,"updatePoint":{"line":86,"column":27,"index":4490},"code":"  it('should perform step 3', function () {\n    expect(stemmer.step3('lovleg')).toBe('lov');\n    expect(stemmer.step3('konkurranseskadeleg')).toBe('konkurranseskad');\n    expect(stemmer.step3('lystig')).toBe('lyst');\n    expect(stemmer.step3('utrolig')).toBe('utro');\n    expect(stemmer.step3('utrøstelig')).toBe('utrøst');\n    expect(stemmer.step3('boliglov')).toBe('bolig');\n    expect(stemmer.step3('samvirkelov')).toBe('samvirk');\n    expect(stemmer.step3('arveavgiftslov')).toBe('arveavgift');\n  });","file":"porter_stemmer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should perform stemming on a lot of words","suites":["porter_stemmer_no"],"line":96,"updatePoint":{"line":96,"column":47,"index":5015},"code":"  it('should perform stemming on a lot of words', function () {\n    const ok = [];\n    const ko = [];\n    Object.keys(snowBallDict).forEach(word => {\n      const stemmed = stemmer.stem(word);\n      const expectedStem = snowBallDict[word];\n      if (stemmed === expectedStem) {\n        ok.push(word);\n      } else {\n        ko.push({\n          word: word,\n          expected: expectedStem,\n          actual: stemmed\n        });\n      }\n    });\n    expect(ko.length).toBe(0);\n  });","file":"porter_stemmer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should perform complete stemming","suites":["porter_stemmer_no"],"line":114,"updatePoint":{"line":114,"column":38,"index":5486},"code":"  it('should perform complete stemming', function () {\n    /*\n    expect(stemmer.step1a('forenkla')).toBe('forenkl')\n    expect(stemmer.step1a('aase')).toBe('aas')\n    expect(stemmer.step1a('allerede')).toBe('aller')\n    expect(stemmer.step1a('aukande')).toBe('auk')\n    */\n    expect(stemmer.step1a('avbøtende')).toBe('avbøt');\n    expect(stemmer.step1b('hinder')).toBe('hinder');\n    expect(stemmer.step1b('erwerbs')).toBe('erwerb');\n    expect(stemmer.step3('boliglov')).toBe('bolig');\n    expect(stemmer.step3('samvirkelov')).toBe('samvirk');\n    expect(stemmer.step3('arveavgiftslov')).toBe('arveavgift');\n  });","file":"porter_stemmer_no_spec.js","skipped":false,"dir":"spec"},{"name":"should perform stemming on a lot of words","suites":["porter_stemmer_pt"],"line":29,"updatePoint":{"line":29,"column":47,"index":1350},"code":"  it('should perform stemming on a lot of words', function () {\n    const errors = [];\n    Object.keys(snowBallDict).forEach(word => {\n      const stemmed = stemmer.stem(word);\n      const expectedStem = snowBallDict[word];\n      if (stemmed !== snowBallDict[word]) {\n        DEBUG && console.log('Error:', word, 'Expected:', expectedStem, 'Got:', stemmed);\n        errors.push({\n          word: word,\n          expected: expectedStem,\n          actual: stemmed\n        });\n      }\n    });\n    expect(errors.length).toEqual(0);\n  });","file":"porter_stemmer_pt_spec.js","skipped":false,"dir":"spec"},{"name":"should permof stem","suites":["porter_stemmer"],"line":29,"updatePoint":{"line":29,"column":24,"index":3612},"code":"  it('should permof stem', function () {\n    for (let i = 0; i < test.length; i++) {\n      expect(stemmer.stem(test[i])).toBe(testResult[i]);\n    }\n  }); /*,","file":"porter_stemmer_ru_spec.js","skipped":false,"dir":"spec"},{"name":"should categorizeGroups","suites":["porter_stemmer"],"line":28,"updatePoint":{"line":28,"column":29,"index":1276},"code":"  it('should categorizeGroups', function () {\n    expect(stemmer.categorizeGroups('syllog')).toBe('CVCVC');\n    expect(stemmer.categorizeGroups('gypsy')).toBe('CVCV');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should measure","suites":["porter_stemmer"],"line":32,"updatePoint":{"line":32,"column":20,"index":1441},"code":"  it('should measure', function () {\n    expect(stemmer.measure('syllog')).toBe(2);\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step 1a","suites":["porter_stemmer"],"line":35,"updatePoint":{"line":35,"column":28,"index":1539},"code":"  it('should perform step 1a', function () {\n    expect(stemmer.step1a('caresses')).toBe('caress');\n    expect(stemmer.step1a('ponies')).toBe('poni');\n    expect(stemmer.step1a('ties')).toBe('ti');\n    expect(stemmer.step1a('caress')).toBe('caress');\n    expect(stemmer.step1a('cats')).toBe('cat');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step 1b","suites":["porter_stemmer"],"line":42,"updatePoint":{"line":42,"column":28,"index":1844},"code":"  it('should perform step 1b', function () {\n    expect(stemmer.step1b('feed')).toBe('feed');\n    expect(stemmer.step1b('agreed')).toBe('agree');\n    expect(stemmer.step1b('plastered')).toBe('plaster');\n    expect(stemmer.step1b('bled')).toBe('bled');\n    expect(stemmer.step1b('motoring')).toBe('motor');\n    expect(stemmer.step1b('sing')).toBe('sing');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step 1c","suites":["porter_stemmer"],"line":50,"updatePoint":{"line":50,"column":28,"index":2205},"code":"  it('should perform step 1c', function () {\n    expect(stemmer.step1c('happy')).toBe('happi');\n    expect(stemmer.step1c('sky')).toBe('sky');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step 2","suites":["porter_stemmer"],"line":54,"updatePoint":{"line":54,"column":27,"index":2353},"code":"  it('should perform step 2', function () {\n    expect(stemmer.step2('relational')).toBe('relate');\n    expect(stemmer.step2('conditional')).toBe('condition');\n    expect(stemmer.step2('rational')).toBe('rational');\n    expect(stemmer.step2('valenci')).toBe('valence');\n    expect(stemmer.step2('hesitanci')).toBe('hesitance');\n    expect(stemmer.step2('digitizer')).toBe('digitize');\n    expect(stemmer.step2('conformabli')).toBe('conformable');\n    expect(stemmer.step2('radicalli')).toBe('radical');\n    expect(stemmer.step2('differentli')).toBe('different');\n    expect(stemmer.step2('vileli')).toBe('vile');\n    expect(stemmer.step2('analogousli')).toBe('analogous');\n    expect(stemmer.step2('vietnamization')).toBe('vietnamize');\n    expect(stemmer.step2('predication')).toBe('predicate');\n    expect(stemmer.step2('operator')).toBe('operate');\n    expect(stemmer.step2('feudalism')).toBe('feudal');\n    expect(stemmer.step2('decisiveness')).toBe('decisive');\n    expect(stemmer.step2('hopefulness')).toBe('hopeful');\n    expect(stemmer.step2('callousness')).toBe('callous');\n    expect(stemmer.step2('formaliti')).toBe('formal');\n    expect(stemmer.step2('sensitiviti')).toBe('sensitive');\n    expect(stemmer.step2('sensibiliti')).toBe('sensible');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step 3","suites":["porter_stemmer"],"line":77,"updatePoint":{"line":77,"column":27,"index":3616},"code":"  it('should perform step 3', function () {\n    expect(stemmer.step3('triplicate')).toBe('triplic');\n    expect(stemmer.step3('formative')).toBe('form');\n    expect(stemmer.step3('formalize')).toBe('formal');\n    expect(stemmer.step3('electriciti')).toBe('electric');\n    expect(stemmer.step3('electrical')).toBe('electric');\n    expect(stemmer.step3('hopeful')).toBe('hope');\n    expect(stemmer.step3('goodness')).toBe('good');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step 4","suites":["porter_stemmer"],"line":86,"updatePoint":{"line":86,"column":27,"index":4051},"code":"  it('should perform step 4', function () {\n    expect(stemmer.step4('revival')).toBe('reviv');\n    expect(stemmer.step4('allowance')).toBe('allow');\n    expect(stemmer.step4('inference')).toBe('infer');\n    expect(stemmer.step4('airliner')).toBe('airlin');\n    expect(stemmer.step4('gyroscopic')).toBe('gyroscop');\n    expect(stemmer.step4('adjustable')).toBe('adjust');\n    expect(stemmer.step4('defensible')).toBe('defens');\n    expect(stemmer.step4('irritant')).toBe('irrit');\n    expect(stemmer.step4('replacement')).toBe('replac');\n    expect(stemmer.step4('adjustment')).toBe('adjust');\n    expect(stemmer.step4('dependent')).toBe('depend');\n    expect(stemmer.step4('adoption')).toBe('adopt');\n    expect(stemmer.step4('homologou')).toBe('homolog');\n    expect(stemmer.step4('communism')).toBe('commun');\n    expect(stemmer.step4('activate')).toBe('activ');\n    expect(stemmer.step4('angulariti')).toBe('angular');\n    expect(stemmer.step4('homologous')).toBe('homolog');\n    expect(stemmer.step4('effective')).toBe('effect');\n    expect(stemmer.step4('bowdlerize')).toBe('bowdler');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step 5a","suites":["porter_stemmer"],"line":107,"updatePoint":{"line":107,"column":28,"index":5150},"code":"  it('should perform step 5a', function () {\n    expect(stemmer.step5a('probate')).toBe('probat');\n    expect(stemmer.step5a('rate')).toBe('rate');\n    expect(stemmer.step5a('cease')).toBe('ceas');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should perform step5b","suites":["porter_stemmer"],"line":112,"updatePoint":{"line":112,"column":27,"index":5353},"code":"  it('should perform step5b', function () {\n    expect(stemmer.step5b('controll')).toBe('control');\n    expect(stemmer.step5b('roll')).toBe('roll');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should perform complete stemming","suites":["porter_stemmer"],"line":116,"updatePoint":{"line":116,"column":38,"index":5519},"code":"  it('should perform complete stemming', function () {\n    expect(stemmer.stem('scoring')).toBe('score');\n    expect(stemmer.stem('scored')).toBe('score');\n    expect(stemmer.stem('scores')).toBe('score');\n    expect(stemmer.stem('score')).toBe('score');\n    expect(stemmer.stem('SCORING')).toBe('score');\n    expect(stemmer.stem('SCORED')).toBe('score');\n    expect(stemmer.stem('SCORES')).toBe('score');\n    expect(stemmer.stem('SCORE')).toBe('score');\n    expect(stemmer.stem('nationals')).toBe('nation');\n    expect(stemmer.stem('doing')).toBe('do');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should perform stem animated to anim","suites":["porter_stemmer"],"line":128,"updatePoint":{"line":128,"column":42,"index":6084},"code":"  it('should perform stem animated to anim', function () {\n    expect(stemmer.stem('animated')).toBe('anim');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"issue 176 - attemptReplace ed|ing","suites":["porter_stemmer"],"line":131,"updatePoint":{"line":131,"column":39,"index":6197},"code":"  it('issue 176 - attemptReplace ed|ing', function () {\n    expect(stemmer.stem('aedile')).toBe('aedil');\n    expect(stemmer.stem('adoptedly')).toBe('adoptedli');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"issue 176 - minimum length for final e","suites":["porter_stemmer"],"line":135,"updatePoint":{"line":135,"column":44,"index":6371},"code":"  it('issue 176 - minimum length for final e', function () {\n    expect(stemmer.stem('ace')).toBe('ac');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"issue 176 - minimum length for final s","suites":["porter_stemmer"],"line":138,"updatePoint":{"line":138,"column":44,"index":6482},"code":"  it('issue 176 - minimum length for final s', function () {\n    expect(stemmer.stem('yes')).toBe('ye');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"issue 176 - s/tion measure","suites":["porter_stemmer"],"line":141,"updatePoint":{"line":141,"column":32,"index":6581},"code":"  it('issue 176 - s/tion measure', function () {\n    expect(stemmer.stem('invasion')).toBe('invas');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"issue 176 - vehement - step 4","suites":["porter_stemmer"],"line":144,"updatePoint":{"line":144,"column":35,"index":6691},"code":"  it('issue 176 - vehement - step 4', function () {\n    expect(stemmer.stem('vehement')).toBe('vehement');\n    expect(stemmer.stem('syllogism')).toBe('syllog');\n    expect(stemmer.stem('gypsy')).toBe('gypsi');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"issue 176 step 2","suites":["porter_stemmer"],"line":149,"updatePoint":{"line":149,"column":22,"index":6894},"code":"  it('issue 176 step 2', function () {\n    expect(stemmer.stem('terribly')).toBe('terribl');\n    expect(stemmer.stem('apology')).toBe('apolog');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"issue 176 - step 5a","suites":["porter_stemmer"],"line":153,"updatePoint":{"line":153,"column":25,"index":7048},"code":"  it('issue 176 - step 5a', function () {\n    expect(stemmer.stem('type')).toBe('type');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"issue 176 - corruptiblity","suites":["porter_stemmer"],"line":156,"updatePoint":{"line":156,"column":31,"index":7149},"code":"  it('issue 176 - corruptiblity', function () {\n    expect(stemmer.stem('corruptibility')).toBe('corrupt');\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize and stem ignoring stopwords","suites":["porter_stemmer"],"line":168,"updatePoint":{"line":168,"column":49,"index":7528},"code":"  it('should tokenize and stem ignoring stopwords', function () {\n    expect(stemmer.tokenizeAndStem('My dog is very fun TO play with And another thing, he is A poodle.')).toEqual(['dog', 'fun', 'plai', 'thing', 'poodl']);\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize and stem ignoring all capital stopwords","suites":["porter_stemmer"],"line":171,"updatePoint":{"line":171,"column":61,"index":7769},"code":"  it('should tokenize and stem ignoring all capital stopwords', function () {\n    const allCapitalStopwords = stopwords.words.join(' ').toUpperCase();\n    expect(stemmer.tokenizeAndStem(allCapitalStopwords)).toEqual([]);\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize and stem including stopwords","suites":["porter_stemmer"],"line":175,"updatePoint":{"line":175,"column":50,"index":7985},"code":"  it('should tokenize and stem including stopwords', function () {\n    expect(stemmer.tokenizeAndStem('My dog is very fun TO play with And another thing, he is A poodle.', true)).toEqual(['my', 'dog', 'is', 'veri', 'fun', 'to', 'plai', 'with', 'and', 'anoth', 'thing', 'he', 'is', 'a', 'poodl']);\n  });","file":"porter_stemmer_spec.js","skipped":false,"dir":"spec"},{"name":"Stem whole sample data","suites":["porter_stemmer_sv"],"line":27,"updatePoint":{"line":27,"column":28,"index":1224},"code":"  it('Stem whole sample data', function () {\n    const sample = require('./test_data/sv_stemmer_sample.js');\n    const res = sample.words.map(w => stemmer.stem(w));\n    expect(res).toEqual(sample.stemmedWords);\n  });","file":"porter_stemmer_sv_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular ES forms","suites":["present verb inflector","singularization"],"line":27,"updatePoint":{"line":27,"column":43,"index":1322},"code":"    it('should singularize regular ES forms', function () {\n      expect(inflector.singularize('catch')).toBe('catches');\n      expect(inflector.singularize('do')).toBe('does');\n      expect(inflector.singularize('go')).toBe('goes');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle [CS]HES forms","suites":["present verb inflector","singularization"],"line":32,"updatePoint":{"line":32,"column":35,"index":1556},"code":"    it('should handle [CS]HES forms', function () {\n      expect(inflector.singularize('cash')).toBe('cashes');\n      expect(inflector.singularize('ach')).toBe('aches');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should ignore XES forms","suites":["present verb inflector","singularization"],"line":36,"updatePoint":{"line":36,"column":31,"index":1730},"code":"    it('should ignore XES forms', function () {\n      expect(inflector.singularize('annex')).toBe('annexes');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle SSES forms","suites":["present verb inflector","singularization"],"line":39,"updatePoint":{"line":39,"column":32,"index":1849},"code":"    it('should handle SSES forms', function () {\n      expect(inflector.singularize('access')).toBe('accesses');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should ignore ZZES forms","suites":["present verb inflector","singularization"],"line":42,"updatePoint":{"line":42,"column":32,"index":1970},"code":"    it('should ignore ZZES forms', function () {\n      expect(inflector.singularize('buzz')).toBe('buzzes');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize regular S forms","suites":["present verb inflector","singularization"],"line":45,"updatePoint":{"line":45,"column":42,"index":2097},"code":"    it('should singularize regular S forms', function () {\n      expect(inflector.singularize('claim')).toBe('claims');\n      expect(inflector.singularize('drink')).toBe('drinks');\n      expect(inflector.singularize('become')).toBe('becomes');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize irregular forms","suites":["present verb inflector","singularization"],"line":50,"updatePoint":{"line":50,"column":42,"index":2349},"code":"    it('should singularize irregular forms', function () {\n      expect(inflector.singularize('are')).toBe('is');\n      expect(inflector.singularize('were')).toBe('was');\n      expect(inflector.singularize('have')).toBe('has');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should singularize ies forms","suites":["present verb inflector","singularization"],"line":55,"updatePoint":{"line":55,"column":36,"index":2579},"code":"    it('should singularize ies forms', function () {\n      expect(inflector.singularize('fly')).toBe('flies');\n      expect(inflector.singularize('try')).toBe('tries');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle ambiguous forms","suites":["present verb inflector","singularization"],"line":59,"updatePoint":{"line":59,"column":37,"index":2757},"code":"    it('should handle ambiguous forms', function () {\n      expect(inflector.singularize('will')).toBe('will');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular ES forms","suites":["present verb inflector","pluralization"],"line":64,"updatePoint":{"line":64,"column":41,"index":2929},"code":"    it('should pluralize regular ES forms', function () {\n      expect(inflector.pluralize('catches')).toBe('catch');\n      expect(inflector.pluralize('does')).toBe('do');\n      expect(inflector.pluralize('goes')).toBe('go');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle [CS]HES forms","suites":["present verb inflector","pluralization"],"line":69,"updatePoint":{"line":69,"column":35,"index":3157},"code":"    it('should handle [CS]HES forms', function () {\n      expect(inflector.pluralize('cashes')).toBe('cash');\n      expect(inflector.pluralize('aches')).toBe('ach');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle XES forms","suites":["present verb inflector","pluralization"],"line":73,"updatePoint":{"line":73,"column":31,"index":3327},"code":"    it('should handle XES forms', function () {\n      expect(inflector.pluralize('annexes')).toBe('annex');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle SSES forms","suites":["present verb inflector","pluralization"],"line":76,"updatePoint":{"line":76,"column":32,"index":3444},"code":"    it('should handle SSES forms', function () {\n      expect(inflector.pluralize('accesses')).toBe('access');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle ZZES forms","suites":["present verb inflector","pluralization"],"line":79,"updatePoint":{"line":79,"column":32,"index":3563},"code":"    it('should handle ZZES forms', function () {\n      expect(inflector.pluralize('buzzes')).toBe('buzz');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular S forms that done drop e","suites":["present verb inflector","pluralization"],"line":82,"updatePoint":{"line":82,"column":57,"index":3703},"code":"    it('should pluralize regular S forms that done drop e', function () {\n      expect(inflector.pluralize('becomes')).toBe('become');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize regular S forms","suites":["present verb inflector","pluralization"],"line":85,"updatePoint":{"line":85,"column":40,"index":3829},"code":"    it('should pluralize regular S forms', function () {\n      expect(inflector.pluralize('drinks')).toBe('drink');\n      expect(inflector.pluralize('claims')).toBe('claim');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize irregular forms","suites":["present verb inflector","pluralization"],"line":89,"updatePoint":{"line":89,"column":40,"index":4012},"code":"    it('should pluralize irregular forms', function () {\n      expect(inflector.pluralize('was')).toBe('were');\n      expect(inflector.pluralize('is')).toBe('are');\n      expect(inflector.pluralize('am')).toBe('are');\n      expect(inflector.pluralize('has')).toBe('have');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize ies forms","suites":["present verb inflector","pluralization"],"line":95,"updatePoint":{"line":95,"column":34,"index":4287},"code":"    it('should pluralize ies forms', function () {\n      expect(inflector.pluralize('flies')).toBe('fly');\n      expect(inflector.pluralize('tries')).toBe('try');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should handle ambiguous forms","suites":["present verb inflector","pluralization"],"line":99,"updatePoint":{"line":99,"column":37,"index":4461},"code":"    it('should handle ambiguous forms', function () {\n      expect(inflector.pluralize('will')).toBe('will');\n    });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should pluralize and singularize string","suites":["present verb inflector","pluralization"],"line":103,"updatePoint":{"line":103,"column":45,"index":4593},"code":"  it('should pluralize and singularize string', function () {\n    expect(inflector.pluralize('becomes')).toBe('become');\n    expect(inflector.singularize('become')).toBe('becomes');\n  });","file":"present_verb_inflector_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly remove diacritics","suites":["remove_diacritics"],"line":25,"updatePoint":{"line":25,"column":40,"index":1230},"code":"  it('should correctly remove diacritics', function () {\n    const originalPhrase = 'piñon ça va über résumé œdipe';\n    expect(removeDiacritics(originalPhrase)).toEqual('pinon ca va uber resume oedipe');\n  });","file":"remove_diacritics_spec.js","skipped":false,"dir":"spec"},{"name":"should load","suites":["sentence analyzer"],"line":28,"updatePoint":{"line":28,"column":17,"index":1259},"code":"  it('should load', function () {\n    new SentenceAnalyzer(null, function () {});\n  });","file":"sentence_analyzer_spec.js","skipped":false,"dir":"spec"},{"name":"should determine PP and SP, given a POS","suites":["sentence analyzer"],"line":55,"updatePoint":{"line":55,"column":45,"index":2126},"code":"  it('should determine PP and SP, given a POS', function () {\n    const sentenceTags = [{\n      token: 'The',\n      pos: 'DT'\n    }, {\n      token: 'angry',\n      pos: 'JJ'\n    }, {\n      token: 'bear',\n      pos: 'NN'\n    }, {\n      token: 'chased',\n      pos: 'VB'\n    }, {\n      token: 'the',\n      pos: 'DT'\n    }, {\n      token: 'frightened',\n      pos: 'JJ'\n    }, {\n      token: 'little',\n      pos: 'JJ'\n    }, {\n      token: 'squirrel',\n      pos: 'NN'\n    }];\n    const tests = [{\n      token: 'angry',\n      argument: 'spos',\n      result: 'SP'\n    }, {\n      token: 'squirrel',\n      argument: 'spos',\n      result: 'PP'\n    }];\n    testSentenceAnalyzer(sentenceTags, tests, function (analyzer) {\n      expect(analyzer.subjectToString().trim()).toEqual('The angry bear');\n      expect(analyzer.predicateToString().trim()).toEqual('chased the frightened little squirrel');\n      expect(analyzer.toString().trim()).toEqual('The angry bear chased the frightened little squirrel');\n      expect(analyzer.implicitYou()).toEqual(false);\n    });\n  });","file":"sentence_analyzer_spec.js","skipped":false,"dir":"spec"},{"name":"should determine PP and SP given a POS that begins with a verb","suites":["sentence analyzer"],"line":97,"updatePoint":{"line":97,"column":68,"index":3206},"code":"  it('should determine PP and SP given a POS that begins with a verb', function () {\n    const sentenceTags = [{\n      token: 'Vote',\n      pos: 'VB'\n    }, {\n      token: 'for',\n      pos: 'IN'\n    }, {\n      token: 'me',\n      pos: 'PRP'\n    }];\n    const tests = [{\n      token: 'Vote',\n      argument: 'spos',\n      result: 'PP'\n    }, {\n      token: 'me',\n      argument: 'pp',\n      result: true\n    }, {\n      index: 'LAST',\n      argument: 'token',\n      result: 'You'\n    }, {\n      index: 'LAST',\n      argument: 'pos',\n      result: 'PRP'\n    }, {\n      index: 'LAST',\n      argument: 'added',\n      result: true\n    }];\n    testSentenceAnalyzer(sentenceTags, tests, function (analyzer) {\n      expect(analyzer.implicitYou()).toEqual(true);\n    });\n  });","file":"sentence_analyzer_spec.js","skipped":false,"dir":"spec"},{"name":"should look for EX before VB","suites":["sentence analyzer"],"line":133,"updatePoint":{"line":133,"column":34,"index":3938},"code":"  it('should look for EX before VB', function () {\n    const sentenceTags = [{\n      token: 'There',\n      pos: 'EX'\n    }, {\n      token: 'is',\n      pos: 'VB'\n    }, {\n      token: 'a',\n      pos: 'DT'\n    }, {\n      token: 'house',\n      pos: 'NN'\n    }, {\n      token: 'in',\n      pos: 'IN'\n    }, {\n      token: 'the',\n      pos: 'DT'\n    }, {\n      token: 'valley',\n      pos: 'DT'\n    }];\n    const tests = [{\n      token: 'There',\n      argument: 'spos',\n      result: 'SP'\n    }, {\n      token: 'is',\n      argument: 'spos',\n      result: 'SP'\n    }];\n    testSentenceAnalyzer(sentenceTags, tests, function (analyzer) {\n      expect(analyzer.subjectToString().trim()).toEqual('There is a house');\n      expect(analyzer.predicateToString().trim()).toEqual('');\n      expect(analyzer.toString().trim()).toEqual('There is a house in the valley');\n      expect(analyzer.implicitYou()).toEqual(false);\n    });\n  });","file":"sentence_analyzer_spec.js","skipped":false,"dir":"spec"},{"name":"should determine a command without punctuation","suites":["sentence analyzer","#type"],"line":183,"updatePoint":{"line":183,"column":54,"index":5229},"code":"    it('should determine a command without punctuation', function () {\n      const sentenceTags = [{\n        token: 'Vote',\n        pos: 'VB'\n      }, {\n        token: 'for',\n        pos: 'IN'\n      }, {\n        token: 'me',\n        pos: 'PRP'\n      }];\n      const punct = function () {\n        return [];\n      };\n      testSentenceType({\n        tags: sentenceTags,\n        punct: punct\n      }, 'COMMAND', null);\n    });","file":"sentence_analyzer_spec.js","skipped":false,"dir":"spec"},{"name":"should determine an interrogative beginning with who","suites":["sentence analyzer","#type"],"line":202,"updatePoint":{"line":202,"column":60,"index":5660},"code":"    it('should determine an interrogative beginning with who', function () {\n      const sentenceTags = [{\n        token: 'Who',\n        pos: 'WP'\n      }, {\n        token: 'voted',\n        pos: 'VB'\n      }];\n      const punct = function () {\n        return [];\n      };\n      testSentenceType({\n        tags: sentenceTags,\n        punct: punct\n      }, 'INTERROGATIVE', null);\n    });","file":"sentence_analyzer_spec.js","skipped":false,"dir":"spec"},{"name":"should determine an interrogative ending with a personal pronoun","suites":["sentence analyzer","#type"],"line":218,"updatePoint":{"line":218,"column":72,"index":6059},"code":"    it('should determine an interrogative ending with a personal pronoun', function () {\n      const sentenceTags = [{\n        token: 'Should',\n        pos: 'MD'\n      }, {\n        token: 'we',\n        pos: 'PRP'\n      }];\n      const punct = function () {\n        return '';\n      };\n      testSentenceType({\n        tags: sentenceTags,\n        punct: punct\n      }, 'INTERROGATIVE', null);\n    });","file":"sentence_analyzer_spec.js","skipped":false,"dir":"spec"},{"name":"should classify other sentences as unknown","suites":["sentence analyzer","#type"],"line":234,"updatePoint":{"line":234,"column":50,"index":6437},"code":"    it('should classify other sentences as unknown', function () {\n      const sentenceTags = [{\n        token: 'I',\n        pos: 'PRP'\n      }, {\n        token: 'am',\n        pos: 'VB'\n      }, {\n        token: 'unsure',\n        pos: 'JJ'\n      }];\n      const punct = function () {\n        return '';\n      };\n      testSentenceType({\n        tags: sentenceTags,\n        punct: punct\n      }, 'UNKNOWN', null);\n    });","file":"sentence_analyzer_spec.js","skipped":false,"dir":"spec"},{"name":"should determine an interrogative ending with a question mark","suites":["sentence analyzer","#type"],"line":253,"updatePoint":{"line":253,"column":69,"index":6877},"code":"    it('should determine an interrogative ending with a question mark', function () {\n      const sentenceTags = [{\n        token: 'Do',\n        pos: 'VB'\n      }, {\n        token: 'I',\n        pos: 'PRP'\n      }, {\n        token: 'care',\n        pos: 'VB'\n      }];\n      const punct = function () {\n        return [{\n          token: '?',\n          pos: '.'\n        }];\n      };\n      testSentenceType({\n        tags: sentenceTags,\n        punct: punct\n      }, 'INTERROGATIVE', null);\n    });","file":"sentence_analyzer_spec.js","skipped":false,"dir":"spec"},{"name":"should determine a command ending in an exclamation point","suites":["sentence analyzer","#type"],"line":285,"updatePoint":{"line":285,"column":65,"index":7540},"code":"    it('should determine a command ending in an exclamation point', function () {\n      const punct = function () {\n        return [{\n          token: '!',\n          pos: '.'\n        }];\n      };\n      testSentenceType({\n        tags: taggedSentForCommand,\n        punct: punct\n      }, 'COMMAND', null);\n    });","file":"sentence_analyzer_spec.js","skipped":false,"dir":"spec"},{"name":"should determine a command ending with .","suites":["sentence analyzer","#type"],"line":297,"updatePoint":{"line":297,"column":48,"index":7836},"code":"    it('should determine a command ending with .', function () {\n      const punct = function () {\n        return [{\n          token: '.',\n          pos: '.'\n        }];\n      };\n      testSentenceType({\n        tags: taggedSentForCommand,\n        punct: punct\n      }, 'COMMAND', null);\n    });","file":"sentence_analyzer_spec.js","skipped":false,"dir":"spec"},{"name":"should determine an exclamation ending in an exclamation point","suites":["sentence analyzer","#type"],"line":319,"updatePoint":{"line":319,"column":70,"index":8330},"code":"    it('should determine an exclamation ending in an exclamation point', function () {\n      const punct = function () {\n        return [{\n          token: '!',\n          pos: '.'\n        }];\n      };\n      testSentenceType({\n        tags: taggedSentenceForExclam,\n        punct: punct\n      }, 'EXCLAMATORY', null);\n    });","file":"sentence_analyzer_spec.js","skipped":false,"dir":"spec"},{"name":"should determine a declaration ending with a .","suites":["sentence analyzer","#type"],"line":331,"updatePoint":{"line":331,"column":54,"index":8639},"code":"    it('should determine a declaration ending with a .', function () {\n      const punct = function () {\n        return [{\n          token: '.',\n          pos: '.'\n        }];\n      };\n      testSentenceType({\n        tags: taggedSentenceForExclam,\n        punct: punct\n      }, 'DECLARATIVE', null);\n    });","file":"sentence_analyzer_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings and trim whitespace","suites":["sentence_tokenizer"],"line":28,"updatePoint":{"line":28,"column":49,"index":1316},"code":"  it('should tokenize strings and trim whitespace', function () {\n    expect(tokenizer.tokenize('This is a sentence. This is another sentence.')).toEqual(['This is a sentence.', 'This is another sentence.']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should include quotation marks","suites":["sentence_tokenizer"],"line":53,"updatePoint":{"line":53,"column":36,"index":2075},"code":"  it('should include quotation marks', function () {\n    expect(tokenizer.tokenize('\"This is a sentence.\" This is another sentence.')).toEqual(['\"This is a sentence.\"', 'This is another sentence.']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should include brackets","suites":["sentence_tokenizer"],"line":56,"updatePoint":{"line":56,"column":29,"index":2274},"code":"  it('should include brackets', function () {\n    expect(tokenizer.tokenize('This is a sentence. [This is another sentence.]')).toEqual(['This is a sentence.', '[This is another sentence.]']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should handle repetitive punctuation","suites":["sentence_tokenizer"],"line":59,"updatePoint":{"line":59,"column":42,"index":2486},"code":"  it('should handle repetitive punctuation', function () {\n    expect(tokenizer.tokenize('I love you!! Do you love me??')).toEqual(['I love you!!', 'Do you love me??']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should handle repetitive punctuation with space","suites":["sentence_tokenizer"],"line":62,"updatePoint":{"line":62,"column":53,"index":2673},"code":"  it('should handle repetitive punctuation with space', function () {\n    expect(tokenizer.tokenize('I love you! ! Do you love me? ?')).toEqual(['I love you! !', 'Do you love me? ?']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should handle decimal numbers in sentences","suites":["sentence_tokenizer"],"line":65,"updatePoint":{"line":65,"column":48,"index":2859},"code":"  it('should handle decimal numbers in sentences', function () {\n    expect(tokenizer.tokenize('Pi is approximately equal to 3.14.')).toEqual(['Pi is approximately equal to 3.14.']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize text with a number like \"1)\" present ","suites":["sentence_tokenizer"],"line":68,"updatePoint":{"line":68,"column":59,"index":3059},"code":"  it('should tokenize text with a number like \"1)\" present ', function () {\n    expect(tokenizer.tokenize(\"This is a sentence that can't 1) be parsed with SentenceTokenizerNew. Here is another sentence.\")).toEqual([\"This is a sentence that can't 1) be parsed with SentenceTokenizerNew.\", 'Here is another sentence.']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should handle periods in email addresses","suites":["sentence_tokenizer"],"line":71,"updatePoint":{"line":71,"column":46,"index":3371},"code":"  it('should handle periods in email addresses', function () {\n    expect(tokenizer.tokenize('My email address is batman@example.com.')).toEqual(['My email address is batman@example.com.']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should handle periods in web addresses","suites":["sentence_tokenizer"],"line":74,"updatePoint":{"line":74,"column":44,"index":3566},"code":"  it('should handle periods in web addresses', function () {\n    expect(tokenizer.tokenize('My twitter feed can be found at https://twitter.com/user1.')).toEqual(['My twitter feed can be found at https://twitter.com/user1.']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should handle an ellipsis followed by punctuation","suites":["sentence_tokenizer"],"line":77,"updatePoint":{"line":77,"column":55,"index":3810},"code":"  it('should handle an ellipsis followed by punctuation', function () {\n    expect(tokenizer.tokenize('Is this the end for our heroes...?')).toEqual(['Is this the end for our heroes...?']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should handle multiple spaces separating sentences","suites":["sentence_tokenizer"],"line":80,"updatePoint":{"line":80,"column":56,"index":4007},"code":"  it('should handle multiple spaces separating sentences', function () {\n    expect(tokenizer.tokenize('Tune in tomorrow and find out!  Same Bat-Time!  Same Bat-Channel!')).toEqual(['Tune in tomorrow and find out!', 'Same Bat-Time!', 'Same Bat-Channel!']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should handle a sentence that does not end with punctuation (issue #549))","suites":["sentence_tokenizer"],"line":83,"updatePoint":{"line":83,"column":79,"index":4293},"code":"  it('should handle a sentence that does not end with punctuation (issue #549))', function () {\n    expect(tokenizer.tokenize('This is a sentence. But is this also one')).toEqual(['This is a sentence.', 'But is this also one']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should handle a sentence that contains a quoted phrase (issue #550 but with . and ’ reversed))","suites":["sentence_tokenizer"],"line":86,"updatePoint":{"line":86,"column":100,"index":4549},"code":"  it('should handle a sentence that contains a quoted phrase (issue #550 but with . and ’ reversed))', function () {\n    expect(tokenizer.tokenize('This is a sentence. And another where ‘Someone says something’.')).toEqual(['This is a sentence.', 'And another where ‘Someone says something’.']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should handle sentences with an abbreviation","suites":["sentence_tokenizer"],"line":89,"updatePoint":{"line":89,"column":50,"index":4801},"code":"  it('should handle sentences with an abbreviation', function () {\n    expect(tokenizer.tokenize('Acme, Inc. is creating exciting products. Use at your own risk.')).toEqual(['Acme, Inc. is creating exciting products.', 'Use at your own risk.']);\n    expect(tokenizer.tokenize('I need the parts A.S.A.P. please. Send them when they are ready')).toEqual(['I need the parts A.S.A.P. please.', 'Send them when they are ready']);\n    expect(tokenizer.tokenize('I need the parts from Inc.. Send them when they are ready!')).toEqual(['I need the parts from Inc..', 'Send them when they are ready!']);\n  });","file":"sentence_tokenizer_parser_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings and trim whitespace","suites":["sentence_tokenizer"],"line":28,"updatePoint":{"line":28,"column":49,"index":1283},"code":"  it('should tokenize strings and trim whitespace', function () {\n    expect(tokenizer.tokenize('This is a sentence. This is another sentence.')).toEqual(['This is a sentence.', 'This is another sentence.']);\n  });","file":"sentence_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should include quotation marks","suites":["sentence_tokenizer"],"line":53,"updatePoint":{"line":53,"column":36,"index":2042},"code":"  it('should include quotation marks', function () {\n    expect(tokenizer.tokenize('\"This is a sentence.\" This is another sentence.')).toEqual(['\"This is a sentence.\"', 'This is another sentence.']);\n  });","file":"sentence_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should include brackets","suites":["sentence_tokenizer"],"line":56,"updatePoint":{"line":56,"column":29,"index":2241},"code":"  it('should include brackets', function () {\n    expect(tokenizer.tokenize('This is a sentence. [This is another sentence.]')).toEqual(['This is a sentence.', '[This is another sentence.]']);\n  });","file":"sentence_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should handle repetitive punctuation","suites":["sentence_tokenizer"],"line":59,"updatePoint":{"line":59,"column":42,"index":2453},"code":"  it('should handle repetitive punctuation', function () {\n    expect(tokenizer.tokenize('I love you!! Do you love me??')).toEqual(['I love you!!', 'Do you love me??']);\n  });","file":"sentence_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should handle repetitive punctuation with space","suites":["sentence_tokenizer"],"line":62,"updatePoint":{"line":62,"column":53,"index":2640},"code":"  it('should handle repetitive punctuation with space', function () {\n    expect(tokenizer.tokenize('I love you! ! Do you love me? ?')).toEqual(['I love you! !', 'Do you love me? ?']);\n  });","file":"sentence_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should handle decimal points in numbers","suites":["sentence_tokenizer"],"line":65,"updatePoint":{"line":65,"column":45,"index":2823},"code":"  it('should handle decimal points in numbers', function () {\n    expect(tokenizer.tokenize('Pi is approximately equal to 3.14.')).toEqual(['Pi is approximately equal to 3.14.']);\n  });","file":"sentence_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should handle periods in email addresses","suites":["sentence_tokenizer"],"line":68,"updatePoint":{"line":68,"column":46,"index":3010},"code":"  it('should handle periods in email addresses', function () {\n    expect(tokenizer.tokenize('My email address is batman@example.com.')).toEqual(['My email address is batman@example.com.']);\n  });","file":"sentence_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should handle periods in web addresses","suites":["sentence_tokenizer"],"line":71,"updatePoint":{"line":71,"column":44,"index":3205},"code":"  it('should handle periods in web addresses', function () {\n    expect(tokenizer.tokenize('My twitter feed can be found at https://twitter.com/user1.')).toEqual(['My twitter feed can be found at https://twitter.com/user1.']);\n  });","file":"sentence_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should handle an ellipsis followed by punctuation","suites":["sentence_tokenizer"],"line":74,"updatePoint":{"line":74,"column":55,"index":3449},"code":"  it('should handle an ellipsis followed by punctuation', function () {\n    expect(tokenizer.tokenize('Is this the end for our heroes...?')).toEqual(['Is this the end for our heroes...?']);\n  });","file":"sentence_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should handle multiple spaces separating sentences","suites":["sentence_tokenizer"],"line":77,"updatePoint":{"line":77,"column":56,"index":3646},"code":"  it('should handle multiple spaces separating sentences', function () {\n    expect(tokenizer.tokenize('Tune in tomorrow and find out!  Same Bat-Time!  Same Bat-Channel!')).toEqual(['Tune in tomorrow and find out!', 'Same Bat-Time!', 'Same Bat-Channel!']);\n  });","file":"sentence_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should handle braces and quotes (issue #591)","suites":["sentence_tokenizer"],"line":80,"updatePoint":{"line":80,"column":50,"index":3903},"code":"  it('should handle braces and quotes (issue #591)', function () {\n    expect(tokenizer.tokenize('Teste. Test test. Test test: “Test.”')).toEqual(['Teste.', 'Test test.', 'Test test: “Test.”']);\n    expect(tokenizer.tokenize('Test Test. Test test, test test (test test) test: “Test.”')).toEqual(['Test Test.', 'Test test, test test (test test) test: “Test.”']);\n    expect(tokenizer.tokenize('Test Test. Test test, test (test) test (test test) test: “Test.”')).toEqual(['Test Test.', 'Test test, test (test) test (test test) test: “Test.”']);\n    expect(tokenizer.tokenize('Test: Test (test) test “Test.”')).toEqual(['Test: Test (test) test “Test.”']);\n  });","file":"sentence_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should get the numbers of vertexs","suites":["shortest path tree","edge weighted digraph normal operations"],"line":44,"updatePoint":{"line":44,"column":41,"index":1837},"code":"    it('should get the numbers of vertexs', function () {\n      expect(digraph.v()).toBe(8);\n    });","file":"shortest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should get the numbers of edges","suites":["shortest path tree","edge weighted digraph normal operations"],"line":47,"updatePoint":{"line":47,"column":39,"index":1936},"code":"    it('should get the numbers of edges', function () {\n      expect(digraph.e()).toBe(13);\n    });","file":"shortest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should print all item in digraph","suites":["shortest path tree","edge weighted digraph normal operations"],"line":50,"updatePoint":{"line":50,"column":40,"index":2037},"code":"    it('should print all item in digraph', function () {\n      expect(digraph.toString()).toBe('0 -> 2, 0.26\\n' + '1 -> 3, 0.29\\n' + '3 -> 7, 0.39\\n' + '3 -> 6, 0.52\\n' + '4 -> 7, 0.37\\n' + '4 -> 0, 0.38\\n' + '5 -> 4, 0.35\\n' + '5 -> 7, 0.28\\n' + '5 -> 1, 0.32\\n' + '6 -> 2, 0.4\\n' + '6 -> 0, 0.58\\n' + '6 -> 4, 0.93\\n' + '7 -> 2, 0.34');\n    });","file":"shortest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should sort all the vertexs","suites":["shortest path tree","topo sort for digraph"],"line":55,"updatePoint":{"line":55,"column":35,"index":2435},"code":"    it('should sort all the vertexs', function () {\n      const topoSort = new Topological(digraph);\n      expect(topoSort.isDAG()).toBe(true);\n      expect(topoSort.order()).toEqual([5, 1, 3, 6, 4, 7, 0, 2]);\n    });","file":"shortest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should determine existence of paths","suites":["shortest path tree","shortest path tree normal operations"],"line":63,"updatePoint":{"line":63,"column":43,"index":2769},"code":"    it('should determine existence of paths', function () {\n      expect(lpt.hasPathTo(0)).toBe(true);\n      expect(lpt.hasPathTo(1)).toBe(true);\n      expect(lpt.hasPathTo(2)).toBe(true);\n      expect(lpt.hasPathTo(3)).toBe(true);\n      expect(lpt.hasPathTo(4)).toBe(true);\n      expect(lpt.hasPathTo(5)).toBe(false);\n      expect(lpt.hasPathTo(6)).toBe(true);\n      expect(lpt.hasPathTo(7)).toBe(true);\n    });","file":"shortest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should determine paths","suites":["shortest path tree","shortest path tree normal operations"],"line":73,"updatePoint":{"line":73,"column":30,"index":3169},"code":"    it('should determine paths', function () {\n      expect(lpt.pathTo(0)).toEqual([5, 4, 0]);\n      expect(lpt.pathTo(1)).toEqual([5, 1]);\n      expect(lpt.pathTo(2)).toEqual([5, 7, 2]);\n      expect(lpt.pathTo(3)).toEqual([5, 1, 3]);\n      expect(lpt.pathTo(4)).toEqual([5, 4]);\n      expect(lpt.pathTo(5)).toEqual([]);\n      expect(lpt.pathTo(6)).toEqual([5, 1, 3, 6]);\n      expect(lpt.pathTo(7)).toEqual([5, 7]);\n    });","file":"shortest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should calculate distances","suites":["shortest path tree","shortest path tree normal operations"],"line":83,"updatePoint":{"line":83,"column":34,"index":3599},"code":"    it('should calculate distances', function () {\n      expect(lpt.getDistTo(0)).toBe(0.73);\n      expect(lpt.getDistTo(1)).toBe(0.32);\n      expect(lpt.getDistTo(2)).toBe(0.62);\n      expect(lpt.getDistTo(3)).toBe(0.61);\n      expect(lpt.getDistTo(4)).toBe(0.35);\n      expect(lpt.getDistTo(5)).toBe(0);\n      expect(lpt.getDistTo(6)).toBe(1.13);\n      expect(lpt.getDistTo(7)).toBe(0.28);\n    });","file":"shortest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should find the lightest weight path","suites":["shortest path tree","lightest weight path"],"line":99,"updatePoint":{"line":99,"column":44,"index":4108},"code":"    it('should find the lightest weight path', function () {\n      const digraph = new EdgeWeightedDigraph();\n      digraph.add(1, 3, 0.29);\n      digraph.add(1, 6, 0);\n      digraph.add(3, 6, 0);\n      const spt = new SPT(digraph, 1);\n      const path = spt.pathTo(6);\n      expect(path).toEqual([1, 6]);\n    });","file":"shortest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should find the lightest weight path","suites":["shortest path tree","lightest weight path"],"line":108,"updatePoint":{"line":108,"column":44,"index":4422},"code":"    it('should find the lightest weight path', function () {\n      const digraph = new EdgeWeightedDigraph();\n      digraph.add(1, 3, -1);\n      digraph.add(1, 6, 0);\n      digraph.add(3, 6, 0);\n      const spt = new SPT(digraph, 1);\n      const path = spt.pathTo(6);\n      const hasPath = spt.hasPathTo(6);\n      expect(hasPath).toBe(true);\n      expect(path).toEqual([1, 3, 6]);\n    });","file":"shortest_path_tree_spec.js","skipped":false,"dir":"spec"},{"name":"should replace B, F, P, and V with 1","suites":["soundex","transformLipps"],"line":28,"updatePoint":{"line":28,"column":44,"index":1261},"code":"    it('should replace B, F, P, and V with 1', function () {\n      expect(soundex.transformLipps('bopper')).toBe('1o11er');\n      expect(soundex.transformLipps('valumf')).toBe('1alum1');\n    });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should replace C, G, J, K, Q, S, X, Z with 2","suites":["soundex","transformThroats"],"line":34,"updatePoint":{"line":34,"column":52,"index":1515},"code":"    it('should replace C, G, J, K, Q, S, X, Z with 2', function () {\n      expect(soundex.transformLipps('bopper')).toBe('1o11er');\n      expect(soundex.transformLipps('valumf')).toBe('1alum1');\n    });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should replace D and T with 3","suites":["soundex","transformTongue"],"line":40,"updatePoint":{"line":40,"column":37,"index":1753},"code":"    it('should replace D and T with 3', function () {\n      expect(soundex.transformToungue('dat')).toBe('3a3');\n    });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should replace L with 4","suites":["soundex","transformL"],"line":45,"updatePoint":{"line":45,"column":31,"index":1913},"code":"    it('should replace L with 4', function () {\n      expect(soundex.transformL('lala')).toBe('4a4a');\n    });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should replace M and N with 5","suites":["soundex","transformHum"],"line":50,"updatePoint":{"line":50,"column":37,"index":2077},"code":"    it('should replace M and N with 5', function () {\n      expect(soundex.transformHum('mummification')).toBe('5u55ificatio5');\n    });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should replace R with 6","suites":["soundex","transformR"],"line":55,"updatePoint":{"line":55,"column":31,"index":2253},"code":"    it('should replace R with 6', function () {\n      expect(soundex.transformR('render')).toBe('6ende6');\n    });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"sound condense sequences","suites":["soundex","condense"],"line":60,"updatePoint":{"line":60,"column":32,"index":2412},"code":"    it('sound condense sequences', function () {\n      expect(soundex.condense('11222556')).toBe('1256');\n    });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"sound padd zeros on the right to a lenght of 3","suites":["soundex","padRight0"],"line":65,"updatePoint":{"line":65,"column":54,"index":2592},"code":"    it('sound padd zeros on the right to a lenght of 3', function () {\n      expect(soundex.padRight0('1')).toBe('100');\n      expect(soundex.padRight0('11')).toBe('110');\n      expect(soundex.padRight0('111')).toBe('111');\n    });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should not code the first character","suites":["soundex","padRight0"],"line":71,"updatePoint":{"line":71,"column":41,"index":2817},"code":"  it('should not code the first character', function () {\n    expect(soundex.process('render').charAt(0)).toBe('R');\n  });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should pad right with zeros","suites":["soundex","padRight0"],"line":74,"updatePoint":{"line":74,"column":33,"index":2932},"code":"  it('should pad right with zeros', function () {\n    expect(soundex.process('super')).toBe('S160');\n    expect(soundex.process('butt')).toBe('B300');\n    expect(soundex.process('a')).toBe('A000');\n  });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should pad right with zeros","suites":["soundex","padRight0"],"line":79,"updatePoint":{"line":79,"column":33,"index":3136},"code":"  it('should pad right with zeros', function () {\n    expect(soundex.process('but')).toBe('B300');\n  });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should perform soundex","suites":["soundex","padRight0"],"line":82,"updatePoint":{"line":82,"column":28,"index":3236},"code":"  it('should perform soundex', function () {\n    expect(soundex.process('BLACKBERRY')).toBe('B421');\n    expect(soundex.process('blackberry')).toBe('B421');\n    expect(soundex.process('calculate')).toBe('C424');\n    expect(soundex.process('CALCULATE')).toBe('C424');\n    expect(soundex.process('fox')).toBe('F200');\n    expect(soundex.process('FOX')).toBe('F200');\n    expect(soundex.process('jump')).toBe('J510');\n    expect(soundex.process('JUMP')).toBe('J510');\n    expect(soundex.process('phonetics')).toBe('P532');\n    expect(soundex.process('PHONETICS')).toBe('P532');\n  });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should perform soundex via compare method","suites":["soundex","padRight0"],"line":94,"updatePoint":{"line":94,"column":47,"index":3836},"code":"  it('should perform soundex via compare method', function () {\n    expect(soundex.compare('ant', 'and')).toBeTruthy();\n    expect(soundex.compare('ant', 'anne')).toBeFalsy();\n    expect(soundex.compare('band', 'bant')).toBeTruthy();\n    expect(soundex.compare('band', 'gand')).toBeFalsy();\n  });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should max out at four characters long by default","suites":["soundex","padRight0"],"line":132,"updatePoint":{"line":132,"column":55,"index":5565},"code":"  it('should max out at four characters long by default', function () {\n    expect(soundex.process('supercalifragilisticexpialidocious').length).toBe(4);\n  });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should truncate to specified length if maxLength passed","suites":["soundex","padRight0"],"line":135,"updatePoint":{"line":135,"column":61,"index":5731},"code":"  it('should truncate to specified length if maxLength passed', function () {\n    expect(soundex.process('supercalifragilisticexpialidocious', 8).length).toBe(8);\n  });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should handle a maxLength beyond code length","suites":["soundex","padRight0"],"line":138,"updatePoint":{"line":138,"column":50,"index":5889},"code":"  it('should handle a maxLength beyond code length', function () {\n    expect(soundex.process('JUMP', 8)).toBe('J510');\n  });","file":"soundex_spec.js","skipped":false,"dir":"spec"},{"name":"should fix deletions","suites":["spellcheck","distance 1 corrections"],"line":7,"updatePoint":{"line":7,"column":28,"index":257},"code":"    it('should fix deletions', function () {\n      expect(spellcheck.getCorrections('smething')).toContain('something');\n    });","file":"spellcheck_spec.js","skipped":false,"dir":"spec"},{"name":"should fix insertions","suites":["spellcheck","distance 1 corrections"],"line":10,"updatePoint":{"line":10,"column":29,"index":387},"code":"    it('should fix insertions', function () {\n      expect(spellcheck.getCorrections('somethhing')).toContain('something');\n    });","file":"spellcheck_spec.js","skipped":false,"dir":"spec"},{"name":"should fix transpositions","suites":["spellcheck","distance 1 corrections"],"line":13,"updatePoint":{"line":13,"column":33,"index":523},"code":"    it('should fix transpositions', function () {\n      expect(spellcheck.getCorrections('somehting')).toContain('something');\n    });","file":"spellcheck_spec.js","skipped":false,"dir":"spec"},{"name":"should fix replacements","suites":["spellcheck","distance 1 corrections"],"line":16,"updatePoint":{"line":16,"column":31,"index":656},"code":"    it('should fix replacements', function () {\n      expect(spellcheck.getCorrections('sometzing')).toContain('something');\n    });","file":"spellcheck_spec.js","skipped":false,"dir":"spec"},{"name":"should fix a deletion then a transposition","suites":["spellcheck","distance 2 corrections"],"line":22,"updatePoint":{"line":22,"column":50,"index":919},"code":"    it('should fix a deletion then a transposition', function () {\n      expect(spellcheck.getCorrections('smtehing', 2)).toContain('something');\n    });","file":"spellcheck_spec.js","skipped":false,"dir":"spec"},{"name":"should compute insertions","suites":["spellcheck","edits"],"line":29,"updatePoint":{"line":29,"column":33,"index":1181},"code":"    it('should compute insertions', function () {\n      const insertions = ['xab', 'axb', 'abx'];\n      for (const i in insertions) {\n        expect(edits.indexOf(insertions[i])).toBeGreaterThan(-1);\n      }\n    });","file":"spellcheck_spec.js","skipped":false,"dir":"spec"},{"name":"should compute deletions","suites":["spellcheck","edits"],"line":35,"updatePoint":{"line":35,"column":32,"index":1396},"code":"    it('should compute deletions', function () {\n      const deletions = ['a', 'b'];\n      for (const i in deletions) {\n        expect(edits.indexOf(deletions[i])).toBeGreaterThan(-1);\n      }\n    });","file":"spellcheck_spec.js","skipped":false,"dir":"spec"},{"name":"should compute transpositions","suites":["spellcheck","edits"],"line":41,"updatePoint":{"line":41,"column":37,"index":1602},"code":"    it('should compute transpositions', function () {\n      const transpositions = ['ba'];\n      for (const i in transpositions) {\n        expect(edits.indexOf(transpositions[i])).toBeGreaterThan(-1);\n      }\n    });","file":"spellcheck_spec.js","skipped":false,"dir":"spec"},{"name":"should compute replacements","suites":["spellcheck","edits"],"line":47,"updatePoint":{"line":47,"column":35,"index":1817},"code":"    it('should compute replacements', function () {\n      const replacements = ['zb', 'af'];\n      for (const i in replacements) {\n        expect(edits.indexOf(replacements[i])).toBeGreaterThan(-1);\n      }\n    });","file":"spellcheck_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly compute edits at distance","suites":["spellcheck","edits up to distance"],"line":56,"updatePoint":{"line":56,"column":50,"index":2145},"code":"    it('should correctly compute edits at distance', function () {\n      const edits = spellcheck.editsWithMaxDistance('abc', 2);\n      expect(edits[1].indexOf('abzc')).toBeGreaterThan(-1); // 1 insertion\n      expect(edits[2].indexOf('a')).toBeGreaterThan(-1); // 2 deletions\n    });","file":"spellcheck_spec.js","skipped":false,"dir":"spec"},{"name":"should consider cat a word","suites":["spellcheck","boolean spellcheck"],"line":65,"updatePoint":{"line":65,"column":34,"index":2516},"code":"    it('should consider cat a word', function () {\n      expect(spellcheck.isCorrect('cat')).toBe(true);\n    });","file":"spellcheck_spec.js","skipped":false,"dir":"spec"},{"name":"should not consider dog a word","suites":["spellcheck","boolean spellcheck"],"line":68,"updatePoint":{"line":68,"column":38,"index":2633},"code":"    it('should not consider dog a word', function () {\n      expect(spellcheck.isCorrect('dog')).toBe(false);\n    });","file":"spellcheck_spec.js","skipped":false,"dir":"spec"},{"name":"should return the input word as the most probable correction if it is already correct","suites":["spellcheck","special cases"],"line":74,"updatePoint":{"line":74,"column":93,"index":2902},"code":"    it('should return the input word as the most probable correction if it is already correct', function () {\n      expect(spellcheck.getCorrections('cat').indexOf('cat')).toBe(0);\n    });","file":"spellcheck_spec.js","skipped":false,"dir":"spec"},{"name":"should stem words","suites":["StemmerJa"],"line":30,"updatePoint":{"line":30,"column":23,"index":1390},"code":"  it('should stem words', function () {\n    for (let i = 0; i < test.length; i++) {\n      expect(stemmer.stem(test[i])).toBe(testResult[i]);\n    }\n  });","file":"stemmer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should not tokenize halfwidth katakana","suites":["StemmerJa"],"line":35,"updatePoint":{"line":35,"column":44,"index":1564},"code":"  it('should not tokenize halfwidth katakana', function () {\n    expect(stemmer.stem('ﾀｸｼｰ')).toEqual('ﾀｸｼｰ');\n  });","file":"stemmer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize, stem and exclude stop words (default behavior)","suites":["StemmerJa"],"line":38,"updatePoint":{"line":38,"column":69,"index":1706},"code":"  it('should tokenize, stem and exclude stop words (default behavior)', function () {\n    const tokens = ['明後日', 'パーティ', '行く', '予定',\n    // パーティー should be stemmed\n    '図書館', '資料', 'コピー', 'まし'];\n    expect(stemmer.tokenizeAndStem(text)).toEqual(tokens);\n    expect(stemmer.tokenizeAndStem(text, false)).toEqual(tokens);\n  });","file":"stemmer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize, stem and keep stop words","suites":["StemmerJa"],"line":45,"updatePoint":{"line":45,"column":47,"index":2010},"code":"  it('should tokenize, stem and keep stop words', function () {\n    expect(stemmer.tokenizeAndStem(text, true)).toEqual(['明後日', 'パーティ', 'に', '行く', '予定', 'が', 'ある', '図書館', 'で', '資料', 'を', 'コピー', 'し', 'まし', 'た']);\n  });","file":"stemmer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should receive a string","suites":["stemmer token"],"line":27,"updatePoint":{"line":27,"column":29,"index":1214},"code":"  it('should receive a string', function () {\n    const string = 'test';\n    const token = new StemmerToken(string);\n    expect(token.string).toBe(string);\n  });","file":"stemmer_token_spec.js","skipped":false,"dir":"spec"},{"name":"should hold the original token string","suites":["stemmer token"],"line":32,"updatePoint":{"line":32,"column":43,"index":1390},"code":"  it('should hold the original token string', function () {\n    const string = 'test';\n    const token = new StemmerToken(string);\n    expect(token.original).toBe(string);\n  });","file":"stemmer_token_spec.js","skipped":false,"dir":"spec"},{"name":"should replace all instances of a string","suites":["stemmer token"],"line":37,"updatePoint":{"line":37,"column":46,"index":1571},"code":"  it('should replace all instances of a string', function () {\n    const string = 'tester';\n    const token = new StemmerToken(string);\n    token.replaceAll('e', 'a');\n    expect(token.string).toBe('tastar');\n    token.replaceAll('ar', 'er');\n    expect(token.string).toBe('taster');\n    token.replaceAll('r', '');\n    expect(token.string).toBe('taste');\n  });","file":"stemmer_token_spec.js","skipped":false,"dir":"spec"},{"name":"should allow vowels to be set","suites":["stemmer token"],"line":47,"updatePoint":{"line":47,"column":35,"index":1921},"code":"  it('should allow vowels to be set', function () {\n    const vowels = 'aeiou';\n    const vowelsArray = vowels.split('');\n    const token = new StemmerToken('');\n    token.usingVowels(vowels);\n    expect(token.vowels).toBe(vowels);\n    token.usingVowels(vowelsArray);\n    expect(token.vowels).toBe(vowelsArray);\n  });","file":"stemmer_token_spec.js","skipped":false,"dir":"spec"},{"name":"should check for vowels","suites":["stemmer token"],"line":56,"updatePoint":{"line":56,"column":29,"index":2233},"code":"  it('should check for vowels', function () {\n    const vowels = 'aeiou';\n    const token = new StemmerToken('test');\n    token.usingVowels(vowels);\n    expect(token.hasVowelAtIndex(0)).toBe(false);\n    expect(token.hasVowelAtIndex(1)).toBe(true);\n    expect(token.hasVowelAtIndex(99)).toBe(false);\n    token.usingVowels(vowels.split(''));\n    expect(token.hasVowelAtIndex(0)).toBe(false);\n    expect(token.hasVowelAtIndex(1)).toBe(true);\n    expect(token.hasVowelAtIndex(99)).toBe(false);\n  });","file":"stemmer_token_spec.js","skipped":false,"dir":"spec"},{"name":"should find the next vowel","suites":["stemmer token"],"line":68,"updatePoint":{"line":68,"column":32,"index":2732},"code":"  it('should find the next vowel', function () {\n    const token = new StemmerToken('tester').usingVowels('aeiou');\n    expect(token.nextVowelIndex(0)).toBe(1);\n    expect(token.nextVowelIndex(1)).toBe(1);\n    expect(token.nextVowelIndex(2)).toBe(4);\n    expect(token.nextVowelIndex(5)).toBe(6);\n    expect(token.nextVowelIndex(99)).toBe(6);\n    expect(token.nextVowelIndex(-1)).toBe(6);\n  });","file":"stemmer_token_spec.js","skipped":false,"dir":"spec"},{"name":"should find the next consonant","suites":["stemmer token"],"line":77,"updatePoint":{"line":77,"column":36,"index":3130},"code":"  it('should find the next consonant', function () {\n    const token = new StemmerToken('testee').usingVowels('aeiou');\n    expect(token.nextConsonantIndex(0)).toBe(0);\n    expect(token.nextConsonantIndex(1)).toBe(2);\n    expect(token.nextConsonantIndex(5)).toBe(6);\n    expect(token.nextConsonantIndex(99)).toBe(6);\n    expect(token.nextConsonantIndex(-1)).toBe(6);\n  });","file":"stemmer_token_spec.js","skipped":false,"dir":"spec"},{"name":"should mark regions","suites":["stemmer token"],"line":85,"updatePoint":{"line":85,"column":25,"index":3492},"code":"  it('should mark regions', function () {\n    const token = new StemmerToken('tester');\n    token.markRegion('test', 1);\n    expect(token.regions.test).toBe(1);\n  });","file":"stemmer_token_spec.js","skipped":false,"dir":"spec"},{"name":"should mark regions with a callback","suites":["stemmer token"],"line":90,"updatePoint":{"line":90,"column":41,"index":3675},"code":"  it('should mark regions with a callback', function () {\n    const token = new StemmerToken('tester');\n    const context = {\n      value: 99\n    };\n    token.markRegion('test', 1, function (arg) {\n      return arg;\n    });\n    expect(token.regions.test).toBe(1);\n    token.markRegion('test', [1], function (arg) {\n      return arg;\n    });\n    expect(token.regions.test).toBe(1);\n    token.markRegion('test', [1, 1], function (a1, a2) {\n      return a1 + a2;\n    });\n    expect(token.regions.test).toBe(2);\n    token.markRegion('test', null, function () {\n      return this.string.length;\n    });\n    expect(token.regions.test).toBe(6);\n    token.markRegion('test', null, function () {\n      return this.value;\n    }, context);\n    expect(token.regions.test).toBe(99);\n  });","file":"stemmer_token_spec.js","skipped":false,"dir":"spec"},{"name":"should check for suffixes","suites":["stemmer token"],"line":116,"updatePoint":{"line":116,"column":31,"index":4441},"code":"  it('should check for suffixes', function () {\n    const token = new StemmerToken('tester');\n    expect(token.hasSuffix('er')).toBe(true);\n    expect(token.hasSuffix('st')).toBe(false);\n  });","file":"stemmer_token_spec.js","skipped":false,"dir":"spec"},{"name":"should check for suffixes within a region","suites":["stemmer token"],"line":121,"updatePoint":{"line":121,"column":47,"index":4650},"code":"  it('should check for suffixes within a region', function () {\n    const token = new StemmerToken('tester').markRegion('region', 2);\n    expect(token.hasSuffixInRegion('st', 'region')).toBe(false);\n    expect(token.hasSuffixInRegion('ster', 'region')).toBe(true);\n    expect(token.hasSuffixInRegion('ester', 'region')).toBe(false);\n  });","file":"stemmer_token_spec.js","skipped":false,"dir":"spec"},{"name":"should replace the suffix within a region","suites":["stemmer token"],"line":127,"updatePoint":{"line":127,"column":47,"index":4989},"code":"  it('should replace the suffix within a region', function () {\n    const t1 = new StemmerToken('tester').markRegion('region', 4);\n    const t2 = new StemmerToken('tester').markRegion('region', 0);\n    t1.replaceSuffixInRegion('ter', '<s>', 'region');\n    expect(t1.string).toBe('tester');\n    t1.replaceSuffixInRegion('er', '<s>', 'region');\n    expect(t1.string).toBe('test<s>');\n    t2.replaceSuffixInRegion('protester', '<s>', 'region');\n    expect(t2.string).toBe('tester');\n  });","file":"stemmer_token_spec.js","skipped":false,"dir":"spec"},{"name":"should tf","suites":["tfidf","stateless operations"],"line":29,"updatePoint":{"line":29,"column":17,"index":1252},"code":"    it('should tf', function () {\n      expect(TfIdf.tf('document', {\n        document: 2,\n        one: 1\n      })).toBe(2);\n      expect(TfIdf.tf('document', {\n        greetings: 1,\n        program: 1\n      })).toBe(0);\n      expect(TfIdf.tf('program', {\n        greetings: 1,\n        program: 1\n      })).toBe(1);\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should store and recall keys","suites":["tfidf","keys"],"line":45,"updatePoint":{"line":45,"column":36,"index":1634},"code":"    it('should store and recall keys', function () {\n      tfidf = new TfIdf();\n      tfidf.addDocument('document one', 'un');\n      tfidf.addDocument('document Two', 'deux');\n      tfidf.tfidfs('two', function (i, tfidf, key) {\n        if (i === 0) {\n          expect(key).toBe('un');\n        } else {\n          expect(key).toBe('deux');\n        }\n      });\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should handle a deserialized object passed to the constructor","suites":["tfidf","keys"],"line":57,"updatePoint":{"line":57,"column":69,"index":2034},"code":"    it('should handle a deserialized object passed to the constructor', function () {\n      tfidf = new TfIdf({\n        documents: [{\n          __key: 'un',\n          document: 1,\n          one: 1\n        }, {\n          __key: 'deux',\n          document: 1,\n          two: 1\n        }]\n      });\n      tfidf.tfidfs('two', function (i, tfidf, key) {\n        if (i === 1) {\n          expect(key).toBe('deux');\n        } else {\n          expect(key).toBe('un');\n        }\n      });\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should work when called without a callback","suites":["tfidf","keys"],"line":77,"updatePoint":{"line":77,"column":50,"index":2502},"code":"    it('should work when called without a callback', function () {\n      tfidf = new TfIdf({\n        documents: [{\n          __key: 'un',\n          document: 1,\n          one: 1\n        }, {\n          __key: 'deux',\n          document: 1,\n          two: 1\n        }]\n      });\n      const tfidfs = tfidf.tfidfs('two');\n      expect(tfidfs[1]).toBe(1 + Math.log(2.0 / 2.0));\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should work with the restoreCache flag set to true","suites":["tfidf","keys"],"line":92,"updatePoint":{"line":92,"column":58,"index":2892},"code":"    it('should work with the restoreCache flag set to true', function () {\n      tfidf = new TfIdf();\n      tfidf.addDocument('document one', 'un');\n      expect(tfidf.idf('one')).toBe(1 + Math.log(1.0 / 2.0));\n      tfidf.addDocument('document Two', 'deux', true);\n      tfidf.tfidfs('two', function (i, tfidf, key) {\n        if (i === 0) {\n          expect(key).toBe('un');\n        } else {\n          expect(key).toBe('deux');\n        }\n      });\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should list important terms","suites":["tfidf","stateful operations"],"line":112,"updatePoint":{"line":112,"column":35,"index":3526},"code":"    it('should list important terms', function () {\n      const terms = tfidf.listTerms(0);\n      expect(terms[0].tfidf).toBeGreaterThan(terms[1].tfidf);\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should handle reserved function names correctly in documents","suites":["tfidf","special cases"],"line":119,"updatePoint":{"line":119,"column":68,"index":3791},"code":"    it('should handle reserved function names correctly in documents', function () {\n      const reservedWords = ['toString', 'toLocaleString', 'valueOf', 'hasOwnProperty', 'isPrototypeOf', 'propertyIsEnumerable', 'constructor'];\n      tfidf = new TfIdf();\n      tfidf.addDocument(reservedWords.join(' '));\n      for (const i in reservedWords) {\n        expect(tfidf.tfidf(reservedWords[i], 0)).toBe(1 * (1 + Math.log(1 / 2)));\n      }\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should handle an array passed to tfidf()","suites":["tfidf","special cases"],"line":127,"updatePoint":{"line":127,"column":48,"index":4215},"code":"    it('should handle an array passed to tfidf()', function () {\n      tfidf = new TfIdf();\n      const terms = ['this', 'document', 'is', 'about', 'poetry'];\n      tfidf.addDocument(terms.join(' '));\n      expect(tfidf.tfidf(terms, 0)).toBe(2 * (1 + Math.log(1.0 / 2.0)));\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should compute idf correctly","suites":["tfidf","correct calculations"],"line":135,"updatePoint":{"line":135,"column":36,"index":4540},"code":"    it('should compute idf correctly', function () {\n      tfidf = new TfIdf();\n      tfidf.addDocument('this document is about node.');\n      tfidf.addDocument('this document is about ruby.');\n      tfidf.addDocument('this document is about ruby and node.');\n      tfidf.addDocument('this document is about node. it has node examples');\n      expect(tfidf.idf('node')).toBe(1 + Math.log(4.0 / 4.0));\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should compute idf correctly with non-string documents","suites":["tfidf","correct calculations"],"line":143,"updatePoint":{"line":143,"column":62,"index":4975},"code":"    it('should compute idf correctly with non-string documents', function () {\n      tfidf = new TfIdf();\n      tfidf.addDocument('this document is about node.');\n      tfidf.addDocument('this document is about ruby.');\n      tfidf.addDocument('this document is about ruby and node.');\n      tfidf.addDocument('this document is about node. it has node examples');\n      tfidf.addDocument({\n        text: 'this document is about python'\n      });\n      tfidf.addDocument(['this', 'document', 'is', 'about', 'node', 'and', 'JavaScript']);\n      expect(tfidf.idf('node')).toBe(1 + Math.log(6.0 / 5.0));\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should compute tf correctly","suites":["tfidf","correct calculations"],"line":155,"updatePoint":{"line":155,"column":35,"index":5556},"code":"    it('should compute tf correctly', function () {\n      expect(TfIdf.tf('node', {\n        this: 1,\n        document: 1,\n        is: 1,\n        about: 1,\n        node: 1\n      })).toBe(1);\n      expect(TfIdf.tf('node', {\n        this: 1,\n        document: 1,\n        is: 1,\n        about: 1,\n        ruby: 1\n      })).toBe(0);\n      expect(TfIdf.tf('node', {\n        this: 1,\n        document: 1,\n        is: 1,\n        about: 1,\n        ruby: 1,\n        and: 1,\n        node: 1\n      })).toBe(1);\n      expect(TfIdf.tf('node', {\n        this: 1,\n        document: 1,\n        is: 1,\n        about: 1,\n        node: 2,\n        it: 1,\n        has: 1,\n        examples: 1\n      })).toBe(2);\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should compute tf-idf correctly","suites":["tfidf","correct calculations"],"line":192,"updatePoint":{"line":192,"column":39,"index":6320},"code":"    it('should compute tf-idf correctly', function () {\n      const correctCalculations = [1 * (1 + Math.log(4.0 / 4.0)), 0, 2 * (1 + Math.log(4.0 / 4.0)), 1 * (1 + Math.log(4.0 / 3.0))];\n      tfidf = new TfIdf();\n      tfidf.addDocument('this document is about node.', {\n        node: 0,\n        ruby: 1\n      });\n      tfidf.addDocument('this document is about ruby.', {\n        node: 1,\n        ruby: 3\n      });\n      tfidf.addDocument('this document is about ruby and node.', {\n        node: 0,\n        ruby: 3\n      });\n      tfidf.addDocument('this document is about node. it has node examples', {\n        node: 2,\n        ruby: 1\n      });\n      tfidf.tfidfs('node', function (i, measure, k) {\n        expect(measure).toBe(correctCalculations[k.node]);\n      });\n      tfidf.tfidfs('ruby', function (i, measure, k) {\n        expect(measure).toBe(correctCalculations[k.ruby]);\n      });\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should not return NaN if a term is not present in any documents","suites":["tfidf","correct calculations"],"line":218,"updatePoint":{"line":218,"column":71,"index":7255},"code":"    it('should not return NaN if a term is not present in any documents', function () {\n      tfidf = new TfIdf();\n      tfidf.addDocument('this document is about node.');\n      expect(tfidf.tfidf('ruby', 0)).toBe(0);\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should update a terms tf-idf score after adding documents","suites":["tfidf","correct calculations"],"line":227,"updatePoint":{"line":227,"column":65,"index":7707},"code":"    it('should update a terms tf-idf score after adding documents', function () {\n      tfidf = new TfIdf();\n\n      // Add 2 documents\n      tfidf.addDocument('this document is about node.', 0);\n      tfidf.addDocument('this document is about ruby.', 1);\n\n      // check the tf-idf for 'node'\n      expect(tfidf.tfidf('node', 0)).toBe(1 * (1 + Math.log(2.0 / 2.0)));\n\n      // Add 2 more documents\n      tfidf.addDocument('this document is about ruby and node.');\n      tfidf.addDocument('this document is about node. it has node examples');\n\n      // Ensure that the tf-idf in the same document has changed to reflect the new idf.\n      expect(tfidf.tfidf('node', 0)).toBe(1 * (1 + Math.log(4.0 / 4.0)));\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should allow for specific types of Tokenizers","suites":["tfidf","correct calculations"],"line":246,"updatePoint":{"line":246,"column":53,"index":8439},"code":"    it('should allow for specific types of Tokenizers', function () {\n      tfidf = new TfIdf();\n      tfidf.addDocument('this document isn\\'t about node.', 0);\n      tfidf.addDocument('that doc is about node.', 1);\n      expect(tfidf.tfidf('n\\'t', 0)).toBe(0);\n      expect(tfidf.tfidf('isn', 0)).toBe(1 * (1 + Math.log(2 / 2)));\n      tfidf = new TfIdf();\n      tfidf.addDocument('this document isn\\'t about node.', 0);\n      tfidf.addDocument('this document isn\\'t about node.', 1);\n      expect(tfidf.tfidf('isn', 0)).toBe(1 * (1 + Math.log(2 / 3)));\n      tfidf = new TfIdf();\n      const TreebankWordTokenizer = require('../lib/natural/tokenizers/treebank_word_tokenizer');\n      const tokenizer = new TreebankWordTokenizer();\n      tfidf.addDocument('this document isn\\'t about node.', 0);\n      tfidf.setTokenizer(tokenizer);\n      tfidf.addDocument('this doc isn\\'t about node.', 1);\n      expect(tfidf.tfidf('isn', 0)).toBe(1 * (1 + Math.log(2 / 2)));\n      expect(tfidf.tfidf('n\\'t', 1)).toBe(1 * (1 + Math.log(2 / 2)));\n      expect(tfidf.tfidf('isn', 1)).toBe(0);\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should require a valid tokenizer when using setTokenizer","suites":["tfidf","correct calculations"],"line":266,"updatePoint":{"line":266,"column":64,"index":9535},"code":"    it('should require a valid tokenizer when using setTokenizer', function () {\n      tfidf = new TfIdf();\n      expect(function () {\n        tfidf.setTokenizer(1);\n      }).toThrow(new Error('Expected a valid Tokenizer'));\n      expect(function () {\n        tfidf.setTokenizer({});\n      }).toThrow(new Error('Expected a valid Tokenizer'));\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should load a custom set of stopwords","suites":["tfidf","Stopwords"],"line":277,"updatePoint":{"line":277,"column":45,"index":9911},"code":"    it('should load a custom set of stopwords', function () {\n      tfidf = new TfIdf();\n      const stopwords = require('lib/natural/util/stopwords').words;\n      expect(tfidf.setStopwords(stopwords)).toEqual(true);\n      tfidf.addDocument('this document is about node.', 0);\n      const terms = tfidf.listTerms(0);\n      const tokens = terms.map(t => t.term);\n      expect(tokens.indexOf('about')).toEqual(-1);\n      expect(tokens.indexOf('this')).toEqual(-1);\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should detect an incorrect stopwords list (not an array)","suites":["tfidf","Stopwords"],"line":287,"updatePoint":{"line":287,"column":64,"index":10401},"code":"    it('should detect an incorrect stopwords list (not an array)', function () {\n      const stopwords = {};\n      expect(tfidf.setStopwords(stopwords)).toEqual(false);\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should detect an incorrect stopwords list (one of the elements is not a string)","suites":["tfidf","Stopwords"],"line":291,"updatePoint":{"line":291,"column":87,"index":10601},"code":"    it('should detect an incorrect stopwords list (one of the elements is not a string)', function () {\n      const stopwords = [function f() {}];\n      expect(tfidf.setStopwords(stopwords)).toEqual(false);\n    });","file":"tfidf_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize numbers","suites":["case_tokenizer_numbers"],"line":28,"updatePoint":{"line":28,"column":29,"index":1298},"code":"  it('should tokenize numbers', function () {\n    expect(tokenizer.tokenize('0 1 2 3 4 5 6 7 8 9 10')).toEqual(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["case_tokenizer_es"],"line":33,"updatePoint":{"line":33,"column":29,"index":1522},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('hola yo me llamo eduardo y esudié ingeniería')).toEqual(['hola', 'yo', 'me', 'llamo', 'eduardo', 'y', 'esudié', 'ingeniería']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["case_tokenizer_fr"],"line":48,"updatePoint":{"line":48,"column":29,"index":2981},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize(text)).toEqual(tokenized);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["case_tokenizer_nl"],"line":61,"updatePoint":{"line":61,"column":29,"index":3300},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('\\'s Morgens is het nog erg koud, vertelde de weerman over een van de radio\\'s', true)).toEqual(['\\'s', 'Morgens', 'is', 'het', 'nog', 'erg', 'koud', 'vertelde', 'de', 'weerman', 'over', 'een', 'van', 'de', 'radio\\'s']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["case_tokenizer_pt"],"line":74,"updatePoint":{"line":74,"column":29,"index":3991},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('isso é coração')).toEqual(['isso', 'é', 'coração']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow punctuation","suites":["case_tokenizer_pt"],"line":89,"updatePoint":{"line":89,"column":32,"index":4497},"code":"  it('should swallow punctuation', function () {\n    expect(tokenizer.tokenize('isso é coração, no')).toEqual(['isso', 'é', 'coração', 'no']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow final punctuation","suites":["case_tokenizer_pt"],"line":92,"updatePoint":{"line":92,"column":38,"index":4652},"code":"  it('should swallow final punctuation', function () {\n    expect(tokenizer.tokenize('isso é coração, no?')).toEqual(['isso', 'é', 'coração', 'no']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow initial punctuation","suites":["case_tokenizer_pt"],"line":95,"updatePoint":{"line":95,"column":40,"index":4810},"code":"  it('should swallow initial punctuation', function () {\n    expect(tokenizer.tokenize('.isso é coração, no')).toEqual(['isso', 'é', 'coração', 'no']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow duplicate punctuation","suites":["case_tokenizer_pt"],"line":98,"updatePoint":{"line":98,"column":42,"index":4970},"code":"  it('should swallow duplicate punctuation', function () {\n    expect(tokenizer.tokenize('eu vou... pause')).toEqual(['eu', 'vou', 'pause']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["case_tokenizer_aggressive_tokenizer"],"line":103,"updatePoint":{"line":103,"column":29,"index":5171},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('these are things')).toEqual(['these', 'are', 'things']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow punctuation","suites":["case_tokenizer_aggressive_tokenizer"],"line":118,"updatePoint":{"line":118,"column":32,"index":5689},"code":"  it('should swallow punctuation', function () {\n    expect(tokenizer.tokenize('these are things, no')).toEqual(['these', 'are', 'things', 'no']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow final punctuation","suites":["case_tokenizer_aggressive_tokenizer"],"line":121,"updatePoint":{"line":121,"column":38,"index":5848},"code":"  it('should swallow final punctuation', function () {\n    expect(tokenizer.tokenize('these are things, no?')).toEqual(['these', 'are', 'things', 'no']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow initial punctuation","suites":["case_tokenizer_aggressive_tokenizer"],"line":124,"updatePoint":{"line":124,"column":40,"index":6010},"code":"  it('should swallow initial punctuation', function () {\n    expect(tokenizer.tokenize('.these are things, no')).toEqual(['these', 'are', 'things', 'no']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should swallow duplicate punctuation","suites":["case_tokenizer_aggressive_tokenizer"],"line":127,"updatePoint":{"line":127,"column":42,"index":6174},"code":"  it('should swallow duplicate punctuation', function () {\n    expect(tokenizer.tokenize('i shal... pause')).toEqual(['i', 'shal', 'pause']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["case_tokenizer_it"],"line":132,"updatePoint":{"line":132,"column":29,"index":6357},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('Mi piacerebbe visitare l\\'Italia un giorno di questi!')).toEqual(['Mi', 'piacerebbe', 'visitare', 'l', 'Italia', 'un', 'giorno', 'di', 'questi']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["case_tokenizer_no"],"line":137,"updatePoint":{"line":137,"column":29,"index":6635},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('Gå rett fram. Så tek du til venstre/høgre.')).toEqual(['Gå', 'rett', 'fram', 'Så', 'tek', 'du', 'til', 'venstre', 'høgre']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["case_tokenizer_pl"],"line":145,"updatePoint":{"line":145,"column":29,"index":6947},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('Bardzo za tobą tęskniłem/tęskniłam!')).toEqual(['Bardzo', 'za', 'tobą', 'tęskniłem', 'tęskniłam']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["case_tokenizer_pt"],"line":150,"updatePoint":{"line":150,"column":29,"index":7178},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('Siga em frente, depois vire à esquerda/direita!')).toEqual(['Siga', 'em', 'frente', 'depois', 'vire', 'à', 'esquerda', 'direita']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["case_tokenizer_ru"],"line":155,"updatePoint":{"line":155,"column":29,"index":7441},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('Vy mOzhite mne pamOch? Вы можете мне помочь?')).toEqual(['Vy', 'mOzhite', 'mne', 'pamOch', 'Вы', 'можете', 'мне', 'помочь']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize strings","suites":["case_tokenizer_fi"],"line":160,"updatePoint":{"line":160,"column":29,"index":7698},"code":"  it('should tokenize strings', function () {\n    expect(tokenizer.tokenize('Mene suoraan käänny sitten vasempaan/oikeaan!')).toEqual(['Mene', 'suoraan', 'käänny', 'sitten', 'vasempaan', 'oikeaan']);\n  });","file":"tokenizer_case_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize","suites":["TokenizerJa"],"line":30,"updatePoint":{"line":30,"column":21,"index":2010},"code":"  it('should tokenize', function () {\n    const tokens = tokenizer.tokenize(text);\n    expect(tokens).toEqual(result);\n\n    // This test is very hard to pass through, so we comment for now.\n    // tokens = tokenizer.tokenize('すもももももももものうち。');\n    // expect(tokens).toEqual(['すもも', 'も', 'もも', 'も', 'もも', 'の', 'うち', '。']);\n  });","file":"tokenizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should normalize input","suites":["TokenizerJa"],"line":39,"updatePoint":{"line":39,"column":28,"index":2345},"code":"  it('should normalize input', function () {\n    const converters = require('../lib/natural/normalizers/normalizer_ja').converters;\n    const tokens = tokenizer.tokenize(converters.halfwidthToFullwidth.alphabet(converters.halfwidthToFullwidth.numbers(converters.fullwidthToHalfwidth.punctuation(converters.fullwidthToHalfwidth.katakana(text)))));\n    expect(tokens).toEqual(result);\n  });","file":"tokenizer_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transliterate hiragana","suites":["transliterateJa"],"line":27,"updatePoint":{"line":27,"column":35,"index":1230},"code":"  it('should transliterate hiragana', function () {\n    expect(transliterateJa('あいうえお かきくけこ')).toEqual('aiueo kakikukeko');\n    expect(transliterateJa('さしすせそ たちつてと')).toEqual('sashisuseso tachitsuteto');\n  });","file":"transliterator_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transliterate katakana","suites":["transliterateJa"],"line":31,"updatePoint":{"line":31,"column":35,"index":1440},"code":"  it('should transliterate katakana', function () {\n    expect(transliterateJa('アイウエオ カキクケコ')).toEqual('aiueo kakikukeko');\n    expect(transliterateJa('サシスセソ タチツテト')).toEqual('sashisuseso tachitsuteto');\n  });","file":"transliterator_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transliterate small tsu differently depending on context","suites":["transliterateJa"],"line":35,"updatePoint":{"line":35,"column":69,"index":1684},"code":"  it('should transliterate small tsu differently depending on context', function () {\n    expect(transliterateJa('まっか ざっし たった はっぱ')).toEqual('makka zasshi tatta happa');\n    expect(transliterateJa('マッカ ザッシ タッタ ハッパ')).toEqual('makka zasshi tatta happa');\n  });","file":"transliterator_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transliterate final small tsu","suites":["transliterateJa"],"line":39,"updatePoint":{"line":39,"column":42,"index":1917},"code":"  it('should transliterate final small tsu', function () {\n    expect(transliterateJa('ああっ')).toEqual('aat');\n    expect(transliterateJa('アアッ')).toEqual('aat');\n  });","file":"transliterator_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transliterate n differently depending on context","suites":["transliterateJa"],"line":43,"updatePoint":{"line":43,"column":61,"index":2103},"code":"  it('should transliterate n differently depending on context', function () {\n    expect(transliterateJa('まんと ばんび ほんや')).toEqual(\"manto bambi hon'ya\");\n    expect(transliterateJa('マント バンビ ホンヤ')).toEqual(\"manto bambi hon'ya\");\n  });","file":"transliterator_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transliterate long vowels","suites":["transliterateJa"],"line":47,"updatePoint":{"line":47,"column":38,"index":2312},"code":"  it('should transliterate long vowels', function () {\n    expect(transliterateJa('ああ　いい　うう　ええ　おお　おう')).toEqual('aa　ii　ū　ee　oo　ō');\n    expect(transliterateJa('あー　いー　うー　えー　おー')).toEqual('ā　ī　ū　ē　ō');\n    expect(transliterateJa('アア　イイ　ウウ　エエ　オオ　オウ')).toEqual('aa　ii　ū　ee　oo　ō');\n    expect(transliterateJa('アー　イー　ウー　エー　オー')).toEqual('ā　ī　ū　ē　ō');\n  });","file":"transliterator_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should leave non fullwidth kana unchanged","suites":["transliterateJa"],"line":53,"updatePoint":{"line":53,"column":47,"index":2672},"code":"  it('should leave non fullwidth kana unchanged', function () {\n    expect(transliterateJa('abc ABC 漢字 (.)')).toEqual('abc ABC 漢字 (.)');\n    expect(transliterateJa('ｱｲｳｴｵ ｶｷｸｹｺ')).toEqual('ｱｲｳｴｵ ｶｷｸｹｺ');\n  });","file":"transliterator_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should transliterate misc. words correctly","suites":["transliterateJa"],"line":57,"updatePoint":{"line":57,"column":48,"index":2883},"code":"  it('should transliterate misc. words correctly', function () {\n    expect(transliterateJa('アヴァンギャルド')).toEqual('avangyarudo'); // Avant-garde\n    expect(transliterateJa('アクサン・スィルコンフレックス')).toEqual('akusan sirukonfurekkusu'); // Accent circonflexe\n    expect(transliterateJa('アドレシッング')).toEqual('adoreshinngu'); // Addressing\n    expect(transliterateJa('イェス')).toEqual('yesu'); // Yes\n    expect(transliterateJa('インテリジェンス')).toEqual('interijensu'); // Intelligence\n    expect(transliterateJa('インテルメッツォ')).toEqual('interumettso'); // Intermezzo\n    expect(transliterateJa('グァテマラ')).toEqual('gwatemara'); // Guatemala\n    expect(transliterateJa('クァルテット')).toEqual('kwarutetto'); // Quartet\n    expect(transliterateJa('クィンテット')).toEqual('kwintetto'); // Quintet\n    expect(transliterateJa('クォーター')).toEqual('kwōtā'); // Quarter\n    expect(transliterateJa('サブウーファー')).toEqual('sabuūfā'); // Sub woofer\n    expect(transliterateJa('ソフトウェア')).toEqual('sofutowea'); // Software\n    expect(transliterateJa('ツィーター')).toEqual('tsītā'); // Tweeter\n    expect(transliterateJa('デューティー')).toEqual('dyūtī'); // Duty\n    expect(transliterateJa('ドキュメントィンドウ')).toEqual('dokyumentwindō'); // Document window\n    expect(transliterateJa('ヌーヴェルヴァーグ')).toEqual('nūveruvāgu'); // Nouvelle vague\n    expect(transliterateJa('ハイジャッンプ')).toEqual('haijanmpu'); // High jump\n    expect(transliterateJa('バッファ')).toEqual('baffa'); // Buffer\n    expect(transliterateJa('フーホェア')).toEqual('fūhwea'); // WhoWhere?\n    expect(transliterateJa('フェイズィング')).toEqual('feizingu'); // Phasing\n    expect(transliterateJa('フッロピー')).toEqual('furropī'); // Floppy (alternative transcription)\n    expect(transliterateJa('ブュー')).toEqual('byū'); // View\n    expect(transliterateJa('フューチャー')).toEqual('fyūchā'); // Future\n    expect(transliterateJa('フロッピィ')).toEqual('furoppī'); // Floppy\n    expect(transliterateJa('ボージョレー・ヌーヴォー')).toEqual('bōjorē nūvō'); // Beaujolais nouveau\n    expect(transliterateJa('ボスニア・ヘルツェゴビナ')).toEqual('bosunia herutsegobina'); // Bosnia and Herzegovina\n    expect(transliterateJa('マッハ')).toEqual('mahha'); // Mach\n    expect(transliterateJa('レヴュー')).toEqual('revyū'); // Review\n    expect(transliterateJa('レクリェーション')).toEqual('rekuryēshon'); // Recreation\n  });","file":"transliterator_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should use fallback for small vowels on special case","suites":["transliterateJa"],"line":89,"updatePoint":{"line":89,"column":58,"index":5134},"code":"  it('should use fallback for small vowels on special case', function () {\n    // These words use rare combination of small vowels.\n    expect(transliterateJa('アゲィンスト')).toEqual('ageinsuto'); // Against (alternative transcription)\n    expect(transliterateJa('エッセィ')).toEqual('essei'); // Essay (alternative transcription)\n    expect(transliterateJa('ゾゥアラジィ')).toEqual('zouarajī'); // Zoology (alternative transcription)\n    expect(transliterateJa('ゾゥァラジカル')).toEqual('zouarajikaru'); // Zoological (alternative transcription)\n    expect(transliterateJa('ボランテァ')).toEqual('borantea'); // Volunteer (alternative transcription)\n  });","file":"transliterator_ja_spec.js","skipped":false,"dir":"spec"},{"name":"should tokenize","suites":["treebank"],"line":28,"updatePoint":{"line":28,"column":21,"index":1250},"code":"  it('should tokenize', function () {\n    let tokens = tokenizer.tokenize(\"If we 'all' can't go. I'll stay home.\");\n    expect(tokens).toEqual(['If', 'we', \"'all\", \"'\", 'ca', \"n't\", 'go.', 'I', \"'ll\", 'stay', 'home', '.']);\n    tokens = tokenizer.tokenize(\"If we 'all' can't go. I'll stay home. If we 'all' can't go. I'll stay home.\");\n    expect(tokens).toEqual(['If', 'we', \"'all\", \"'\", 'ca', \"n't\", 'go.', 'I', \"'ll\", 'stay', 'home.', 'If', 'we', \"'all\", \"'\", 'ca', \"n't\", 'go.', 'I', \"'ll\", 'stay', 'home', '.']);\n  });","file":"treebank_word_tokenizer_spec.js","skipped":false,"dir":"spec"},{"name":"should add words one at a time","suites":["trie","adding strings"],"line":28,"updatePoint":{"line":28,"column":38,"index":1237},"code":"    it('should add words one at a time', function () {\n      const trie = new Trie();\n      trie.addString('test');\n      expect(trie.contains('test')).toBe(true);\n      trie.addString('abcd');\n      expect(trie.contains('abcd')).toBe(true);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should return true if a string is added that already existed","suites":["trie","adding strings"],"line":35,"updatePoint":{"line":35,"column":68,"index":1517},"code":"    it('should return true if a string is added that already existed', function () {\n      const trie = new Trie();\n      expect(trie.addString('test')).toBe(false);\n      expect(trie.addString('test')).toBe(true);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"Should add an array of strings","suites":["trie","adding strings"],"line":40,"updatePoint":{"line":40,"column":38,"index":1710},"code":"    it('Should add an array of strings', function () {\n      const trie = new Trie();\n      const testWords = ['test', 'abcd', 'ffff'];\n      trie.addStrings(testWords);\n      for (let i = testWords.length - 1; i >= 0; i--) {\n        expect(trie.contains(testWords[i])).toBe(true);\n      }\n      ;\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should return 1 for an empty trie","suites":["trie","getSize"],"line":51,"updatePoint":{"line":51,"column":41,"index":2061},"code":"    it('should return 1 for an empty trie', function () {\n      const trie = new Trie();\n      expect(trie.getSize()).toBe(1);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should return the correct size","suites":["trie","getSize"],"line":55,"updatePoint":{"line":55,"column":38,"index":2193},"code":"    it('should return the correct size', function () {\n      const trie = new Trie();\n      trie.addString('a');\n      expect(trie.getSize()).toBe(2);\n      trie.addString('ab');\n      expect(trie.getSize()).toBe(3);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should count all branches","suites":["trie","getSize"],"line":62,"updatePoint":{"line":62,"column":33,"index":2413},"code":"    it('should count all branches', function () {\n      const trie = new Trie();\n      trie.addString('a');\n      expect(trie.getSize()).toBe(2);\n      trie.addString('ba');\n      expect(trie.getSize()).toBe(4);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should not find words that haven't been added","suites":["trie","searching"],"line":71,"updatePoint":{"line":71,"column":53,"index":2697},"code":"    it(\"should not find words that haven't been added\", function () {\n      const trie = new Trie();\n      expect(trie.contains('aaaaa')).toBe(false);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should not return prefixes of words that have been added as words","suites":["trie","searching"],"line":75,"updatePoint":{"line":75,"column":73,"index":2876},"code":"    it('should not return prefixes of words that have been added as words', function () {\n      const trie = new Trie();\n      trie.addString('test');\n      expect(trie.contains('test')).toBe(true);\n      expect(trie.contains('tes')).toBe(false);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should not return suffixes of words that have been added as words","suites":["trie","searching"],"line":81,"updatePoint":{"line":81,"column":73,"index":3131},"code":"    it('should not return suffixes of words that have been added as words', function () {\n      const trie = new Trie();\n      trie.addString('test');\n      expect(trie.contains('test')).toBe(true);\n      expect(trie.contains('est')).toBe(false);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should not find a word that falls in between two other words but has not been added","suites":["trie","searching"],"line":87,"updatePoint":{"line":87,"column":91,"index":3404},"code":"    it('should not find a word that falls in between two other words but has not been added', function () {\n      const trie = new Trie();\n      trie.addString('test');\n      trie.addString('tested');\n      expect(trie.contains('test')).toBe(true);\n      expect(trie.contains('tested')).toBe(true);\n      expect(trie.contains('teste')).toBe(false);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should be able to find all full prefix matched words along a path.","suites":["trie","prefix searching"],"line":104,"updatePoint":{"line":104,"column":74,"index":4028},"code":"    it('should be able to find all full prefix matched words along a path.', function () {\n      const trie = new Trie();\n      trie.addStrings(['a', 'ab', 'bc', 'cd', 'abc']);\n      const results = trie.findMatchesOnPath('abcd');\n      expectResults(results);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should be able to guess all full prefix matched words.","suites":["trie","prefix searching"],"line":110,"updatePoint":{"line":110,"column":62,"index":4285},"code":"    it('should be able to guess all full prefix matched words.', function () {\n      const trie = new Trie();\n      trie.addStrings(['a', 'ab', 'bc', 'cd', 'abc']);\n      let results = trie.keysWithPrefix('a');\n      expectResults(results);\n      results = trie.keysWithPrefix('ab');\n      expect(results).toContain('ab');\n      expect(results).toContain('abc');\n      expect(results).not.toContain('a');\n      expect(results).not.toContain('bc');\n      expect(results).not.toContain('cd');\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should be able to execute the find_prefix search with a match","suites":["trie","prefix searching"],"line":122,"updatePoint":{"line":122,"column":69,"index":4791},"code":"    it('should be able to execute the find_prefix search with a match', function () {\n      const trie = new Trie();\n      trie.addStrings(['their', 'and', 'they']);\n      const results = trie.findPrefix('theyre');\n      expect(results[0]).toBe('they');\n      expect(results[1]).toBe('re');\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should return empty array if no full prefix matches found.","suites":["trie","prefix searching"],"line":129,"updatePoint":{"line":129,"column":66,"index":5087},"code":"    it('should return empty array if no full prefix matches found.', function () {\n      const trie = new Trie();\n      trie.addStrings(['a', 'ab', 'bc', 'cd', 'abc']);\n      const results = trie.keysWithPrefix('not-found');\n      expect(results.length).toEqual(0);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should be able to execute the find_prefix search without a match","suites":["trie","prefix searching"],"line":135,"updatePoint":{"line":135,"column":72,"index":5367},"code":"    it('should be able to execute the find_prefix search without a match', function () {\n      const trie = new Trie();\n      trie.addStrings(['their', 'and']);\n      const results = trie.findPrefix('theyre');\n      expect(results[0]).toBe(null);\n      expect(results[1]).toBe('yre');\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should be case sensitive by default","suites":["trie","case sensitivity"],"line":144,"updatePoint":{"line":144,"column":43,"index":5682},"code":"    it('should be case sensitive by default', function () {\n      const trie = new Trie();\n      trie.addString('test');\n      expect(trie.contains('TEST')).toBe(false);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should have contains in case-insensitive mode","suites":["trie","case sensitivity"],"line":149,"updatePoint":{"line":149,"column":53,"index":5870},"code":"    it('should have contains in case-insensitive mode', function () {\n      const trie = new Trie(false);\n      trie.addString('test');\n      expect(trie.contains('TEST')).toBe(true);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should have case-insensitive contains when the strings are added with case","suites":["trie","case sensitivity"],"line":154,"updatePoint":{"line":154,"column":82,"index":6091},"code":"    it('should have case-insensitive contains when the strings are added with case', function () {\n      const trie = new Trie(false);\n      trie.addString('teSt');\n      expect(trie.contains('test')).toBe(true);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should have findMatchesOnPath in case-insensitive mode","suites":["trie","case sensitivity"],"line":159,"updatePoint":{"line":159,"column":62,"index":6292},"code":"    it('should have findMatchesOnPath in case-insensitive mode', function () {\n      const trie = new Trie(false);\n      trie.addStrings(['a', 'ab', 'bc', 'cd', 'abc']);\n      const results = trie.findMatchesOnPath('ABcD');\n      expectResults(results);\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should have findPrefix in case-insensitive mode","suites":["trie","case sensitivity"],"line":165,"updatePoint":{"line":165,"column":55,"index":6547},"code":"    it('should have findPrefix in case-insensitive mode', function () {\n      const trie = new Trie(false);\n      trie.addStrings(['thEIr', 'And', 'theY']);\n      const results = trie.findPrefix('ThEyRe');\n      expect(results[0]).toBe('they');\n      expect(results[1]).toBe('re');\n    });","file":"trie_spec.js","skipped":false,"dir":"spec"},{"name":"should correctly tokenize words and punctuation symbols","suites":["Word Punctuation Tokenizer"],"line":30,"updatePoint":{"line":30,"column":61,"index":3239},"code":"  it('should correctly tokenize words and punctuation symbols', function () {\n    sentences.forEach((sentence, index) => {\n      const result = tokenizer.tokenize(sentence);\n      expect(result).toEqual(expectedResults[index]);\n    });\n  });","file":"WordPunctTokenizer_spec.js","skipped":false,"dir":"spec"}]}